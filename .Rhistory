startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
tabl <- function(..., TAB=tab, rtf=doc, fontt=8, row.names=TRUE, tabc=tabcount, boldt=TRUE, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
startParagraph(this=rtf)
addText(this=rtf, paste0("Table ", tabc, ".  "), bold=boldt)
addText(this=rtf, ...)
addNewLine(this=rtf)
endParagraph(this=rtf)
addTable(this=rtf, TAB, font.size=fontt, row.names=row.names)
addNewLine(this=rtf)
addNewLine(this=rtf)
tabcount <<- tabc + 1
}
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, wf=6.5, hf=8.2, rf=300, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
end.rtf <- function(rtf=doc, details=TRUE, ...) {
addPageBreak(rtf, ...)
addSessionInfo(rtf)
done(rtf)
}
}
### CRUNCHING
if(casefold(substring(explore.ac, 1, 1))=="y") { # read in AC data
# 1.  Import data
# Concatenate all the files of type csv in a particular folder into one file for Sv, one for TS.  
# Trawl data will already be in one file per type of data (catch, lf, op).  
# Need to make R parse out year from the sampling date field.
### A.  Sv data from Echoview, one file per transect
sv <- combine.csv(svdir)
# create date variables that R recognizes as dates
sv$date.s <- ymd(sv$Date_S, quiet=TRUE)
sv$date.e <- ymd(sv$Date_E, quiet=TRUE)
sv$date.m <- ymd(sv$Date_M, quiet=TRUE)
# create variable for year
sv$year <- year(sv$date.m)
# assign NAs to missing value codes
sv2 <- missings(sv)
# eliminate records with missing values for Lat_M, Sv_mean, or PRC_ABC
sv2 <- sv2[!(is.na(sv2$Lat_M) | is.na(sv2$Sv_mean) | is.na(sv2$PRC_ABC)), ]
### B.  Single target frequencies by TS bin, one for each transect
ts <- combine.csv(tsdir)
# create date variables that R recognizes as dates
ts$date.s <- ymd(ts$Date_S, quiet=TRUE)
ts$date.e <- ymd(ts$Date_E, quiet=TRUE)
ts$date.m <- ymd(ts$Date_M, quiet=TRUE)
# create variable for year
ts$year <- year(ts$date.m)
# assign NAs to missing value codes
ts2 <- missings(ts)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# read in trawl data
# RVCAT data
optrop <- read.csv(optrop.f, as.is=TRUE, na.strings="NULL")
trcatch <- read.csv(trcatch.f, as.is=TRUE, na.strings="NULL")
trlf <- read.csv(trlf.f, as.is=TRUE, na.strings="NULL")
key106 <- read.csv(key106.f, as.is=TRUE)
}
}
search()
cleanup()
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\ACMT Explore Works on Huron files 20 Jan 2015 JVAmody.r
load("C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/L3 Y2014 ACMT Data.RData")
if(FALSE) {
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where output will be placed
# outputs include an Rdata file with data to be used for estimation and
# a summary of the data exploration in a Word document
# make sure you use forward slashes (/) not back slashes (\)
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22"
### Do you want to explore acoustic data? ###
explore.ac <- "yes"
# if you DON'T want to explore acoustic data, you can ignore the next several lines 
# directory where all Sv data from Echoview are stored in csv files, one file per transect
# svdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Sv/"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
# directory where all single target frequencies by TS bin are stored in csv files, one for each transect
# tsdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/TS/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
### Do you want to explore trawl data? ###
explore.tr <- "yes"
# if you DON'T want to explore trawl data, you can ignore the next several lines 
# RVCAT data files
# midwater trawl op file which includes data from both OP and TR_OP
# optrop.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron Midwater Trawl Op.csv"
optrop.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron Midwater Trawl Op.csv"
# midwater trawl catch file
# trcatch.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater catch.csv"
trcatch.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater catch.csv"
# midwater trawl tr_lf file
# trlf.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater LF.csv"
trlf.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater LF.csv"
# alewife age-length key per year
# key106.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/alewifeagelenkey.csv"
key106.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/alewifeagelenkey.csv"
#########################################################################################################
{### FUNCTIONS
get.packages <- function(want) {
# install (if necessary) and attach wanted packages
have <- row.names(installed.packages())
need <- want[!(want %in% have)]
if(length(need)>0) install.packages(need, repos="http://cran.r-project.org")
lapply(want, require, character.only=TRUE)
invisible()
}
get.packages(c("lubridate", "rtf", "maps"))
lakenames <- c("Superior", "Michigan", "Huron", "Erie", "Ontario")
myrecode <- function(x, old, new, must.match=T) {
partial <- match(x, old)
if(must.match) new[partial] else ifelse(!is.na(partial), new[partial], x)
}
combine.csv <- function(mydir, add.source=TRUE) {
# combine all csv files in a given directory into a single data frame
# file names
filenames <- list.files(mydir)[grep(".csv$", list.files(mydir))]
nfiles <- length(filenames)
# create an empty list where all the files will be stored
files.list <- vector(mode="list", length=nfiles)
for(i in 1:nfiles) {
# read the data into a temporary file
temp <- read.csv(paste(mydir, filenames[i], sep=""), as.is=TRUE)
# add a new column identifying the source file
if(add.source) temp$source <- filenames[i]
# put the data into the list
files.list[[i]] <- temp
}
# combined each of the files from the list into one single file
do.call(rbind, files.list)
}
plotmap <- function(lat, long, colorz, main="", pch=1, cex=1.5, rtf=TRUE, xla=0, yla=0) {
# plot longitude versus latitude with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
map("usa", xlim=range(long, na.rm=TRUE) + c(-1, 1)*xla, ylim=range(lat, na.rm=TRUE) + c(-1, 1)*xla, mar=c(0, 0, 2, 0))
points(long, lat, pch=pch, cex=cex, col=colorz)
box()
mtext(main, side=3, cex=1.2)
}
mid <- function(x) {
# calculate the midpoint between the min and the max
(max(x, na.rm=TRUE) + min(x, na.rm=TRUE))/2
}
textg <- function(lt, ln, group, cex=1.5, ...) {
# add text to plot at group lat/long midpoints
sug <- sort(unique(group))
lng <- tapply(ln, group, mid)
ltg <- tapply(lt, group, mid)
text(lng, ltg, sug, cex=cex, ...)
}
plotil <- function(colorz, main="", rtf=TRUE) {
# plot interval versus layer with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
par(mfrow=n2mfrow(length(sur)), mar=c(2, 2, 2, 1), oma=c(2, 2, 2, 0), cex=1)
for(i in order(lat.r, decreasing=T)) {
sel <- Region_name==sur[i]
plot(Interval[sel], -(Layer_depth_min + Layer_depth_max)[sel]/2, col=colorz[sel], pch=16,
ylim=range(-(Layer_depth_min + Layer_depth_max)/2, na.rm=TRUE), xlab="", ylab="", main=sur[i])
}
mtext("Interval", side=1, outer=T, line=1, cex=1.5)
mtext("Layer depth", side=2, outer=T, line=1, cex=1.5)
mtext(main, side=3, outer=T, cex=1.5)
}
plot3 <- function(lo, hi, betw, rtf=TRUE, order.matters=TRUE, varname="Varname", test=FALSE, ...) {
# plot lows, highs, and betweens for a given metric
x <- seq(hi)
if(!order.matters) {
newlo <- pmin(lo, hi, betw, na.rm=TRUE)
newhi <- pmax(lo, hi, betw, na.rm=TRUE)
lo <- newlo
hi <- newhi
}
ord <- order(hi)
lo. <- lo[ord]
be. <- betw[ord]
hi. <- hi[ord]
sel.lo <- (!is.na(lo.) & !is.na(be.) & lo. > be.) | (!is.na(lo.) & !is.na(hi.) & lo. > hi.)
sel.be <- !is.na(be.) & !is.na(hi.) & be. > hi.
problems <- sum(sum(sel.be, na.rm=TRUE), sum(sel.lo, na.rm=TRUE))
if(!test) {
yr <- range(lo., hi., be., na.rm=TRUE)
if(!rtf) windows(w=6.5, h=8.2)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(x, be., type="n", ylim=yr, las=1, xlab="Records ordered by max", ylab=varname, ...)
points(x[!sel.lo], lo.[!sel.lo], pch=6, col="blue")
points(x[!sel.be], be.[!sel.be])
points(x, hi., pch=2, col="red")
points(x[sel.lo], lo.[sel.lo], pch=6, col="cyan")
points(x[sel.be], be.[sel.be], pch=3, col="green")
legend("topleft", c(paste0("Mid > Max (", format(sum(sel.be, na.rm=TRUE), big.mark=","), " records)"), "Max", 
paste0("Min > Mid or Max (", format(sum(sel.lo, na.rm=TRUE), big.mark=","), " records)"), "Mid < Max", "Min < Mid and Max"), 
col=c("green", "red", "cyan", "black", "blue"), pch=c(3, 2, 6, 1, 6), cex=1.5)
}
if(is.na(problems) | problems < 1) return(FALSE)
}
plotsp <- function(y, ylabb, rtf=TRUE) {
if(!rtf) windows(w=6.5, h=9)
par(mar=c(4, 4, 1, 1))
plot(as.factor(Species), sqrt(y), xlab="Species", ylab=ylabb, axes=F)
axis(1, at=seq(sus), labels=sus)
axis(2, at=pretty(sqrt(y)), labels=pretty(sqrt(y))^2, las=1)
box()
}
missings <- function(df, misscodes=c(-9999, -999.9, -999, 999, 9999), misschars=c("NA", "NULL", ".", " ", "  ")) {
# assign NAs to missing value codes in numeric and character columns of data.frame
df.cur <- df
number.colz <- sapply(df, class) %in% c("integer", "numeric")
df[, number.colz] <- lapply(df[, number.colz], function(x) {
x[x %in% misscodes] <- NA
x
})
char.colz <- sapply(df, class) %in% c("character")
df[, char.colz] <- lapply(df[, char.colz, drop=FALSE], function(x) {
x[x %in% misschars] <- ""
xwe <- tryCatch.W.E(as.numeric(x))
if(is.null(xwe$warning)) x <- xwe$value
x
})
# cat("current = the original dataframe, target = the new dataframe\n")
# print(all.equal(current=df.cur, df))
df
}
qksmry <- function(df) {
no.unique <- sapply(df, function(x) length(table(x)))
no.entered <- sapply(df, function(x) if(is.character(x)) sum(x!="") else sum(!is.na(x)))
no.missing <- sapply(df, function(x) if(is.character(x)) sum(x=="") else sum(is.na(x)))
cbind(no.unique, no.entered, no.missing)
}
plot.df <- function(df, one=FALSE, mcex=1.2, cex=0.8, ...) {
if(one) windows(record=T)
par(cex=cex, ...)
# plot the columns of a data frame
for(i in 1:dim(df)[2]) {
x <- df[[i]]
if(sum(!is.na(x))>0) {
name <- names(df)[i]
if(is.factor(x)) plot(x, main=name, cex.main=mcex) else
if(is.character(x) & length(unique(x))<=50) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x) & sum(is.finite(x)) < 2) plot(1, 1, type="n", xlab="", ylab="", axes=F, 
main=paste(name, ": numeric", sep=""), cex.main=mcex) else
if(is.numeric(x) & length(unique(x))<=10) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x)) plot.default(x[is.finite(x)], main=name, cex.main=mcex) else
if(is.character(x)) plot(1, 1, type="n", xlab="", ylab="", axes=F, main=paste(name, ": character", sep=""), cex.main=mcex)
}
}
invisible()
}
plot.matrix <- function(m, inc=0.3, ...) {
# plots values of matrix as different sized circles
# dimension 1 is plotted on x-axis, dimension 2 on y-axis
uxy <- dimnames(m)
ux <- as.numeric(uxy[[1]])
uxlab <- NULL
if(any(is.na(ux))) {
ux <- seq(along=ux)
uxlab <- uxy[[1]]
}
uy <- as.numeric(uxy[[2]])
uylab <- NULL
if(any(is.na(uy))) {
uy <- seq(along=uy)
uylab <- uxy[[2]]
}
colorz <- c("cyan", "black", "red")[as.numeric(cut(as.vector(m), breaks=c(-Inf, -1e-7, 1e-7, Inf)))]
cush.x <- mean(abs(diff(ux)))
cush.y <- mean(abs(diff(uy)))
plot(ux[row(m)], uy[col(m)], type="n", xlim=range(ux) + cush.x*c(-1, 1), ylim=range(uy) + cush.y*c(-1, 1), axes=FALSE, ...)
symbols(ux[row(m)], uy[col(m)], circle=abs(as.vector(m)), inches=inc, fg=colorz, add=TRUE)
if(is.null(uxlab)) axis(1) else axis(1, at=ux, labels=uxlab)
if(is.null(uylab)) axis(2, las=1) else axis(2, at=uy, labels=uylab, las=1)
box()
}
tryCatch.W.E <- function(expr) {
# Martin Maechler, ETH Zurich, 9 Dec 2010, R-help subject: "How to catch both warnings and errors?"
W <- NULL
w.handler <- function(w) {
W <<- w
invokeRestart("muffleWarning")
}
list(value=withCallingHandlers(tryCatch(expr, error=function(e) e), warning=w.handler), warning=W)
}
scale.02n <- function(x, n=100) {
# scale a collection of values from 0 to some specified maximum
n*as.numeric(scale(x, center=min(x, na.rm=TRUE), scale=diff(range(x, na.rm=TRUE))))
}
rain.n <- function(x, n=100, start=0/6, end=4/6) {
# assign a specified number of rainbow colors to a collection of values
# default color range from blue (4/6) to red (0/6)
y <- round(scale.02n(x, n=n-1)) + 1
# reverse the direction of y (e.g., from 1:100 to 100:1)
y <- n + 1 - y
rainbow(n, start=start, end=end)[y]
}
start.rtf <- function(file=NULL, dir=getwd(), width=8.5, height=11, font.size=12, omi=c(1, 1, 1, 1), quiet=FALSE) {
# create a new RTF file readable by Word
# create two new variables to keep count of tables and figures
tabcount <<- 1
figcount <<- 1
if(is.null(file)) file <- paste0("RGeneratedDocument", Sys.Date())
dirfiledoc <- if(length(grep(".doc", file))>0) paste(dir, file, sep="/") else paste(dir, paste0(file, ".doc"), sep="/")
if(!quiet) cat(paste0("New RTF document created, ", dirfiledoc, "\n"))
RTF(dirfiledoc, width=width, height=height, font.size=font.size, omi=omi)
}
head1 <- function(words, rtf=doc, font.size=16) {
addHeader(this=rtf, title=words)
}
head2 <- function(words, rtf=doc, bold=TRUE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, words, bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
head3 <- function(..., rtf=doc, bold=FALSE, italic=TRUE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
para <- function(..., rtf=doc, bold=FALSE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
tabl <- function(..., TAB=tab, rtf=doc, fontt=8, row.names=TRUE, tabc=tabcount, boldt=TRUE, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
startParagraph(this=rtf)
addText(this=rtf, paste0("Table ", tabc, ".  "), bold=boldt)
addText(this=rtf, ...)
addNewLine(this=rtf)
endParagraph(this=rtf)
addTable(this=rtf, TAB, font.size=fontt, row.names=row.names)
addNewLine(this=rtf)
addNewLine(this=rtf)
tabcount <<- tabc + 1
}
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, wf=6.5, hf=8.2, rf=300, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
end.rtf <- function(rtf=doc, details=TRUE, ...) {
addPageBreak(rtf, ...)
addSessionInfo(rtf)
done(rtf)
}
}
### CRUNCHING
if(casefold(substring(explore.ac, 1, 1))=="y") { # read in AC data
# 1.  Import data
# Concatenate all the files of type csv in a particular folder into one file for Sv, one for TS.  
# Trawl data will already be in one file per type of data (catch, lf, op).  
# Need to make R parse out year from the sampling date field.
### A.  Sv data from Echoview, one file per transect
sv <- combine.csv(svdir)
# create date variables that R recognizes as dates
sv$date.s <- ymd(sv$Date_S, quiet=TRUE)
sv$date.e <- ymd(sv$Date_E, quiet=TRUE)
sv$date.m <- ymd(sv$Date_M, quiet=TRUE)
# create variable for year
sv$year <- year(sv$date.m)
# assign NAs to missing value codes
sv2 <- missings(sv)
# eliminate records with missing values for Lat_M, Sv_mean, or PRC_ABC
sv2 <- sv2[!(is.na(sv2$Lat_M) | is.na(sv2$Sv_mean) | is.na(sv2$PRC_ABC)), ]
### B.  Single target frequencies by TS bin, one for each transect
ts <- combine.csv(tsdir)
# create date variables that R recognizes as dates
ts$date.s <- ymd(ts$Date_S, quiet=TRUE)
ts$date.e <- ymd(ts$Date_E, quiet=TRUE)
ts$date.m <- ymd(ts$Date_M, quiet=TRUE)
# create variable for year
ts$year <- year(ts$date.m)
# assign NAs to missing value codes
ts2 <- missings(ts)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# read in trawl data
# RVCAT data
optrop <- read.csv(optrop.f, as.is=TRUE, na.strings="NULL")
trcatch <- read.csv(trcatch.f, as.is=TRUE, na.strings="NULL")
trlf <- read.csv(trlf.f, as.is=TRUE, na.strings="NULL")
key106 <- read.csv(key106.f, as.is=TRUE)
}
}
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Explore ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
explore <- 10*(casefold(substring(explore.ac, 1, 1))=="y") + 1*(casefold(substring(explore.tr, 1, 1))=="y")
descr <- myrecode(explore, c(0, 1, 10, 11), c("No", "Trawl", "Acoustic", "Acoustic and Trawl"))
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Exploration of ", descr, " Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
explore.ac <- "yes"
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Explore ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
explore <- 10*(casefold(substring(explore.ac, 1, 1))=="y") + 1*(casefold(substring(explore.tr, 1, 1))=="y")
descr <- myrecode(explore, c(0, 1, 10, 11), c("No", "Trawl", "Acoustic", "Acoustic and Trawl"))
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Exploration of ", descr, " Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
load("C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/L3 Y2014 ACMT Data.RData")
explore.ac <- "yes"
explore.tr <- "yes"
if(FALSE) {
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where output will be placed
# outputs include an Rdata file with data to be used for estimation and
# a summary of the data exploration in a Word document
# make sure you use forward slashes (/) not back slashes (\)
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22"
### Do you want to explore acoustic data? ###
explore.ac <- "yes"
# if you DON'T want to explore acoustic data, you can ignore the next several lines 
# directory where all Sv data from Echoview are stored in csv files, one file per transect
# svdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Sv/"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
# directory where all single target frequencies by TS bin are stored in csv files, one for each transect
# tsdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/TS/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
### Do you want to explore trawl data? ###
explore.tr <- "yes"
# if you DON'T want to explore trawl data, you can ignore the next several lines 
# RVCAT data files
# midwater trawl op file which includes data from both OP and TR_OP
# optrop.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron Midwater Trawl Op.csv"
optrop.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron Midwater Trawl Op.csv"
# midwater trawl catch file
# trcatch.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater catch.csv"
trcatch.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater catch.csv"
# midwater trawl tr_lf file
# trlf.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater LF.csv"
trlf.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater LF.csv"
# alewife age-length key per year
# key106.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/alewifeagelenkey.csv"
key106.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/alewifeagelenkey.csv"
#########################################################################################################
{### FUNCTIONS
get.packages <- function(want) {
# install (if necessary) and attach wanted packages
have <- row.names(installed.packages())
need <- want[!(want %in% have)]
if(length(need)>0) install.packages(need, repos="http://cran.r-project.org")
lapply(want, require, character.only=TRUE)
invisible()
}
get.packages(c("lubridate", "rtf", "maps"))
lakenames <- c("Superior", "Michigan", "Huron", "Erie", "Ontario")
myrecode <- function(x, old, new, must.match=T) {
partial <- match(x, old)
if(must.match) new[partial] else ifelse(!is.na(partial), new[partial], x)
}
combine.csv <- function(mydir, add.source=TRUE) {
# combine all csv files in a given directory into a single data frame
# file names
filenames <- list.files(mydir)[grep(".csv$", list.files(mydir))]
nfiles <- length(filenames)
# create an empty list where all the files will be stored
files.list <- vector(mode="list", length=nfiles)
for(i in 1:nfiles) {
# read the data into a temporary file
temp <- read.csv(paste(mydir, filenames[i], sep=""), as.is=TRUE)
# add a new column identifying the source file
if(add.source) temp$source <- filenames[i]
# put the data into the list
files.list[[i]] <- temp
}
# combined each of the files from the list into one single file
do.call(rbind, files.list)
}
plotmap <- function(lat, long, colorz, main="", pch=1, cex=1.5, rtf=TRUE, xla=0, yla=0) {
# plot longitude versus latitude with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
map("usa", xlim=range(long, na.rm=TRUE) + c(-1, 1)*xla, ylim=range(lat, na.rm=TRUE) + c(-1, 1)*xla, mar=c(0, 0, 2, 0))
points(long, lat, pch=pch, cex=cex, col=colorz)
box()
mtext(main, side=3, cex=1.2)
}
mid <- function(x) {
# calculate the midpoint between the min and the max
(max(x, na.rm=TRUE) + min(x, na.rm=TRUE))/2
}
textg <- function(lt, ln, group, cex=1.5, ...) {
# add text to plot at group lat/long midpoints
sug <- sort(unique(group))
lng <- tapply(ln, group, mid)
ltg <- tapply(lt, group, mid)
text(lng, ltg, sug, cex=cex, ...)
}
plotil <- function(colorz, main="", rtf=TRUE) {
# plot interval versus layer with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
par(mfrow=n2mfrow(length(sur)), mar=c(2, 2, 2, 1), oma=c(2, 2, 2, 0), cex=1)
for(i in order(lat.r, decreasing=T)) {
sel <- Region_name==sur[i]
plot(Interval[sel], -(Layer_depth_min + Layer_depth_max)[sel]/2, col=colorz[sel], pch=16,
ylim=range(-(Layer_depth_min + Layer_depth_max)/2, na.rm=TRUE), xlab="", ylab="", main=sur[i])
}
mtext("Interval", side=1, outer=T, line=1, cex=1.5)
mtext("Layer depth", side=2, outer=T, line=1, cex=1.5)
mtext(main, side=3, outer=T, cex=1.5)
}
plot3 <- function(lo, hi, betw, rtf=TRUE, order.matters=TRUE, varname="Varname", test=FALSE, ...) {
# plot lows, highs, and betweens for a given metric
x <- seq(hi)
if(!order.matters) {
newlo <- pmin(lo, hi, betw, na.rm=TRUE)
newhi <- pmax(lo, hi, betw, na.rm=TRUE)
lo <- newlo
hi <- newhi
}
ord <- order(hi)
lo. <- lo[ord]
be. <- betw[ord]
hi. <- hi[ord]
sel.lo <- (!is.na(lo.) & !is.na(be.) & lo. > be.) | (!is.na(lo.) & !is.na(hi.) & lo. > hi.)
sel.be <- !is.na(be.) & !is.na(hi.) & be. > hi.
problems <- sum(sum(sel.be, na.rm=TRUE), sum(sel.lo, na.rm=TRUE))
if(!test) {
yr <- range(lo., hi., be., na.rm=TRUE)
if(!rtf) windows(w=6.5, h=8.2)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(x, be., type="n", ylim=yr, las=1, xlab="Records ordered by max", ylab=varname, ...)
points(x[!sel.lo], lo.[!sel.lo], pch=6, col="blue")
points(x[!sel.be], be.[!sel.be])
points(x, hi., pch=2, col="red")
points(x[sel.lo], lo.[sel.lo], pch=6, col="cyan")
points(x[sel.be], be.[sel.be], pch=3, col="green")
legend("topleft", c(paste0("Mid > Max (", format(sum(sel.be, na.rm=TRUE), big.mark=","), " records)"), "Max", 
paste0("Min > Mid or Max (", format(sum(sel.lo, na.rm=TRUE), big.mark=","), " records)"), "Mid < Max", "Min < Mid and Max"), 
col=c("green", "red", "cyan", "black", "blue"), pch=c(3, 2, 6, 1, 6), cex=1.5)
}
if(is.na(problems) | problems < 1) return(FALSE)
}
plotsp <- function(y, ylabb, rtf=TRUE) {
if(!rtf) windows(w=6.5, h=9)
par(mar=c(4, 4, 1, 1))
plot(as.factor(Species), sqrt(y), xlab="Species", ylab=ylabb, axes=F)
axis(1, at=seq(sus), labels=sus)
axis(2, at=pretty(sqrt(y)), labels=pretty(sqrt(y))^2, las=1)
box()
}
missings <- function(df, misscodes=c(-9999, -999.9, -999, 999, 9999), misschars=c("NA", "NULL", ".", " ", "  ")) {
# assign NAs to missing value codes in numeric and character columns of data.frame
df.cur <- df
number.colz <- sapply(df, class) %in% c("integer", "numeric")
df[, number.colz] <- lapply(df[, number.colz], function(x) {
x[x %in% misscodes] <- NA
x
})
char.colz <- sapply(df, class) %in% c("character")
df[, char.colz] <- lapply(df[, char.colz, drop=FALSE], function(x) {
x[x %in% misschars] <- ""
xwe <- tryCatch.W.E(as.numeric(x))
if(is.null(xwe$warning)) x <- xwe$value
x
})
# cat("current = the original dataframe, target = the new dataframe\n")
# print(all.equal(current=df.cur, df))
df
}
qksmry <- function(df) {
no.unique <- sapply(df, function(x) length(table(x)))
no.entered <- sapply(df, function(x) if(is.character(x)) sum(x!="") else sum(!is.na(x)))
no.missing <- sapply(df, function(x) if(is.character(x)) sum(x=="") else sum(is.na(x)))
cbind(no.unique, no.entered, no.missing)
}
plot.df <- function(df, one=FALSE, mcex=1.2, cex=0.8, ...) {
if(one) windows(record=T)
par(cex=cex, ...)
# plot the columns of a data frame
for(i in 1:dim(df)[2]) {
x <- df[[i]]
if(sum(!is.na(x))>0) {
name <- names(df)[i]
if(is.factor(x)) plot(x, main=name, cex.main=mcex) else
if(is.character(x) & length(unique(x))<=50) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x) & sum(is.finite(x)) < 2) plot(1, 1, type="n", xlab="", ylab="", axes=F, 
main=paste(name, ": numeric", sep=""), cex.main=mcex) else
if(is.numeric(x) & length(unique(x))<=10) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x)) plot.default(x[is.finite(x)], main=name, cex.main=mcex) else
if(is.character(x)) plot(1, 1, type="n", xlab="", ylab="", axes=F, main=paste(name, ": character", sep=""), cex.main=mcex)
}
}
invisible()
}
plot.matrix <- function(m, inc=0.3, ...) {
# plots values of matrix as different sized circles
# dimension 1 is plotted on x-axis, dimension 2 on y-axis
uxy <- dimnames(m)
ux <- as.numeric(uxy[[1]])
uxlab <- NULL
if(any(is.na(ux))) {
ux <- seq(along=ux)
uxlab <- uxy[[1]]
}
uy <- as.numeric(uxy[[2]])
uylab <- NULL
if(any(is.na(uy))) {
uy <- seq(along=uy)
uylab <- uxy[[2]]
}
colorz <- c("cyan", "black", "red")[as.numeric(cut(as.vector(m), breaks=c(-Inf, -1e-7, 1e-7, Inf)))]
cush.x <- mean(abs(diff(ux)))
cush.y <- mean(abs(diff(uy)))
plot(ux[row(m)], uy[col(m)], type="n", xlim=range(ux) + cush.x*c(-1, 1), ylim=range(uy) + cush.y*c(-1, 1), axes=FALSE, ...)
symbols(ux[row(m)], uy[col(m)], circle=abs(as.vector(m)), inches=inc, fg=colorz, add=TRUE)
if(is.null(uxlab)) axis(1) else axis(1, at=ux, labels=uxlab)
if(is.null(uylab)) axis(2, las=1) else axis(2, at=uy, labels=uylab, las=1)
box()
}
tryCatch.W.E <- function(expr) {
# Martin Maechler, ETH Zurich, 9 Dec 2010, R-help subject: "How to catch both warnings and errors?"
W <- NULL
w.handler <- function(w) {
W <<- w
invokeRestart("muffleWarning")
}
list(value=withCallingHandlers(tryCatch(expr, error=function(e) e), warning=w.handler), warning=W)
}
scale.02n <- function(x, n=100) {
# scale a collection of values from 0 to some specified maximum
n*as.numeric(scale(x, center=min(x, na.rm=TRUE), scale=diff(range(x, na.rm=TRUE))))
}
rain.n <- function(x, n=100, start=0/6, end=4/6) {
# assign a specified number of rainbow colors to a collection of values
# default color range from blue (4/6) to red (0/6)
y <- round(scale.02n(x, n=n-1)) + 1
# reverse the direction of y (e.g., from 1:100 to 100:1)
y <- n + 1 - y
rainbow(n, start=start, end=end)[y]
}
start.rtf <- function(file=NULL, dir=getwd(), width=8.5, height=11, font.size=12, omi=c(1, 1, 1, 1), quiet=FALSE) {
# create a new RTF file readable by Word
# create two new variables to keep count of tables and figures
tabcount <<- 1
figcount <<- 1
if(is.null(file)) file <- paste0("RGeneratedDocument", Sys.Date())
dirfiledoc <- if(length(grep(".doc", file))>0) paste(dir, file, sep="/") else paste(dir, paste0(file, ".doc"), sep="/")
if(!quiet) cat(paste0("New RTF document created, ", dirfiledoc, "\n"))
RTF(dirfiledoc, width=width, height=height, font.size=font.size, omi=omi)
}
head1 <- function(words, rtf=doc, font.size=16) {
addHeader(this=rtf, title=words)
}
head2 <- function(words, rtf=doc, bold=TRUE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, words, bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
head3 <- function(..., rtf=doc, bold=FALSE, italic=TRUE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
para <- function(..., rtf=doc, bold=FALSE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
tabl <- function(..., TAB=tab, rtf=doc, fontt=8, row.names=TRUE, tabc=tabcount, boldt=TRUE, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
startParagraph(this=rtf)
addText(this=rtf, paste0("Table ", tabc, ".  "), bold=boldt)
addText(this=rtf, ...)
addNewLine(this=rtf)
endParagraph(this=rtf)
addTable(this=rtf, TAB, font.size=fontt, row.names=row.names)
addNewLine(this=rtf)
addNewLine(this=rtf)
tabcount <<- tabc + 1
}
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, wf=6.5, hf=8.2, rf=300, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
end.rtf <- function(rtf=doc, details=TRUE, ...) {
addPageBreak(rtf, ...)
addSessionInfo(rtf)
done(rtf)
}
}
### CRUNCHING
if(casefold(substring(explore.ac, 1, 1))=="y") { # read in AC data
# 1.  Import data
# Concatenate all the files of type csv in a particular folder into one file for Sv, one for TS.  
# Trawl data will already be in one file per type of data (catch, lf, op).  
# Need to make R parse out year from the sampling date field.
### A.  Sv data from Echoview, one file per transect
sv <- combine.csv(svdir)
# create date variables that R recognizes as dates
sv$date.s <- ymd(sv$Date_S, quiet=TRUE)
sv$date.e <- ymd(sv$Date_E, quiet=TRUE)
sv$date.m <- ymd(sv$Date_M, quiet=TRUE)
# create variable for year
sv$year <- year(sv$date.m)
# assign NAs to missing value codes
sv2 <- missings(sv)
# eliminate records with missing values for Lat_M, Sv_mean, or PRC_ABC
sv2 <- sv2[!(is.na(sv2$Lat_M) | is.na(sv2$Sv_mean) | is.na(sv2$PRC_ABC)), ]
### B.  Single target frequencies by TS bin, one for each transect
ts <- combine.csv(tsdir)
# create date variables that R recognizes as dates
ts$date.s <- ymd(ts$Date_S, quiet=TRUE)
ts$date.e <- ymd(ts$Date_E, quiet=TRUE)
ts$date.m <- ymd(ts$Date_M, quiet=TRUE)
# create variable for year
ts$year <- year(ts$date.m)
# assign NAs to missing value codes
ts2 <- missings(ts)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# read in trawl data
# RVCAT data
optrop <- read.csv(optrop.f, as.is=TRUE, na.strings="NULL")
trcatch <- read.csv(trcatch.f, as.is=TRUE, na.strings="NULL")
trlf <- read.csv(trlf.f, as.is=TRUE, na.strings="NULL")
key106 <- read.csv(key106.f, as.is=TRUE)
}
}
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Explore ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
explore <- 10*(casefold(substring(explore.ac, 1, 1))=="y") + 1*(casefold(substring(explore.tr, 1, 1))=="y")
descr <- myrecode(explore, c(0, 1, 10, 11), c("No", "Trawl", "Acoustic", "Acoustic and Trawl"))
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Exploration of ", descr, " Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
rm(descr, docname)
if(casefold(substring(explore.ac, 1, 1))=="y") {# print Sv and TS info to doc
para(paste0("svdir = ", svdir, " = Sv data directory."))
para(paste0("tsdir = ", tsdir, " = single target frequencies data directory."))
}
para(paste0("maindir = ", maindir, " = main input/output directory."))
if(casefold(substring(explore.ac, 1, 1))=="y") {# explore Sv and TS files
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
if(casefold(substring(explore.ac, 1, 1))=="y") {# print Sv and TS info to doc
para(paste0("svdir = ", svdir, " = Sv data directory."))
para(paste0("tsdir = ", tsdir, " = single target frequencies data directory."))
}
para(paste0("maindir = ", maindir, " = main input/output directory."))
casefold(substring(explore.ac, 1, 1))=="y"
### Sv
head2("SV FILES")
para(paste0("The Sv files have ", dim(sv2)[1], " rows and ", dim(sv2)[2], " columns."))
tab <- qksmry(sv2)
tabl("Quick summary table of variables in Sv files.")
para(paste0("Figures 1-9 are exploratory plots of the Sv data that can be examined to check for potential problems.  ",
"Any figures after that (but before the TS section) indicate problems in the data.  ",
"Minimum and maximum values were compared with middle or mean values for each of the following variables: Sv, Depth, Ping, Dist, Date, Lat, Long.  ",
"If any problems were indicated by these comparisons, they will appear as figures after Figure 9 but before the TS section.  "))
qksmry <- function(df) {
no.unique <- sapply(df, function(x) length(table(x)))
no.entered <- sapply(df, function(x) if(is.character(x)) sum(x!="") else sum(!is.na(x)))
no.missing <- sapply(df, function(x) if(is.character(x)) sum(x=="") else sum(is.na(x)))
cbind(no.unique, no.entered, no.missing)
}
tab <- qksmry(sv2)
tabl("Quick summary table of variables in Sv files.")
para(paste0("Figures 1-9 are exploratory plots of the Sv data that can be examined to check for potential problems.  ",
"Any figures after that (but before the TS section) indicate problems in the data.  ",
"Minimum and maximum values were compared with middle or mean values for each of the following variables: Sv, Depth, Ping, Dist, Date, Lat, Long.  ",
"If any problems were indicated by these comparisons, they will appear as figures after Figure 9 but before the TS section.  "))
search()
attach(sv2)
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, 1:35])
}
figu("Plot of the first 35 variables in the Sv files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, -(1:35)])
}
figu("Plot of variables 36+ in the Sv files.", newpage="port")
cleanup()
search()
detach()
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\ACMT Explore Works on Huron files 20 Jan 2015 JVAmody.r
load("C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/L3 Y2014 ACMT Data.RData")
explore.ac <- "yes"
explore.tr <- "yes"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
if(FALSE) {
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where output will be placed
# outputs include an Rdata file with data to be used for estimation and
# a summary of the data exploration in a Word document
# make sure you use forward slashes (/) not back slashes (\)
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22"
### Do you want to explore acoustic data? ###
explore.ac <- "yes"
# if you DON'T want to explore acoustic data, you can ignore the next several lines 
# directory where all Sv data from Echoview are stored in csv files, one file per transect
# svdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Sv/"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
# directory where all single target frequencies by TS bin are stored in csv files, one for each transect
# tsdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/TS/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
### Do you want to explore trawl data? ###
explore.tr <- "yes"
# if you DON'T want to explore trawl data, you can ignore the next several lines 
# RVCAT data files
# midwater trawl op file which includes data from both OP and TR_OP
# optrop.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron Midwater Trawl Op.csv"
optrop.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron Midwater Trawl Op.csv"
# midwater trawl catch file
# trcatch.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater catch.csv"
trcatch.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater catch.csv"
# midwater trawl tr_lf file
# trlf.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater LF.csv"
trlf.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater LF.csv"
# alewife age-length key per year
# key106.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/alewifeagelenkey.csv"
key106.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/alewifeagelenkey.csv"
}
#########################################################################################################
{### FUNCTIONS
get.packages <- function(want) {
# install (if necessary) and attach wanted packages
have <- row.names(installed.packages())
need <- want[!(want %in% have)]
if(length(need)>0) install.packages(need, repos="http://cran.r-project.org")
lapply(want, require, character.only=TRUE)
invisible()
}
get.packages(c("lubridate", "rtf", "maps"))
lakenames <- c("Superior", "Michigan", "Huron", "Erie", "Ontario")
myrecode <- function(x, old, new, must.match=T) {
partial <- match(x, old)
if(must.match) new[partial] else ifelse(!is.na(partial), new[partial], x)
}
combine.csv <- function(mydir, add.source=TRUE) {
# combine all csv files in a given directory into a single data frame
# file names
filenames <- list.files(mydir)[grep(".csv$", list.files(mydir))]
nfiles <- length(filenames)
# create an empty list where all the files will be stored
files.list <- vector(mode="list", length=nfiles)
for(i in 1:nfiles) {
# read the data into a temporary file
temp <- read.csv(paste(mydir, filenames[i], sep=""), as.is=TRUE)
# add a new column identifying the source file
if(add.source) temp$source <- filenames[i]
# put the data into the list
files.list[[i]] <- temp
}
# combined each of the files from the list into one single file
do.call(rbind, files.list)
}
plotmap <- function(lat, long, colorz, main="", pch=1, cex=1.5, rtf=TRUE, xla=0, yla=0) {
# plot longitude versus latitude with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
map("usa", xlim=range(long, na.rm=TRUE) + c(-1, 1)*xla, ylim=range(lat, na.rm=TRUE) + c(-1, 1)*xla, mar=c(0, 0, 2, 0))
points(long, lat, pch=pch, cex=cex, col=colorz)
box()
mtext(main, side=3, cex=1.2)
}
mid <- function(x) {
# calculate the midpoint between the min and the max
(max(x, na.rm=TRUE) + min(x, na.rm=TRUE))/2
}
textg <- function(lt, ln, group, cex=1.5, ...) {
# add text to plot at group lat/long midpoints
sug <- sort(unique(group))
lng <- tapply(ln, group, mid)
ltg <- tapply(lt, group, mid)
text(lng, ltg, sug, cex=cex, ...)
}
plotil <- function(colorz, main="", rtf=TRUE) {
# plot interval versus layer with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
par(mfrow=n2mfrow(length(sur)), mar=c(2, 2, 2, 1), oma=c(2, 2, 2, 0), cex=1)
for(i in order(lat.r, decreasing=T)) {
sel <- Region_name==sur[i]
plot(Interval[sel], -(Layer_depth_min + Layer_depth_max)[sel]/2, col=colorz[sel], pch=16,
ylim=range(-(Layer_depth_min + Layer_depth_max)/2, na.rm=TRUE), xlab="", ylab="", main=sur[i])
}
mtext("Interval", side=1, outer=T, line=1, cex=1.5)
mtext("Layer depth", side=2, outer=T, line=1, cex=1.5)
mtext(main, side=3, outer=T, cex=1.5)
}
plot3 <- function(lo, hi, betw, rtf=TRUE, order.matters=TRUE, varname="Varname", test=FALSE, ...) {
# plot lows, highs, and betweens for a given metric
x <- seq(hi)
if(!order.matters) {
newlo <- pmin(lo, hi, betw, na.rm=TRUE)
newhi <- pmax(lo, hi, betw, na.rm=TRUE)
lo <- newlo
hi <- newhi
}
ord <- order(hi)
lo. <- lo[ord]
be. <- betw[ord]
hi. <- hi[ord]
sel.lo <- (!is.na(lo.) & !is.na(be.) & lo. > be.) | (!is.na(lo.) & !is.na(hi.) & lo. > hi.)
sel.be <- !is.na(be.) & !is.na(hi.) & be. > hi.
problems <- sum(sum(sel.be, na.rm=TRUE), sum(sel.lo, na.rm=TRUE))
if(!test) {
yr <- range(lo., hi., be., na.rm=TRUE)
if(!rtf) windows(w=6.5, h=8.2)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(x, be., type="n", ylim=yr, las=1, xlab="Records ordered by max", ylab=varname, ...)
points(x[!sel.lo], lo.[!sel.lo], pch=6, col="blue")
points(x[!sel.be], be.[!sel.be])
points(x, hi., pch=2, col="red")
points(x[sel.lo], lo.[sel.lo], pch=6, col="cyan")
points(x[sel.be], be.[sel.be], pch=3, col="green")
legend("topleft", c(paste0("Mid > Max (", format(sum(sel.be, na.rm=TRUE), big.mark=","), " records)"), "Max", 
paste0("Min > Mid or Max (", format(sum(sel.lo, na.rm=TRUE), big.mark=","), " records)"), "Mid < Max", "Min < Mid and Max"), 
col=c("green", "red", "cyan", "black", "blue"), pch=c(3, 2, 6, 1, 6), cex=1.5)
}
if(is.na(problems) | problems < 1) return(FALSE)
}
plotsp <- function(y, ylabb, rtf=TRUE) {
if(!rtf) windows(w=6.5, h=9)
par(mar=c(4, 4, 1, 1))
plot(as.factor(Species), sqrt(y), xlab="Species", ylab=ylabb, axes=F)
axis(1, at=seq(sus), labels=sus)
axis(2, at=pretty(sqrt(y)), labels=pretty(sqrt(y))^2, las=1)
box()
}
missings <- function(df, misscodes=c(-9999, -999.9, -999, 999, 9999), misschars=c("NA", "NULL", ".", " ", "  ")) {
# assign NAs to missing value codes in numeric and character columns of data.frame
df.cur <- df
number.colz <- sapply(df, class) %in% c("integer", "numeric")
df[, number.colz] <- lapply(df[, number.colz], function(x) {
x[x %in% misscodes] <- NA
x
})
char.colz <- sapply(df, class) %in% c("character")
df[, char.colz] <- lapply(df[, char.colz, drop=FALSE], function(x) {
x[x %in% misschars] <- ""
xwe <- tryCatch.W.E(as.numeric(x))
if(is.null(xwe$warning)) x <- xwe$value
x
})
# cat("current = the original dataframe, target = the new dataframe\n")
# print(all.equal(current=df.cur, df))
df
}
qksmry <- function(df) {
no.unique <- sapply(df, function(x) length(table(x)))
no.entered <- sapply(df, function(x) if(is.character(x)) sum(x!="") else sum(!is.na(x)))
no.missing <- sapply(df, function(x) if(is.character(x)) sum(x=="") else sum(is.na(x)))
cbind(no.unique, no.entered, no.missing)
}
plot.df <- function(df, one=FALSE, mcex=1.2, cex=0.8, ...) {
if(one) windows(record=T)
par(cex=cex, ...)
# plot the columns of a data frame
for(i in 1:dim(df)[2]) {
x <- df[[i]]
if(sum(!is.na(x))>0) {
name <- names(df)[i]
if(is.factor(x)) plot(x, main=name, cex.main=mcex) else
if(is.character(x) & length(unique(x))<=50) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x) & sum(is.finite(x)) < 2) plot(1, 1, type="n", xlab="", ylab="", axes=F, 
main=paste(name, ": numeric", sep=""), cex.main=mcex) else
if(is.numeric(x) & length(unique(x))<=10) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x)) plot.default(x[is.finite(x)], main=name, cex.main=mcex) else
if(is.character(x)) plot(1, 1, type="n", xlab="", ylab="", axes=F, main=paste(name, ": character", sep=""), cex.main=mcex)
}
}
invisible()
}
plot.matrix <- function(m, inc=0.3, ...) {
# plots values of matrix as different sized circles
# dimension 1 is plotted on x-axis, dimension 2 on y-axis
uxy <- dimnames(m)
ux <- as.numeric(uxy[[1]])
uxlab <- NULL
if(any(is.na(ux))) {
ux <- seq(along=ux)
uxlab <- uxy[[1]]
}
uy <- as.numeric(uxy[[2]])
uylab <- NULL
if(any(is.na(uy))) {
uy <- seq(along=uy)
uylab <- uxy[[2]]
}
colorz <- c("cyan", "black", "red")[as.numeric(cut(as.vector(m), breaks=c(-Inf, -1e-7, 1e-7, Inf)))]
cush.x <- mean(abs(diff(ux)))
cush.y <- mean(abs(diff(uy)))
plot(ux[row(m)], uy[col(m)], type="n", xlim=range(ux) + cush.x*c(-1, 1), ylim=range(uy) + cush.y*c(-1, 1), axes=FALSE, ...)
symbols(ux[row(m)], uy[col(m)], circle=abs(as.vector(m)), inches=inc, fg=colorz, add=TRUE)
if(is.null(uxlab)) axis(1) else axis(1, at=ux, labels=uxlab)
if(is.null(uylab)) axis(2, las=1) else axis(2, at=uy, labels=uylab, las=1)
box()
}
tryCatch.W.E <- function(expr) {
# Martin Maechler, ETH Zurich, 9 Dec 2010, R-help subject: "How to catch both warnings and errors?"
W <- NULL
w.handler <- function(w) {
W <<- w
invokeRestart("muffleWarning")
}
list(value=withCallingHandlers(tryCatch(expr, error=function(e) e), warning=w.handler), warning=W)
}
scale.02n <- function(x, n=100) {
# scale a collection of values from 0 to some specified maximum
n*as.numeric(scale(x, center=min(x, na.rm=TRUE), scale=diff(range(x, na.rm=TRUE))))
}
rain.n <- function(x, n=100, start=0/6, end=4/6) {
# assign a specified number of rainbow colors to a collection of values
# default color range from blue (4/6) to red (0/6)
y <- round(scale.02n(x, n=n-1)) + 1
# reverse the direction of y (e.g., from 1:100 to 100:1)
y <- n + 1 - y
rainbow(n, start=start, end=end)[y]
}
start.rtf <- function(file=NULL, dir=getwd(), width=8.5, height=11, font.size=12, omi=c(1, 1, 1, 1), quiet=FALSE) {
# create a new RTF file readable by Word
# create two new variables to keep count of tables and figures
tabcount <<- 1
figcount <<- 1
if(is.null(file)) file <- paste0("RGeneratedDocument", Sys.Date())
dirfiledoc <- if(length(grep(".doc", file))>0) paste(dir, file, sep="/") else paste(dir, paste0(file, ".doc"), sep="/")
if(!quiet) cat(paste0("New RTF document created, ", dirfiledoc, "\n"))
RTF(dirfiledoc, width=width, height=height, font.size=font.size, omi=omi)
}
head1 <- function(words, rtf=doc, font.size=16) {
addHeader(this=rtf, title=words)
}
head2 <- function(words, rtf=doc, bold=TRUE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, words, bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
head3 <- function(..., rtf=doc, bold=FALSE, italic=TRUE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
para <- function(..., rtf=doc, bold=FALSE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
tabl <- function(..., TAB=tab, rtf=doc, fontt=8, row.names=TRUE, tabc=tabcount, boldt=TRUE, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
startParagraph(this=rtf)
addText(this=rtf, paste0("Table ", tabc, ".  "), bold=boldt)
addText(this=rtf, ...)
addNewLine(this=rtf)
endParagraph(this=rtf)
addTable(this=rtf, TAB, font.size=fontt, row.names=row.names)
addNewLine(this=rtf)
addNewLine(this=rtf)
tabcount <<- tabc + 1
}
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, wf=6.5, hf=8.2, rf=300, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
end.rtf <- function(rtf=doc, details=TRUE, ...) {
addPageBreak(rtf, ...)
addSessionInfo(rtf)
done(rtf)
}
}
if(FALSE) {
### CRUNCHING
if(casefold(substring(explore.ac, 1, 1))=="y") { # read in AC data
# 1.  Import data
# Concatenate all the files of type csv in a particular folder into one file for Sv, one for TS.  
# Trawl data will already be in one file per type of data (catch, lf, op).  
# Need to make R parse out year from the sampling date field.
### A.  Sv data from Echoview, one file per transect
sv <- combine.csv(svdir)
# create date variables that R recognizes as dates
sv$date.s <- ymd(sv$Date_S, quiet=TRUE)
sv$date.e <- ymd(sv$Date_E, quiet=TRUE)
sv$date.m <- ymd(sv$Date_M, quiet=TRUE)
# create variable for year
sv$year <- year(sv$date.m)
# assign NAs to missing value codes
sv2 <- missings(sv)
# eliminate records with missing values for Lat_M, Sv_mean, or PRC_ABC
sv2 <- sv2[!(is.na(sv2$Lat_M) | is.na(sv2$Sv_mean) | is.na(sv2$PRC_ABC)), ]
### B.  Single target frequencies by TS bin, one for each transect
ts <- combine.csv(tsdir)
# create date variables that R recognizes as dates
ts$date.s <- ymd(ts$Date_S, quiet=TRUE)
ts$date.e <- ymd(ts$Date_E, quiet=TRUE)
ts$date.m <- ymd(ts$Date_M, quiet=TRUE)
# create variable for year
ts$year <- year(ts$date.m)
# assign NAs to missing value codes
ts2 <- missings(ts)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# read in trawl data
# RVCAT data
optrop <- read.csv(optrop.f, as.is=TRUE, na.strings="NULL")
trcatch <- read.csv(trcatch.f, as.is=TRUE, na.strings="NULL")
trlf <- read.csv(trlf.f, as.is=TRUE, na.strings="NULL")
key106 <- read.csv(key106.f, as.is=TRUE)
}
}
{# exploratory plots - save output to *.doc file
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Explore ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
explore <- 10*(casefold(substring(explore.ac, 1, 1))=="y") + 1*(casefold(substring(explore.tr, 1, 1))=="y")
descr <- myrecode(explore, c(0, 1, 10, 11), c("No", "Trawl", "Acoustic", "Acoustic and Trawl"))
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Exploration of ", descr, " Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
rm(descr, docname)
}
if(casefold(substring(explore.ac, 1, 1))=="y") {# print Sv and TS info to doc
para(paste0("svdir = ", svdir, " = Sv data directory."))
para(paste0("tsdir = ", tsdir, " = single target frequencies data directory."))
}
para(paste0("maindir = ", maindir, " = main input/output directory."))
if(casefold(substring(explore.ac, 1, 1))=="y") {# explore Sv and TS files
### Sv
head2("SV FILES")
para(paste0("The Sv files have ", dim(sv2)[1], " rows and ", dim(sv2)[2], " columns."))
tab <- qksmry(sv2)
tabl("Quick summary table of variables in Sv files.")
para(paste0("Figures 1-9 are exploratory plots of the Sv data that can be examined to check for potential problems.  ",
"Any figures after that (but before the TS section) indicate problems in the data.  ",
"Minimum and maximum values were compared with middle or mean values for each of the following variables: Sv, Depth, Ping, Dist, Date, Lat, Long.  ",
"If any problems were indicated by these comparisons, they will appear as figures after Figure 9 but before the TS section.  "))
attach(sv2)
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, 1:35])
}
figu("Plot of the first 35 variables in the Sv files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, -(1:35)])
}
figu("Plot of variables 36+ in the Sv files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Lat_M, Lon_M, as.numeric(as.factor(Region_name)), "Colors indicate Region")
textg(Lat_M, Lon_M, Region_name)
}
figu("Location of transects in Sv files.  Colors indicate Region.", newpage="port")
# close up look at each transect - lat/long
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=n2mfrow(length(sur)), mar=c(1, 0, 2, 0), cex=1.2)
for(i in order(lat.r, decreasing=F)) {
sel <- Region_name==sur[i]
map("usa", xlim=range(Lon_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, ylim=range(Lat_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, mar=c(1, 0, 2, 0), type="n")
points(Lon_M[sel], Lat_M[sel], col=as.numeric(as.factor(Region_name))[sel])
mtext(sur[i], side=3, cex=1.5)
box()
}
}
figu("Close up look at each transect location in Sv files.", newpage="port")
# interval by layer plots
fig <- function() plotil(rain.n(-Depth_mean), "Colors indicate Depth_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Depth_mean.", newpage="port")
fig <- function() plotil(rain.n(Sv_mean), "Colors indicate Sv_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Sv_mean.", newpage="port")
fig <- function() plotil(rain.n(PRC_ABC^0.2), "Colors indicate PRC_ABC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_ABC.", newpage="port")
fig <- function() plotil(rain.n(PRC_NASC^0.2), "Colors indicate PRC_NASC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_NASC.", newpage="port")
fig <- function() plotil(rain.n(Samples), "Colors indicate Samples")
figu("Interval by layer plots for Sv files.  Colors indicate Samples.", newpage="port")
search()
head(sv2)
head(sv2)
stringin("samp", names(sv2))
fig <- function() plotil(rain.n(Samples_In_Domain), "Colors indicate Samples")
figu("Interval by layer plots for Sv files.  Colors indicate Samples.", newpage="port")
# plots comparing extremes with middle values
fig <- function() plot3(Ping_S, Ping_E, Ping_M, varname="Ping")
np <- plot3(Ping_S, Ping_E, Ping_M, test=TRUE)
if(is.null(np)) figu("Comparing Ping_S, Ping_E, and Ping_M for Sv files.", newpage="port")
fig <- function() plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, varname="Dist")
np <- plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Dist_S, Dist_E, and Dist_M for Sv files.", newpage="port")
fig <- function() plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), varname="Date")
np <- plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), test=TRUE)
if(is.null(np)) figu("Comparing Date_S, Date_E, and Date_M for Sv files.", newpage="port")
fig <- function() plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, varname="Latitude")
np <- plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lat_S, Lat_E, and Lat_M for Sv files.", newpage="port")
fig <- function() plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, varname="Longitude")
np <- plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lon_S, Lon_E, and Lon_M for Sv files.", newpage="port")
detach(sv2)
rm(sur, lat.r, lon.r, np)
head2("TS FILES")
para(paste0("The TS files have ", dim(ts2)[1], " rows and ", dim(ts2)[2], " columns."))
tab <- qksmry(ts2)
tabl("Quick summary table of variables in TS files.")
attach(ts2)
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 1:28])
}
figu("Plot of the first 28 variables in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 28+(1:28)])
}
figu("Plot of variables 29-56 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 56+(1:28)])
}
figu("Plot of variables 57-84 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, -(1:84)])
}
figu("Plot of variables 85+ in the TS files.", newpage="port")
# interval by layer plots
start <- seq(16, 66, 10)
end <- start+10
for(i in seq(along=start)) {
colz <- paste("X.", start[i]:end[i], ".000000", sep="")
sumtargs <- apply(ts2[, colz], 1, sum)
title. <- paste("Colors indicate binned targets from, -", end[i], " to -", start[i], " dB", sep="")
fig <- function() plotil(rain.n(sqrt(sumtargs)), title.)
figu(paste("Interval by layer plots for TS files. ", title.), newpage="port")
}
detach(ts2)
rm(sur, lon.r, lat.r, start, end, i, colz, sumtargs, title.)
#####################################################################################################################################################
# compare interval gaps in Sv and TS files
head2("SV and TS FILES COMPARISON")
t1 <- table(sv2$Interval, sv2$Region_name)
t2 <- table(ts2$Interval, ts2$Region_name)
# create matrices with all the intervals and all the regions
iu <- union(rownames(t1), rownames(t2))
iu <- iu[order(as.numeric(iu))]
ju <- union(colnames(t1), colnames(t2))
bigt1 <- matrix(0, nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
bigt2 <- bigt1
bigt1[rownames(t1), colnames(t1)] <- t1
bigt2[rownames(t2), colnames(t2)] <- t2
results <- matrix("", nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
# assign values when rows/columns don't match
results[bigt1 < 0.5 & bigt2 > 0.5] <- "Gap in Sv"
results[bigt1 > 0.5 & bigt2 < 0.5] <- "Gap in TS"
res <- results[apply(results!="", 1, sum) > 0, apply(results!="", 2, sum) > 0]
if(sum(dim(res)) > 0) {
tab <- res
tabl("Interval gaps in Sv and TS files don't match up.")
} else {
para("Interval gaps in Sv and TS files match up.")
}
rm(t1, t2, iu, ju, bigt1, bigt2, results, res)
casefold(substring(explore.tr, 1, 1))=="y"
### OPTROP
head2("OP and TROP FILES")
optrop2 <- missings(optrop)
para(paste0("The OP/TROP files have ", dim(optrop2)[1], " rows and ", dim(optrop2)[2], " columns."))
tab <- qksmry(optrop2)
tabl("Quick summary table of variables in OP/TROP files.")
attach(optrop2)
pcols <- c("Op.Id", "Vessel", "Cruise", "Serial", "Lake", "Port", 
"Beg.Depth", "End.Depth", "Distance", "Fishing_Temp", "Fishing_Depth", "Transect")
head(optrop2[, pcols])
names(optrop2)
tab <- optrop2[is.na(Beg.Depth) | is.na(End.Depth) | is.na(Distance) | is.na(Fishing_Temp), pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with missing depth, distance, or temperature.", newpage="land")
} else {
para("All OP/TROP records have depth, distance, and temperature entered.")
}
set.time <- floor(Set_Time/100) + (Set_Time - 100*floor(Set_Time/100))/60
tod <- rep("night", length(set.time))
tod[set.time > 7 & set.time < 19] <- "day"
tt <- table(tod)
mostall <- names(which.max(table(tod)))
if(length(tt)<1.5) {
if(mostall=="night") {
para("All OP/TROP records were taken at night.")
} else {
para("All OP/TROP records were taken during the day.")
}
} else {
if(mostall=="night") {
tab <- optrop2[tod=="day", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken at night, but some were taken during the day.")
} else {
tab <- optrop2[tod=="night", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken during the day, but some were taken at night.")
}
}
tab <- optrop2[!is.na(Beg.Depth) & !is.na(End.Depth) & abs(Beg.Depth - End.Depth) > 20, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with > 20 m difference between beginning and ending bottom depth.")
} else {
para("All OP/TROP records have < 20 m difference between beginning and ending bottom depth.")
}
mind <- pmin(Beg.Depth, End.Depth, na.rm=T)
tab <- optrop2[!is.na(mind) & !is.na(Fishing_Depth) & Fishing_Depth > mind, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with fishing depth > beginning or ending bottom depth.")
} else {
para("All OP/TROP records have fishing depths < beginning and ending bottom depths.")
}
fig <- function() {
par(mfrow=c(6, 4), mar=c(3, 3, 2, 1))
plot.df(optrop2)
}
figu("Plot of variables in the OP/TROP files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Port))), "Colors indicate Port", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Port)
}
figu("Identification of ports in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Cruise))), "Colors indicate Cruise", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Cruise)
}
figu("Identification of cruises in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Transect))), "Colors indicate Transect", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Transect)
}
figu("Identification of transects in OP/TROP files.", newpage="port")
maxd <- pmax(Beg.Depth, End.Depth, na.rm=T)
fig <- function() {
plotmap(Latitude, Longitude, rain.n(-maxd), "Colors indicate Bottom Depth", pch=16, xla=0.15, yla=0.1)
}
figu("Bottom depths in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(Tow_Time), "Colors indicate Tow_Time", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tow_Time)
}
figu("Identification of tow times in OP/TROP files.", newpage="port")
names(optrop2)
stringin("tr", names(optrop2))
stringin("des", names(optrop2))
if("Tr_Design" %in% names(optrop2)) {
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Tr_Design))), "Colors indicate Tr_Design", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tr_Design)
}
figu("Identification of trawl designs in OP/TROP files.", newpage="port")
}
detach(optrop2)
rm(set.time, tod, tt, mostall, mind, maxd)
### trcatch
addPageBreak(doc, width=11, height=8.5)
head2("TRCATCH FILE")
trcatch2 <- missings(trcatch)
para(paste0("The TRCATCH file has ", dim(trcatch2)[1], " rows and ", dim(trcatch2)[2], " columns."))
tab <- qksmry(trcatch2)
tabl("Quick summary table of variables in TRCATCH file.")
attach(trcatch2)
sus <- sort(unique(Species))
if("Beg.Depth" %in% names(trcatch2)) {
tab <- trcatch2[is.na(Beg.Depth) | is.na(End.Depth), 
c("Op.Id", "Year", "Vessel", "Serial", "Lake", "Species", "Port_Name", "Beg.Depth", "End.Depth", "N")]
if(dim(tab)[1] > 0) {
tabl("TRCATCH records with missing beginning or ending depth.")
} else {
para("All TRCATCH records have beginning and ending depth entered.")
}
}
fig <- function() {
par(mfrow=c(5, 4), mar=c(3, 3, 2, 1))
plot.df(trcatch2)
plotsp(N, "N")
plotsp(Weight, "Weight")
plotsp(Weight/N, "Weight/N")
}
figu("Plot of variables in the TRCATCH file.", newpage="port")
detach(trcatch2)
rm(sus)
### trlf
head2("TRLF FILE")
trlf2 <- missings(trlf)
para(paste0("The TRCATCH file has ", dim(trlf2)[1], " rows and ", dim(trlf2)[2], " columns."))
tab <- qksmry(trlf2)
tabl("Quick summary table of variables in TRLF file.")
attach(trlf)
fig <- function() {
par(mfrow=c(5, 3), mar=c(3, 3, 2, 1))
plot.df(trlf)
}
figu("Plot of variables in the TRLF file.", newpage="port")
sus <- sort(unique(Species))
fig <- function() {
par(mfrow=n2mfrow(length(sus)), mar=c(3, 3, 2, 1), oma=c(2, 2, 1, 1), yaxs="i", cex=1)
for(i in seq(along=sus)) {
sel <- Species==sus[i]
a <- hist(rep(Length[sel], N[sel]), breaks=seq(-5, max(Length)+5, 5), plot=FALSE)
xr <- range(Length[sel])
hist(rep(Length[sel], N[sel]), xlim=xr+10*c(-1, 1), ylim=c(0, max(a$counts))*1.05, 
breaks=seq(-5, max(Length)+5, 5), col="blue", main=sus[i], las=1)
abline(v=xr, lwd=2, col="red")
box()
}
mtext("Length  (mm)", side=1, outer=TRUE, cex=1.5)
mtext("Frequency", side=2, outer=TRUE, cex=1.5)
}
figu("Length frequency histograms of species in the TRLF file.  Vertical red lines indicate the minimum and maximum lengths recorded.", 
newpage="port")
detach(trlf)
rm(trlf2, sus) #, i, sel, sul, a, xr)
### key106
head2("Alewife Age-Length Key")
m <- key106[-25, -(1:2)]
dimnames(m)[[1]] <- key106$mmgroup[-25]
dimnames(m)[[2]] <- substring(dimnames(m)[[2]], 4, 4)
m <- as.matrix(m)
m2 <- m[apply(m, 1, sum) > 0, apply(m, 2, sum) > 0]
para(paste0("The alewife age-length key has ", dim(m2)[1], " length categories and ", dim(m2)[2], " age categories."))
tab <- m2
tabl("Alewife age-length key.")
fig <- function() {
par(mar=c(4, 4, 2, 1), cex=1.5)
plot.matrix(m2, inc=0.2, xlab="Length  (mm)", ylab="Age", main="Alewife Age-Length Key")
}
figu("Alewife age-length key.  Circle size is proportional to probability of age, given length.",
"  Probabilities for all ages of a given length sum to one.", newpage="port")
rm(m, m2)
end.rtf()
cleanup()
search()
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\ACMT Explore Works on Huron files 20 Jan 2015 JVAmody.r
load("C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/L3 Y2014 ACMT Data Daves Original.RData")
explore.ac <- "yes"
explore.tr <- "yes"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22"
if(FALSE) {
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where output will be placed
# outputs include an Rdata file with data to be used for estimation and
# a summary of the data exploration in a Word document
# make sure you use forward slashes (/) not back slashes (\)
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22"
### Do you want to explore acoustic data? ###
explore.ac <- "yes"
# if you DON'T want to explore acoustic data, you can ignore the next several lines 
# directory where all Sv data from Echoview are stored in csv files, one file per transect
# svdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Sv/"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
# directory where all single target frequencies by TS bin are stored in csv files, one for each transect
# tsdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/TS/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
### Do you want to explore trawl data? ###
explore.tr <- "yes"
# if you DON'T want to explore trawl data, you can ignore the next several lines 
# RVCAT data files
# midwater trawl op file which includes data from both OP and TR_OP
# optrop.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron Midwater Trawl Op.csv"
optrop.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron Midwater Trawl Op.csv"
# midwater trawl catch file
# trcatch.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater catch.csv"
trcatch.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater catch.csv"
# midwater trawl tr_lf file
# trlf.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater LF.csv"
trlf.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/Lake Huron midwater LF.csv"
# alewife age-length key per year
# key106.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/alewifeagelenkey.csv"
key106.f <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/alewifeagelenkey.csv"
}
#########################################################################################################
{### FUNCTIONS
get.packages <- function(want) {
# install (if necessary) and attach wanted packages
have <- row.names(installed.packages())
need <- want[!(want %in% have)]
if(length(need)>0) install.packages(need, repos="http://cran.r-project.org")
lapply(want, require, character.only=TRUE)
invisible()
}
get.packages(c("lubridate", "rtf", "maps"))
lakenames <- c("Superior", "Michigan", "Huron", "Erie", "Ontario")
myrecode <- function(x, old, new, must.match=T) {
partial <- match(x, old)
if(must.match) new[partial] else ifelse(!is.na(partial), new[partial], x)
}
combine.csv <- function(mydir, add.source=TRUE) {
# combine all csv files in a given directory into a single data frame
# file names
filenames <- list.files(mydir)[grep(".csv$", list.files(mydir))]
nfiles <- length(filenames)
# create an empty list where all the files will be stored
files.list <- vector(mode="list", length=nfiles)
for(i in 1:nfiles) {
# read the data into a temporary file
temp <- read.csv(paste(mydir, filenames[i], sep=""), as.is=TRUE)
# add a new column identifying the source file
if(add.source) temp$source <- filenames[i]
# put the data into the list
files.list[[i]] <- temp
}
# combined each of the files from the list into one single file
do.call(rbind, files.list)
}
plotmap <- function(lat, long, colorz, main="", pch=1, cex=1.5, rtf=TRUE, xla=0, yla=0) {
# plot longitude versus latitude with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
map("usa", xlim=range(long, na.rm=TRUE) + c(-1, 1)*xla, ylim=range(lat, na.rm=TRUE) + c(-1, 1)*xla, mar=c(0, 0, 2, 0))
points(long, lat, pch=pch, cex=cex, col=colorz)
box()
mtext(main, side=3, cex=1.2)
}
mid <- function(x) {
# calculate the midpoint between the min and the max
(max(x, na.rm=TRUE) + min(x, na.rm=TRUE))/2
}
textg <- function(lt, ln, group, cex=1.5, ...) {
# add text to plot at group lat/long midpoints
sug <- sort(unique(group))
lng <- tapply(ln, group, mid)
ltg <- tapply(lt, group, mid)
text(lng, ltg, sug, cex=cex, ...)
}
plotil <- function(colorz, main="", rtf=TRUE) {
# plot interval versus layer with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
par(mfrow=n2mfrow(length(sur)), mar=c(2, 2, 2, 1), oma=c(2, 2, 2, 0), cex=1)
for(i in order(lat.r, decreasing=T)) {
sel <- Region_name==sur[i]
plot(Interval[sel], -(Layer_depth_min + Layer_depth_max)[sel]/2, col=colorz[sel], pch=16,
ylim=range(-(Layer_depth_min + Layer_depth_max)/2, na.rm=TRUE), xlab="", ylab="", main=sur[i])
}
mtext("Interval", side=1, outer=T, line=1, cex=1.5)
mtext("Layer depth", side=2, outer=T, line=1, cex=1.5)
mtext(main, side=3, outer=T, cex=1.5)
}
plot3 <- function(lo, hi, betw, rtf=TRUE, order.matters=TRUE, varname="Varname", test=FALSE, ...) {
# plot lows, highs, and betweens for a given metric
x <- seq(hi)
if(!order.matters) {
newlo <- pmin(lo, hi, betw, na.rm=TRUE)
newhi <- pmax(lo, hi, betw, na.rm=TRUE)
lo <- newlo
hi <- newhi
}
ord <- order(hi)
lo. <- lo[ord]
be. <- betw[ord]
hi. <- hi[ord]
sel.lo <- (!is.na(lo.) & !is.na(be.) & lo. > be.) | (!is.na(lo.) & !is.na(hi.) & lo. > hi.)
sel.be <- !is.na(be.) & !is.na(hi.) & be. > hi.
problems <- sum(sum(sel.be, na.rm=TRUE), sum(sel.lo, na.rm=TRUE))
if(!test) {
yr <- range(lo., hi., be., na.rm=TRUE)
if(!rtf) windows(w=6.5, h=8.2)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(x, be., type="n", ylim=yr, las=1, xlab="Records ordered by max", ylab=varname, ...)
points(x[!sel.lo], lo.[!sel.lo], pch=6, col="blue")
points(x[!sel.be], be.[!sel.be])
points(x, hi., pch=2, col="red")
points(x[sel.lo], lo.[sel.lo], pch=6, col="cyan")
points(x[sel.be], be.[sel.be], pch=3, col="green")
legend("topleft", c(paste0("Mid > Max (", format(sum(sel.be, na.rm=TRUE), big.mark=","), " records)"), "Max", 
paste0("Min > Mid or Max (", format(sum(sel.lo, na.rm=TRUE), big.mark=","), " records)"), "Mid < Max", "Min < Mid and Max"), 
col=c("green", "red", "cyan", "black", "blue"), pch=c(3, 2, 6, 1, 6), cex=1.5)
}
if(is.na(problems) | problems < 1) return(FALSE)
}
plotsp <- function(y, ylabb, rtf=TRUE) {
if(!rtf) windows(w=6.5, h=9)
par(mar=c(4, 4, 1, 1))
plot(as.factor(Species), sqrt(y), xlab="Species", ylab=ylabb, axes=F)
axis(1, at=seq(sus), labels=sus)
axis(2, at=pretty(sqrt(y)), labels=pretty(sqrt(y))^2, las=1)
box()
}
missings <- function(df, misscodes=c(-9999, -999.9, -999, 999, 9999), misschars=c("NA", "NULL", ".", " ", "  ")) {
# assign NAs to missing value codes in numeric and character columns of data.frame
df.cur <- df
number.colz <- sapply(df, class) %in% c("integer", "numeric")
df[, number.colz] <- lapply(df[, number.colz], function(x) {
x[x %in% misscodes] <- NA
x
})
char.colz <- sapply(df, class) %in% c("character")
df[, char.colz] <- lapply(df[, char.colz, drop=FALSE], function(x) {
x[x %in% misschars] <- ""
xwe <- tryCatch.W.E(as.numeric(x))
if(is.null(xwe$warning)) x <- xwe$value
x
})
# cat("current = the original dataframe, target = the new dataframe\n")
# print(all.equal(current=df.cur, df))
df
}
qksmry <- function(df) {
no.unique <- sapply(df, function(x) length(table(x)))
no.entered <- sapply(df, function(x) if(is.character(x)) sum(x!="") else sum(!is.na(x)))
no.missing <- sapply(df, function(x) if(is.character(x)) sum(x=="") else sum(is.na(x)))
cbind(no.unique, no.entered, no.missing)
}
plot.df <- function(df, one=FALSE, mcex=1.2, cex=0.8, ...) {
if(one) windows(record=T)
par(cex=cex, ...)
# plot the columns of a data frame
for(i in 1:dim(df)[2]) {
x <- df[[i]]
if(sum(!is.na(x))>0) {
name <- names(df)[i]
if(is.factor(x)) plot(x, main=name, cex.main=mcex) else
if(is.character(x) & length(unique(x))<=50) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x) & sum(is.finite(x)) < 2) plot(1, 1, type="n", xlab="", ylab="", axes=F, 
main=paste(name, ": numeric", sep=""), cex.main=mcex) else
if(is.numeric(x) & length(unique(x))<=10) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x)) plot.default(x[is.finite(x)], main=name, cex.main=mcex) else
if(is.character(x)) plot(1, 1, type="n", xlab="", ylab="", axes=F, main=paste(name, ": character", sep=""), cex.main=mcex)
}
}
invisible()
}
plot.matrix <- function(m, inc=0.3, ...) {
# plots values of matrix as different sized circles
# dimension 1 is plotted on x-axis, dimension 2 on y-axis
uxy <- dimnames(m)
ux <- as.numeric(uxy[[1]])
uxlab <- NULL
if(any(is.na(ux))) {
ux <- seq(along=ux)
uxlab <- uxy[[1]]
}
uy <- as.numeric(uxy[[2]])
uylab <- NULL
if(any(is.na(uy))) {
uy <- seq(along=uy)
uylab <- uxy[[2]]
}
colorz <- c("cyan", "black", "red")[as.numeric(cut(as.vector(m), breaks=c(-Inf, -1e-7, 1e-7, Inf)))]
cush.x <- mean(abs(diff(ux)))
cush.y <- mean(abs(diff(uy)))
plot(ux[row(m)], uy[col(m)], type="n", xlim=range(ux) + cush.x*c(-1, 1), ylim=range(uy) + cush.y*c(-1, 1), axes=FALSE, ...)
symbols(ux[row(m)], uy[col(m)], circle=abs(as.vector(m)), inches=inc, fg=colorz, add=TRUE)
if(is.null(uxlab)) axis(1) else axis(1, at=ux, labels=uxlab)
if(is.null(uylab)) axis(2, las=1) else axis(2, at=uy, labels=uylab, las=1)
box()
}
tryCatch.W.E <- function(expr) {
# Martin Maechler, ETH Zurich, 9 Dec 2010, R-help subject: "How to catch both warnings and errors?"
W <- NULL
w.handler <- function(w) {
W <<- w
invokeRestart("muffleWarning")
}
list(value=withCallingHandlers(tryCatch(expr, error=function(e) e), warning=w.handler), warning=W)
}
scale.02n <- function(x, n=100) {
# scale a collection of values from 0 to some specified maximum
n*as.numeric(scale(x, center=min(x, na.rm=TRUE), scale=diff(range(x, na.rm=TRUE))))
}
rain.n <- function(x, n=100, start=0/6, end=4/6) {
# assign a specified number of rainbow colors to a collection of values
# default color range from blue (4/6) to red (0/6)
y <- round(scale.02n(x, n=n-1)) + 1
# reverse the direction of y (e.g., from 1:100 to 100:1)
y <- n + 1 - y
rainbow(n, start=start, end=end)[y]
}
start.rtf <- function(file=NULL, dir=getwd(), width=8.5, height=11, font.size=12, omi=c(1, 1, 1, 1), quiet=FALSE) {
# create a new RTF file readable by Word
# create two new variables to keep count of tables and figures
tabcount <<- 1
figcount <<- 1
if(is.null(file)) file <- paste0("RGeneratedDocument", Sys.Date())
dirfiledoc <- if(length(grep(".doc", file))>0) paste(dir, file, sep="/") else paste(dir, paste0(file, ".doc"), sep="/")
if(!quiet) cat(paste0("New RTF document created, ", dirfiledoc, "\n"))
RTF(dirfiledoc, width=width, height=height, font.size=font.size, omi=omi)
}
head1 <- function(words, rtf=doc, font.size=16) {
addHeader(this=rtf, title=words)
}
head2 <- function(words, rtf=doc, bold=TRUE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, words, bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
head3 <- function(..., rtf=doc, bold=FALSE, italic=TRUE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
para <- function(..., rtf=doc, bold=FALSE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
tabl <- function(..., TAB=tab, rtf=doc, fontt=8, row.names=TRUE, tabc=tabcount, boldt=TRUE, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
startParagraph(this=rtf)
addText(this=rtf, paste0("Table ", tabc, ".  "), bold=boldt)
addText(this=rtf, ...)
addNewLine(this=rtf)
endParagraph(this=rtf)
addTable(this=rtf, TAB, font.size=fontt, row.names=row.names)
addNewLine(this=rtf)
addNewLine(this=rtf)
tabcount <<- tabc + 1
}
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, wf=6.5, hf=8.2, rf=300, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
end.rtf <- function(rtf=doc, details=TRUE, ...) {
addPageBreak(rtf, ...)
addSessionInfo(rtf)
done(rtf)
}
}
if(FALSE) {
### CRUNCHING
if(casefold(substring(explore.ac, 1, 1))=="y") { # read in AC data
# 1.  Import data
# Concatenate all the files of type csv in a particular folder into one file for Sv, one for TS.  
# Trawl data will already be in one file per type of data (catch, lf, op).  
# Need to make R parse out year from the sampling date field.
### A.  Sv data from Echoview, one file per transect
sv <- combine.csv(svdir)
# create date variables that R recognizes as dates
sv$date.s <- ymd(sv$Date_S, quiet=TRUE)
sv$date.e <- ymd(sv$Date_E, quiet=TRUE)
sv$date.m <- ymd(sv$Date_M, quiet=TRUE)
# create variable for year
sv$year <- year(sv$date.m)
# assign NAs to missing value codes
sv2 <- missings(sv)
# eliminate records with missing values for Lat_M, Sv_mean, or PRC_ABC
sv2 <- sv2[!(is.na(sv2$Lat_M) | is.na(sv2$Sv_mean) | is.na(sv2$PRC_ABC)), ]
### B.  Single target frequencies by TS bin, one for each transect
ts <- combine.csv(tsdir)
# create date variables that R recognizes as dates
ts$date.s <- ymd(ts$Date_S, quiet=TRUE)
ts$date.e <- ymd(ts$Date_E, quiet=TRUE)
ts$date.m <- ymd(ts$Date_M, quiet=TRUE)
# create variable for year
ts$year <- year(ts$date.m)
# assign NAs to missing value codes
ts2 <- missings(ts)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# read in trawl data
# RVCAT data
optrop <- read.csv(optrop.f, as.is=TRUE, na.strings="NULL")
trcatch <- read.csv(trcatch.f, as.is=TRUE, na.strings="NULL")
trlf <- read.csv(trlf.f, as.is=TRUE, na.strings="NULL")
key106 <- read.csv(key106.f, as.is=TRUE)
}
}
{# exploratory plots - save output to *.doc file
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Explore ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
explore <- 10*(casefold(substring(explore.ac, 1, 1))=="y") + 1*(casefold(substring(explore.tr, 1, 1))=="y")
descr <- myrecode(explore, c(0, 1, 10, 11), c("No", "Trawl", "Acoustic", "Acoustic and Trawl"))
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Exploration of ", descr, " Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
rm(descr, docname)
}
if(casefold(substring(explore.ac, 1, 1))=="y") {# print Sv and TS info to doc
para(paste0("svdir = ", svdir, " = Sv data directory."))
para(paste0("tsdir = ", tsdir, " = single target frequencies data directory."))
}
para(paste0("maindir = ", maindir, " = main input/output directory."))
if(casefold(substring(explore.ac, 1, 1))=="y") {# explore Sv and TS files
### Sv
head2("SV FILES")
para(paste0("The Sv files have ", dim(sv2)[1], " rows and ", dim(sv2)[2], " columns."))
tab <- qksmry(sv2)
tabl("Quick summary table of variables in Sv files.")
para(paste0("Figures 1-9 are exploratory plots of the Sv data that can be examined to check for potential problems.  ",
"Any figures after that (but before the TS section) indicate problems in the data.  ",
"Minimum and maximum values were compared with middle or mean values for each of the following variables: Sv, Depth, Ping, Dist, Date, Lat, Long.  ",
"If any problems were indicated by these comparisons, they will appear as figures after Figure 9 but before the TS section.  "))
attach(sv2)
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, 1:35])
}
figu("Plot of the first 35 variables in the Sv files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, -(1:35)])
}
figu("Plot of variables 36+ in the Sv files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Lat_M, Lon_M, as.numeric(as.factor(Region_name)), "Colors indicate Region")
textg(Lat_M, Lon_M, Region_name)
}
figu("Location of transects in Sv files.  Colors indicate Region.", newpage="port")
# close up look at each transect - lat/long
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=n2mfrow(length(sur)), mar=c(1, 0, 2, 0), cex=1.2)
for(i in order(lat.r, decreasing=F)) {
sel <- Region_name==sur[i]
map("usa", xlim=range(Lon_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, ylim=range(Lat_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, mar=c(1, 0, 2, 0), type="n")
points(Lon_M[sel], Lat_M[sel], col=as.numeric(as.factor(Region_name))[sel])
mtext(sur[i], side=3, cex=1.5)
box()
}
}
figu("Close up look at each transect location in Sv files.", newpage="port")
# interval by layer plots
fig <- function() plotil(rain.n(-Depth_mean), "Colors indicate Depth_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Depth_mean.", newpage="port")
fig <- function() plotil(rain.n(Sv_mean), "Colors indicate Sv_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Sv_mean.", newpage="port")
fig <- function() plotil(rain.n(PRC_ABC^0.2), "Colors indicate PRC_ABC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_ABC.", newpage="port")
fig <- function() plotil(rain.n(PRC_NASC^0.2), "Colors indicate PRC_NASC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_NASC.", newpage="port")
fig <- function() plotil(rain.n(Samples_In_Domain), "Colors indicate Samples")
figu("Interval by layer plots for Sv files.  Colors indicate Samples.", newpage="port")
# plots comparing extremes with middle values
fig <- function() plot3(Ping_S, Ping_E, Ping_M, varname="Ping")
np <- plot3(Ping_S, Ping_E, Ping_M, test=TRUE)
if(is.null(np)) figu("Comparing Ping_S, Ping_E, and Ping_M for Sv files.", newpage="port")
fig <- function() plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, varname="Dist")
np <- plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Dist_S, Dist_E, and Dist_M for Sv files.", newpage="port")
fig <- function() plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), varname="Date")
np <- plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), test=TRUE)
if(is.null(np)) figu("Comparing Date_S, Date_E, and Date_M for Sv files.", newpage="port")
fig <- function() plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, varname="Latitude")
np <- plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lat_S, Lat_E, and Lat_M for Sv files.", newpage="port")
fig <- function() plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, varname="Longitude")
np <- plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lon_S, Lon_E, and Lon_M for Sv files.", newpage="port")
detach(sv2)
rm(sur, lat.r, lon.r, np)
### TS
head2("TS FILES")
para(paste0("The TS files have ", dim(ts2)[1], " rows and ", dim(ts2)[2], " columns."))
tab <- qksmry(ts2)
tabl("Quick summary table of variables in TS files.")
attach(ts2)
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 1:28])
}
figu("Plot of the first 28 variables in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 28+(1:28)])
}
figu("Plot of variables 29-56 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 56+(1:28)])
}
figu("Plot of variables 57-84 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, -(1:84)])
}
figu("Plot of variables 85+ in the TS files.", newpage="port")
# interval by layer plots
start <- seq(16, 66, 10)
end <- start+10
for(i in seq(along=start)) {
colz <- paste("X.", start[i]:end[i], ".000000", sep="")
sumtargs <- apply(ts2[, colz], 1, sum)
title. <- paste("Colors indicate binned targets from, -", end[i], " to -", start[i], " dB", sep="")
fig <- function() plotil(rain.n(sqrt(sumtargs)), title.)
figu(paste("Interval by layer plots for TS files. ", title.), newpage="port")
}
detach(ts2)
rm(sur, lon.r, lat.r, start, end, i, colz, sumtargs, title.)
#####################################################################################################################################################
# compare interval gaps in Sv and TS files
head2("SV and TS FILES COMPARISON")
t1 <- table(sv2$Interval, sv2$Region_name)
t2 <- table(ts2$Interval, ts2$Region_name)
# create matrices with all the intervals and all the regions
iu <- union(rownames(t1), rownames(t2))
iu <- iu[order(as.numeric(iu))]
ju <- union(colnames(t1), colnames(t2))
bigt1 <- matrix(0, nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
bigt2 <- bigt1
bigt1[rownames(t1), colnames(t1)] <- t1
bigt2[rownames(t2), colnames(t2)] <- t2
results <- matrix("", nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
# assign values when rows/columns don't match
results[bigt1 < 0.5 & bigt2 > 0.5] <- "Gap in Sv"
results[bigt1 > 0.5 & bigt2 < 0.5] <- "Gap in TS"
res <- results[apply(results!="", 1, sum) > 0, apply(results!="", 2, sum) > 0]
if(sum(dim(res)) > 0) {
tab <- res
tabl("Interval gaps in Sv and TS files don't match up.")
} else {
para("Interval gaps in Sv and TS files match up.")
}
rm(t1, t2, iu, ju, bigt1, bigt2, results, res)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# explore trawl files
### OPTROP
head2("OP and TROP FILES")
optrop2 <- missings(optrop)
para(paste0("The OP/TROP files have ", dim(optrop2)[1], " rows and ", dim(optrop2)[2], " columns."))
tab <- qksmry(optrop2)
tabl("Quick summary table of variables in OP/TROP files.")
attach(optrop2)
pcols <- c("Op.Id", "Vessel", "Cruise", "Serial", "Lake", "Port", 
"Beg.Depth", "End.Depth", "Distance", "Fishing_Temp", "Fishing_Depth", "Transect")
tab <- optrop2[is.na(Beg.Depth) | is.na(End.Depth) | is.na(Distance) | is.na(Fishing_Temp), pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with missing depth, distance, or temperature.", newpage="land")
} else {
para("All OP/TROP records have depth, distance, and temperature entered.")
}
set.time <- floor(Set_Time/100) + (Set_Time - 100*floor(Set_Time/100))/60
tod <- rep("night", length(set.time))
tod[set.time > 7 & set.time < 19] <- "day"
tt <- table(tod)
mostall <- names(which.max(table(tod)))
if(length(tt)<1.5) {
if(mostall=="night") {
para("All OP/TROP records were taken at night.")
} else {
para("All OP/TROP records were taken during the day.")
}
} else {
if(mostall=="night") {
tab <- optrop2[tod=="day", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken at night, but some were taken during the day.")
} else {
tab <- optrop2[tod=="night", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken during the day, but some were taken at night.")
}
}
tab <- optrop2[!is.na(Beg.Depth) & !is.na(End.Depth) & abs(Beg.Depth - End.Depth) > 20, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with > 20 m difference between beginning and ending bottom depth.")
} else {
para("All OP/TROP records have < 20 m difference between beginning and ending bottom depth.")
}
mind <- pmin(Beg.Depth, End.Depth, na.rm=T)
tab <- optrop2[!is.na(mind) & !is.na(Fishing_Depth) & Fishing_Depth > mind, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with fishing depth > beginning or ending bottom depth.")
} else {
para("All OP/TROP records have fishing depths < beginning and ending bottom depths.")
}
fig <- function() {
par(mfrow=c(6, 4), mar=c(3, 3, 2, 1))
plot.df(optrop2)
}
figu("Plot of variables in the OP/TROP files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Port))), "Colors indicate Port", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Port)
}
figu("Identification of ports in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Cruise))), "Colors indicate Cruise", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Cruise)
}
figu("Identification of cruises in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Transect))), "Colors indicate Transect", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Transect)
}
figu("Identification of transects in OP/TROP files.", newpage="port")
maxd <- pmax(Beg.Depth, End.Depth, na.rm=T)
fig <- function() {
plotmap(Latitude, Longitude, rain.n(-maxd), "Colors indicate Bottom Depth", pch=16, xla=0.15, yla=0.1)
}
figu("Bottom depths in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(Tow_Time), "Colors indicate Tow_Time", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tow_Time)
}
figu("Identification of tow times in OP/TROP files.", newpage="port")
if("Tr_Design" %in% names(optrop2)) {
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Tr_Design))), "Colors indicate Tr_Design", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tr_Design)
}
figu("Identification of trawl designs in OP/TROP files.", newpage="port")
}
detach(optrop2)
rm(set.time, tod, tt, mostall, mind, maxd)
### trcatch
addPageBreak(doc, width=11, height=8.5)
head2("TRCATCH FILE")
trcatch2 <- missings(trcatch)
para(paste0("The TRCATCH file has ", dim(trcatch2)[1], " rows and ", dim(trcatch2)[2], " columns."))
tab <- qksmry(trcatch2)
tabl("Quick summary table of variables in TRCATCH file.")
attach(trcatch2)
sus <- sort(unique(Species))
if("Beg.Depth" %in% names(trcatch2)) {
tab <- trcatch2[is.na(Beg.Depth) | is.na(End.Depth), 
c("Op.Id", "Year", "Vessel", "Serial", "Lake", "Species", "Port_Name", "Beg.Depth", "End.Depth", "N")]
if(dim(tab)[1] > 0) {
tabl("TRCATCH records with missing beginning or ending depth.")
} else {
para("All TRCATCH records have beginning and ending depth entered.")
}
}
fig <- function() {
par(mfrow=c(5, 4), mar=c(3, 3, 2, 1))
plot.df(trcatch2)
plotsp(N, "N")
plotsp(Weight, "Weight")
plotsp(Weight/N, "Weight/N")
}
figu("Plot of variables in the TRCATCH file.", newpage="port")
detach(trcatch2)
rm(sus)
### trlf
head2("TRLF FILE")
trlf2 <- missings(trlf)
para(paste0("The TRCATCH file has ", dim(trlf2)[1], " rows and ", dim(trlf2)[2], " columns."))
tab <- qksmry(trlf2)
tabl("Quick summary table of variables in TRLF file.")
attach(trlf)
fig <- function() {
par(mfrow=c(5, 3), mar=c(3, 3, 2, 1))
plot.df(trlf)
}
figu("Plot of variables in the TRLF file.", newpage="port")
sus <- sort(unique(Species))
fig <- function() {
par(mfrow=n2mfrow(length(sus)), mar=c(3, 3, 2, 1), oma=c(2, 2, 1, 1), yaxs="i", cex=1)
for(i in seq(along=sus)) {
sel <- Species==sus[i]
a <- hist(rep(Length[sel], N[sel]), breaks=seq(-5, max(Length)+5, 5), plot=FALSE)
xr <- range(Length[sel])
hist(rep(Length[sel], N[sel]), xlim=xr+10*c(-1, 1), ylim=c(0, max(a$counts))*1.05, 
breaks=seq(-5, max(Length)+5, 5), col="blue", main=sus[i], las=1)
abline(v=xr, lwd=2, col="red")
box()
}
mtext("Length  (mm)", side=1, outer=TRUE, cex=1.5)
mtext("Frequency", side=2, outer=TRUE, cex=1.5)
}
figu("Length frequency histograms of species in the TRLF file.  Vertical red lines indicate the minimum and maximum lengths recorded.", 
newpage="port")
detach(trlf)
rm(trlf2, sus) #, i, sel, sul, a, xr)
### key106
head2("Alewife Age-Length Key")
m <- key106[-25, -(1:2)]
dimnames(m)[[1]] <- key106$mmgroup[-25]
dimnames(m)[[2]] <- substring(dimnames(m)[[2]], 4, 4)
m <- as.matrix(m)
m2 <- m[apply(m, 1, sum) > 0, apply(m, 2, sum) > 0]
para(paste0("The alewife age-length key has ", dim(m2)[1], " length categories and ", dim(m2)[2], " age categories."))
tab <- m2
tabl("Alewife age-length key.")
fig <- function() {
par(mar=c(4, 4, 2, 1), cex=1.5)
plot.matrix(m2, inc=0.2, xlab="Length  (mm)", ylab="Age", main="Alewife Age-Length Key")
}
figu("Alewife age-length key.  Circle size is proportional to probability of age, given length.",
"  Probabilities for all ages of a given length sum to one.", newpage="port")
rm(m, m2)
}
end.rtf()
{# save R data to file
rdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
alwayskeep <- c("LAKE", "YEAR", "maindir", "rdatname", "get.packages", "lakenames", "rain.n", "scale.02n", "myrecode", "explore", 
"start.rtf", "head1", "head2", "head3", "para", "tabl", "figu", "end.rtf")
keep1 <- c("trcatch", "optrop", "trlf", "key106")
keep10 <- c("sv2", "ts2")
if(explore==11) {
save(list=c(alwayskeep, keep10, keep1), file=paste0(maindir, rdatname))
} else {
if(explore==1) {
save(list=c(alwayskeep, keep1), file=paste0(maindir, rdatname))
} else {
if(explore==10) {
save(list=c(alwayskeep, keep10), file=paste0(maindir, rdatname))
}
}
}
rm(fig, tab)
}
rdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
alwayskeep <- c("LAKE", "YEAR", "maindir", "rdatname", "get.packages", "lakenames", "rain.n", "scale.02n", "myrecode", "explore", 
"start.rtf", "head1", "head2", "head3", "para", "tabl", "figu", "end.rtf", "explore.ac", "explore.ac")
keep1 <- c("trcatch", "optrop", "trlf", "key106")
keep10 <- c("sv2", "ts2")
if(explore==11) {
save(list=c(alwayskeep, keep10, keep1), file=paste0(maindir, rdatname))
} else {
if(explore==1) {
save(list=c(alwayskeep, keep1), file=paste0(maindir, rdatname))
} else {
if(explore==10) {
save(list=c(alwayskeep, keep10), file=paste0(maindir, rdatname))
}
}
}
rm(fig, tab)
explore
paste0(maindir, rdatname)
cleanup()
search()
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\ACMT Explore Works on Huron files 20 Jan 2015 JVAmody.r
load("C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/L3 Y2014 ACMT Data Daves Original.RData")
explore.ac <- "yes"
explore.tr <- "yes"
svdir <- "C:/JVA/Consult/Warner/Nearest Trawl/Sv/"
tsdir <- "C:/JVA/Consult/Warner/Nearest Trawl/TS/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
if(FALSE) {
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where output will be placed
# outputs include an Rdata file with data to be used for estimation and
# a summary of the data exploration in a Word document
# make sure you use forward slashes (/) not back slashes (\)
maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
### Do you want to explore acoustic data? ###
explore.ac <- "yes"
# if you DON'T want to explore acoustic data, you can ignore the next several lines 
# directory where all Sv data from Echoview are stored in csv files, one file per transect
svdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Sv/"
# directory where all single target frequencies by TS bin are stored in csv files, one for each transect
tsdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/TS/"
### Do you want to explore trawl data? ###
explore.tr <- "yes"
# if you DON'T want to explore trawl data, you can ignore the next several lines 
# RVCAT data files
# midwater trawl op file which includes data from both OP and TR_OP
optrop.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron Midwater Trawl Op.csv"
# midwater trawl catch file
trcatch.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater catch.csv"
# midwater trawl tr_lf file
trlf.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater LF.csv"
# alewife age-length key per year
key106.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/alewifeagelenkey.csv"
}
#########################################################################################################
{### FUNCTIONS
get.packages <- function(want) {
# install (if necessary) and attach wanted packages
have <- row.names(installed.packages())
need <- want[!(want %in% have)]
if(length(need)>0) install.packages(need, repos="http://cran.r-project.org")
lapply(want, require, character.only=TRUE)
invisible()
}
get.packages(c("lubridate", "rtf", "maps"))
lakenames <- c("Superior", "Michigan", "Huron", "Erie", "Ontario")
myrecode <- function(x, old, new, must.match=T) {
partial <- match(x, old)
if(must.match) new[partial] else ifelse(!is.na(partial), new[partial], x)
}
combine.csv <- function(mydir, add.source=TRUE) {
# combine all csv files in a given directory into a single data frame
# file names
filenames <- list.files(mydir)[grep(".csv$", list.files(mydir))]
nfiles <- length(filenames)
# create an empty list where all the files will be stored
files.list <- vector(mode="list", length=nfiles)
for(i in 1:nfiles) {
# read the data into a temporary file
temp <- read.csv(paste(mydir, filenames[i], sep=""), as.is=TRUE)
# add a new column identifying the source file
if(add.source) temp$source <- filenames[i]
# put the data into the list
files.list[[i]] <- temp
}
# combined each of the files from the list into one single file
do.call(rbind, files.list)
}
plotmap <- function(lat, long, colorz, main="", pch=1, cex=1.5, rtf=TRUE, xla=0, yla=0) {
# plot longitude versus latitude with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
map("usa", xlim=range(long, na.rm=TRUE) + c(-1, 1)*xla, ylim=range(lat, na.rm=TRUE) + c(-1, 1)*xla, mar=c(0, 0, 2, 0))
points(long, lat, pch=pch, cex=cex, col=colorz)
box()
mtext(main, side=3, cex=1.2)
}
mid <- function(x) {
# calculate the midpoint between the min and the max
(max(x, na.rm=TRUE) + min(x, na.rm=TRUE))/2
}
textg <- function(lt, ln, group, cex=1.5, ...) {
# add text to plot at group lat/long midpoints
sug <- sort(unique(group))
lng <- tapply(ln, group, mid)
ltg <- tapply(lt, group, mid)
text(lng, ltg, sug, cex=cex, ...)
}
plotil <- function(colorz, main="", rtf=TRUE) {
# plot interval versus layer with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
par(mfrow=n2mfrow(length(sur)), mar=c(2, 2, 2, 1), oma=c(2, 2, 2, 0), cex=1)
for(i in order(lat.r, decreasing=T)) {
sel <- Region_name==sur[i]
plot(Interval[sel], -(Layer_depth_min + Layer_depth_max)[sel]/2, col=colorz[sel], pch=16,
ylim=range(-(Layer_depth_min + Layer_depth_max)/2, na.rm=TRUE), xlab="", ylab="", main=sur[i])
}
mtext("Interval", side=1, outer=T, line=1, cex=1.5)
mtext("Layer depth", side=2, outer=T, line=1, cex=1.5)
mtext(main, side=3, outer=T, cex=1.5)
}
plot3 <- function(lo, hi, betw, rtf=TRUE, order.matters=TRUE, varname="Varname", test=FALSE, ...) {
# plot lows, highs, and betweens for a given metric
x <- seq(hi)
if(!order.matters) {
newlo <- pmin(lo, hi, betw, na.rm=TRUE)
newhi <- pmax(lo, hi, betw, na.rm=TRUE)
lo <- newlo
hi <- newhi
}
ord <- order(hi)
lo. <- lo[ord]
be. <- betw[ord]
hi. <- hi[ord]
sel.lo <- (!is.na(lo.) & !is.na(be.) & lo. > be.) | (!is.na(lo.) & !is.na(hi.) & lo. > hi.)
sel.be <- !is.na(be.) & !is.na(hi.) & be. > hi.
problems <- sum(sum(sel.be, na.rm=TRUE), sum(sel.lo, na.rm=TRUE))
if(!test) {
yr <- range(lo., hi., be., na.rm=TRUE)
if(!rtf) windows(w=6.5, h=8.2)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(x, be., type="n", ylim=yr, las=1, xlab="Records ordered by max", ylab=varname, ...)
points(x[!sel.lo], lo.[!sel.lo], pch=6, col="blue")
points(x[!sel.be], be.[!sel.be])
points(x, hi., pch=2, col="red")
points(x[sel.lo], lo.[sel.lo], pch=6, col="cyan")
points(x[sel.be], be.[sel.be], pch=3, col="green")
legend("topleft", c(paste0("Mid > Max (", format(sum(sel.be, na.rm=TRUE), big.mark=","), " records)"), "Max", 
paste0("Min > Mid or Max (", format(sum(sel.lo, na.rm=TRUE), big.mark=","), " records)"), "Mid < Max", "Min < Mid and Max"), 
col=c("green", "red", "cyan", "black", "blue"), pch=c(3, 2, 6, 1, 6), cex=1.5)
}
if(is.na(problems) | problems < 1) return(FALSE)
}
plotsp <- function(y, ylabb, rtf=TRUE) {
if(!rtf) windows(w=6.5, h=9)
par(mar=c(4, 4, 1, 1))
plot(as.factor(Species), sqrt(y), xlab="Species", ylab=ylabb, axes=F)
axis(1, at=seq(sus), labels=sus)
axis(2, at=pretty(sqrt(y)), labels=pretty(sqrt(y))^2, las=1)
box()
}
missings <- function(df, misscodes=c(-9999, -999.9, -999, 999, 9999), misschars=c("NA", "NULL", ".", " ", "  ")) {
# assign NAs to missing value codes in numeric and character columns of data.frame
df.cur <- df
number.colz <- sapply(df, class) %in% c("integer", "numeric")
df[, number.colz] <- lapply(df[, number.colz], function(x) {
x[x %in% misscodes] <- NA
x
})
char.colz <- sapply(df, class) %in% c("character")
df[, char.colz] <- lapply(df[, char.colz, drop=FALSE], function(x) {
x[x %in% misschars] <- ""
xwe <- tryCatch.W.E(as.numeric(x))
if(is.null(xwe$warning)) x <- xwe$value
x
})
# cat("current = the original dataframe, target = the new dataframe\n")
# print(all.equal(current=df.cur, df))
df
}
qksmry <- function(df) {
no.unique <- sapply(df, function(x) length(table(x)))
no.entered <- sapply(df, function(x) if(is.character(x)) sum(x!="") else sum(!is.na(x)))
no.missing <- sapply(df, function(x) if(is.character(x)) sum(x=="") else sum(is.na(x)))
cbind(no.unique, no.entered, no.missing)
}
plot.df <- function(df, one=FALSE, mcex=1.2, cex=0.8, ...) {
if(one) windows(record=T)
par(cex=cex, ...)
# plot the columns of a data frame
for(i in 1:dim(df)[2]) {
x <- df[[i]]
if(sum(!is.na(x))>0) {
name <- names(df)[i]
if(is.factor(x)) plot(x, main=name, cex.main=mcex) else
if(is.character(x) & length(unique(x))<=50) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x) & sum(is.finite(x)) < 2) plot(1, 1, type="n", xlab="", ylab="", axes=F, 
main=paste(name, ": numeric", sep=""), cex.main=mcex) else
if(is.numeric(x) & length(unique(x))<=10) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x)) plot.default(x[is.finite(x)], main=name, cex.main=mcex) else
if(is.character(x)) plot(1, 1, type="n", xlab="", ylab="", axes=F, main=paste(name, ": character", sep=""), cex.main=mcex)
}
}
invisible()
}
plot.matrix <- function(m, inc=0.3, ...) {
# plots values of matrix as different sized circles
# dimension 1 is plotted on x-axis, dimension 2 on y-axis
uxy <- dimnames(m)
ux <- as.numeric(uxy[[1]])
uxlab <- NULL
if(any(is.na(ux))) {
ux <- seq(along=ux)
uxlab <- uxy[[1]]
}
uy <- as.numeric(uxy[[2]])
uylab <- NULL
if(any(is.na(uy))) {
uy <- seq(along=uy)
uylab <- uxy[[2]]
}
colorz <- c("cyan", "black", "red")[as.numeric(cut(as.vector(m), breaks=c(-Inf, -1e-7, 1e-7, Inf)))]
cush.x <- mean(abs(diff(ux)))
cush.y <- mean(abs(diff(uy)))
plot(ux[row(m)], uy[col(m)], type="n", xlim=range(ux) + cush.x*c(-1, 1), ylim=range(uy) + cush.y*c(-1, 1), axes=FALSE, ...)
symbols(ux[row(m)], uy[col(m)], circle=abs(as.vector(m)), inches=inc, fg=colorz, add=TRUE)
if(is.null(uxlab)) axis(1) else axis(1, at=ux, labels=uxlab)
if(is.null(uylab)) axis(2, las=1) else axis(2, at=uy, labels=uylab, las=1)
box()
}
tryCatch.W.E <- function(expr) {
# Martin Maechler, ETH Zurich, 9 Dec 2010, R-help subject: "How to catch both warnings and errors?"
W <- NULL
w.handler <- function(w) {
W <<- w
invokeRestart("muffleWarning")
}
list(value=withCallingHandlers(tryCatch(expr, error=function(e) e), warning=w.handler), warning=W)
}
scale.02n <- function(x, n=100) {
# scale a collection of values from 0 to some specified maximum
n*as.numeric(scale(x, center=min(x, na.rm=TRUE), scale=diff(range(x, na.rm=TRUE))))
}
rain.n <- function(x, n=100, start=0/6, end=4/6) {
# assign a specified number of rainbow colors to a collection of values
# default color range from blue (4/6) to red (0/6)
y <- round(scale.02n(x, n=n-1)) + 1
# reverse the direction of y (e.g., from 1:100 to 100:1)
y <- n + 1 - y
rainbow(n, start=start, end=end)[y]
}
start.rtf <- function(file=NULL, dir=getwd(), width=8.5, height=11, font.size=12, omi=c(1, 1, 1, 1), quiet=FALSE) {
# create a new RTF file readable by Word
# create two new variables to keep count of tables and figures
tabcount <<- 1
figcount <<- 1
if(is.null(file)) file <- paste0("RGeneratedDocument", Sys.Date())
dirfiledoc <- if(length(grep(".doc", file))>0) paste(dir, file, sep="/") else paste(dir, paste0(file, ".doc"), sep="/")
if(!quiet) cat(paste0("New RTF document created, ", dirfiledoc, "\n"))
RTF(dirfiledoc, width=width, height=height, font.size=font.size, omi=omi)
}
head1 <- function(words, rtf=doc, font.size=16) {
addHeader(this=rtf, title=words)
}
head2 <- function(words, rtf=doc, bold=TRUE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, words, bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
head3 <- function(..., rtf=doc, bold=FALSE, italic=TRUE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
para <- function(..., rtf=doc, bold=FALSE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
tabl <- function(..., TAB=tab, rtf=doc, fontt=8, row.names=TRUE, tabc=tabcount, boldt=TRUE, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
startParagraph(this=rtf)
addText(this=rtf, paste0("Table ", tabc, ".  "), bold=boldt)
addText(this=rtf, ...)
addNewLine(this=rtf)
endParagraph(this=rtf)
addTable(this=rtf, TAB, font.size=fontt, row.names=row.names)
addNewLine(this=rtf)
addNewLine(this=rtf)
tabcount <<- tabc + 1
}
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, wf=6.5, hf=8.2, rf=300, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
end.rtf <- function(rtf=doc, details=TRUE, ...) {
addPageBreak(rtf, ...)
addSessionInfo(rtf)
done(rtf)
}
}
if(FALSE) {
### CRUNCHING
if(casefold(substring(explore.ac, 1, 1))=="y") { # read in AC data
# 1.  Import data
# Concatenate all the files of type csv in a particular folder into one file for Sv, one for TS.  
# Trawl data will already be in one file per type of data (catch, lf, op).  
# Need to make R parse out year from the sampling date field.
### A.  Sv data from Echoview, one file per transect
sv <- combine.csv(svdir)
# create date variables that R recognizes as dates
sv$date.s <- ymd(sv$Date_S, quiet=TRUE)
sv$date.e <- ymd(sv$Date_E, quiet=TRUE)
sv$date.m <- ymd(sv$Date_M, quiet=TRUE)
# create variable for year
sv$year <- year(sv$date.m)
# assign NAs to missing value codes
sv2 <- missings(sv)
# eliminate records with missing values for Lat_M, Sv_mean, or PRC_ABC
sv2 <- sv2[!(is.na(sv2$Lat_M) | is.na(sv2$Sv_mean) | is.na(sv2$PRC_ABC)), ]
### B.  Single target frequencies by TS bin, one for each transect
ts <- combine.csv(tsdir)
# create date variables that R recognizes as dates
ts$date.s <- ymd(ts$Date_S, quiet=TRUE)
ts$date.e <- ymd(ts$Date_E, quiet=TRUE)
ts$date.m <- ymd(ts$Date_M, quiet=TRUE)
# create variable for year
ts$year <- year(ts$date.m)
# assign NAs to missing value codes
ts2 <- missings(ts)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# read in trawl data
# RVCAT data
optrop <- read.csv(optrop.f, as.is=TRUE, na.strings="NULL")
trcatch <- read.csv(trcatch.f, as.is=TRUE, na.strings="NULL")
trlf <- read.csv(trlf.f, as.is=TRUE, na.strings="NULL")
key106 <- read.csv(key106.f, as.is=TRUE)
}
}
{# exploratory plots - save output to *.doc file
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Explore ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
explore <- 10*(casefold(substring(explore.ac, 1, 1))=="y") + 1*(casefold(substring(explore.tr, 1, 1))=="y")
descr <- myrecode(explore, c(0, 1, 10, 11), c("No", "Trawl", "Acoustic", "Acoustic and Trawl"))
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Exploration of ", descr, " Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
rm(descr, docname)
}
if(casefold(substring(explore.ac, 1, 1))=="y") {# print Sv and TS info to doc
para(paste0("svdir = ", svdir, " = Sv data directory."))
para(paste0("tsdir = ", tsdir, " = single target frequencies data directory."))
}
para(paste0("maindir = ", maindir, " = main input/output directory."))
if(casefold(substring(explore.ac, 1, 1))=="y") {# explore Sv and TS files
### Sv
head2("SV FILES")
para(paste0("The Sv files have ", dim(sv2)[1], " rows and ", dim(sv2)[2], " columns."))
tab <- qksmry(sv2)
tabl("Quick summary table of variables in Sv files.")
para(paste0("Figures 1-9 are exploratory plots of the Sv data that can be examined to check for potential problems.  ",
"Any figures after that (but before the TS section) indicate problems in the data.  ",
"Minimum and maximum values were compared with middle or mean values for each of the following variables: Sv, Depth, Ping, Dist, Date, Lat, Long.  ",
"If any problems were indicated by these comparisons, they will appear as figures after Figure 9 but before the TS section.  "))
attach(sv2)
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, 1:35])
}
figu("Plot of the first 35 variables in the Sv files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, -(1:35)])
}
figu("Plot of variables 36+ in the Sv files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Lat_M, Lon_M, as.numeric(as.factor(Region_name)), "Colors indicate Region")
textg(Lat_M, Lon_M, Region_name)
}
figu("Location of transects in Sv files.  Colors indicate Region.", newpage="port")
# close up look at each transect - lat/long
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=n2mfrow(length(sur)), mar=c(1, 0, 2, 0), cex=1.2)
for(i in order(lat.r, decreasing=F)) {
sel <- Region_name==sur[i]
map("usa", xlim=range(Lon_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, ylim=range(Lat_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, mar=c(1, 0, 2, 0), type="n")
points(Lon_M[sel], Lat_M[sel], col=as.numeric(as.factor(Region_name))[sel])
mtext(sur[i], side=3, cex=1.5)
box()
}
}
figu("Close up look at each transect location in Sv files.", newpage="port")
# interval by layer plots
fig <- function() plotil(rain.n(-Depth_mean), "Colors indicate Depth_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Depth_mean.", newpage="port")
fig <- function() plotil(rain.n(Sv_mean), "Colors indicate Sv_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Sv_mean.", newpage="port")
fig <- function() plotil(rain.n(PRC_ABC^0.2), "Colors indicate PRC_ABC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_ABC.", newpage="port")
fig <- function() plotil(rain.n(PRC_NASC^0.2), "Colors indicate PRC_NASC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_NASC.", newpage="port")
fig <- function() plotil(rain.n(Samples_In_Domain), "Colors indicate Samples")
figu("Interval by layer plots for Sv files.  Colors indicate Samples.", newpage="port")
# plots comparing extremes with middle values
fig <- function() plot3(Ping_S, Ping_E, Ping_M, varname="Ping")
np <- plot3(Ping_S, Ping_E, Ping_M, test=TRUE)
if(is.null(np)) figu("Comparing Ping_S, Ping_E, and Ping_M for Sv files.", newpage="port")
fig <- function() plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, varname="Dist")
np <- plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Dist_S, Dist_E, and Dist_M for Sv files.", newpage="port")
fig <- function() plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), varname="Date")
np <- plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), test=TRUE)
if(is.null(np)) figu("Comparing Date_S, Date_E, and Date_M for Sv files.", newpage="port")
fig <- function() plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, varname="Latitude")
np <- plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lat_S, Lat_E, and Lat_M for Sv files.", newpage="port")
fig <- function() plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, varname="Longitude")
np <- plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lon_S, Lon_E, and Lon_M for Sv files.", newpage="port")
detach(sv2)
rm(sur, lat.r, lon.r, np)
### TS
head2("TS FILES")
para(paste0("The TS files have ", dim(ts2)[1], " rows and ", dim(ts2)[2], " columns."))
tab <- qksmry(ts2)
tabl("Quick summary table of variables in TS files.")
attach(ts2)
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 1:28])
}
figu("Plot of the first 28 variables in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 28+(1:28)])
}
figu("Plot of variables 29-56 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 56+(1:28)])
}
figu("Plot of variables 57-84 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, -(1:84)])
}
figu("Plot of variables 85+ in the TS files.", newpage="port")
# interval by layer plots
start <- seq(16, 66, 10)
end <- start+10
for(i in seq(along=start)) {
colz <- paste("X.", start[i]:end[i], ".000000", sep="")
sumtargs <- apply(ts2[, colz], 1, sum)
title. <- paste("Colors indicate binned targets from, -", end[i], " to -", start[i], " dB", sep="")
fig <- function() plotil(rain.n(sqrt(sumtargs)), title.)
figu(paste("Interval by layer plots for TS files. ", title.), newpage="port")
}
detach(ts2)
rm(sur, lon.r, lat.r, start, end, i, colz, sumtargs, title.)
#####################################################################################################################################################
# compare interval gaps in Sv and TS files
head2("SV and TS FILES COMPARISON")
t1 <- table(sv2$Interval, sv2$Region_name)
t2 <- table(ts2$Interval, ts2$Region_name)
# create matrices with all the intervals and all the regions
iu <- union(rownames(t1), rownames(t2))
iu <- iu[order(as.numeric(iu))]
ju <- union(colnames(t1), colnames(t2))
bigt1 <- matrix(0, nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
bigt2 <- bigt1
bigt1[rownames(t1), colnames(t1)] <- t1
bigt2[rownames(t2), colnames(t2)] <- t2
results <- matrix("", nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
# assign values when rows/columns don't match
results[bigt1 < 0.5 & bigt2 > 0.5] <- "Gap in Sv"
results[bigt1 > 0.5 & bigt2 < 0.5] <- "Gap in TS"
res <- results[apply(results!="", 1, sum) > 0, apply(results!="", 2, sum) > 0]
if(sum(dim(res)) > 0) {
tab <- res
tabl("Interval gaps in Sv and TS files don't match up.")
} else {
para("Interval gaps in Sv and TS files match up.")
}
rm(t1, t2, iu, ju, bigt1, bigt2, results, res)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# explore trawl files
### OPTROP
head2("OP and TROP FILES")
optrop2 <- missings(optrop)
para(paste0("The OP/TROP files have ", dim(optrop2)[1], " rows and ", dim(optrop2)[2], " columns."))
tab <- qksmry(optrop2)
tabl("Quick summary table of variables in OP/TROP files.")
attach(optrop2)
pcols <- c("Op.Id", "Vessel", "Cruise", "Serial", "Lake", "Port", 
"Beg.Depth", "End.Depth", "Distance", "Fishing_Temp", "Fishing_Depth", "Transect")
tab <- optrop2[is.na(Beg.Depth) | is.na(End.Depth) | is.na(Distance) | is.na(Fishing_Temp), pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with missing depth, distance, or temperature.", newpage="land")
} else {
para("All OP/TROP records have depth, distance, and temperature entered.")
}
set.time <- floor(Set_Time/100) + (Set_Time - 100*floor(Set_Time/100))/60
tod <- rep("night", length(set.time))
tod[set.time > 7 & set.time < 19] <- "day"
tt <- table(tod)
mostall <- names(which.max(table(tod)))
if(length(tt)<1.5) {
if(mostall=="night") {
para("All OP/TROP records were taken at night.")
} else {
para("All OP/TROP records were taken during the day.")
}
} else {
if(mostall=="night") {
tab <- optrop2[tod=="day", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken at night, but some were taken during the day.")
} else {
tab <- optrop2[tod=="night", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken during the day, but some were taken at night.")
}
}
tab <- optrop2[!is.na(Beg.Depth) & !is.na(End.Depth) & abs(Beg.Depth - End.Depth) > 20, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with > 20 m difference between beginning and ending bottom depth.")
} else {
para("All OP/TROP records have < 20 m difference between beginning and ending bottom depth.")
}
mind <- pmin(Beg.Depth, End.Depth, na.rm=T)
tab <- optrop2[!is.na(mind) & !is.na(Fishing_Depth) & Fishing_Depth > mind, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with fishing depth > beginning or ending bottom depth.")
} else {
para("All OP/TROP records have fishing depths < beginning and ending bottom depths.")
}
fig <- function() {
par(mfrow=c(6, 4), mar=c(3, 3, 2, 1))
plot.df(optrop2)
}
figu("Plot of variables in the OP/TROP files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Port))), "Colors indicate Port", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Port)
}
figu("Identification of ports in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Cruise))), "Colors indicate Cruise", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Cruise)
}
figu("Identification of cruises in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Transect))), "Colors indicate Transect", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Transect)
}
figu("Identification of transects in OP/TROP files.", newpage="port")
maxd <- pmax(Beg.Depth, End.Depth, na.rm=T)
fig <- function() {
plotmap(Latitude, Longitude, rain.n(-maxd), "Colors indicate Bottom Depth", pch=16, xla=0.15, yla=0.1)
}
figu("Bottom depths in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(Tow_Time), "Colors indicate Tow_Time", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tow_Time)
}
figu("Identification of tow times in OP/TROP files.", newpage="port")
if("Tr_Design" %in% names(optrop2)) {
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Tr_Design))), "Colors indicate Tr_Design", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tr_Design)
}
figu("Identification of trawl designs in OP/TROP files.", newpage="port")
}
detach(optrop2)
rm(set.time, tod, tt, mostall, mind, maxd)
### trcatch
addPageBreak(doc, width=11, height=8.5)
head2("TRCATCH FILE")
trcatch2 <- missings(trcatch)
para(paste0("The TRCATCH file has ", dim(trcatch2)[1], " rows and ", dim(trcatch2)[2], " columns."))
tab <- qksmry(trcatch2)
tabl("Quick summary table of variables in TRCATCH file.")
attach(trcatch2)
sus <- sort(unique(Species))
if("Beg.Depth" %in% names(trcatch2)) {
tab <- trcatch2[is.na(Beg.Depth) | is.na(End.Depth), 
c("Op.Id", "Year", "Vessel", "Serial", "Lake", "Species", "Port_Name", "Beg.Depth", "End.Depth", "N")]
if(dim(tab)[1] > 0) {
tabl("TRCATCH records with missing beginning or ending depth.")
} else {
para("All TRCATCH records have beginning and ending depth entered.")
}
}
fig <- function() {
par(mfrow=c(5, 4), mar=c(3, 3, 2, 1))
plot.df(trcatch2)
plotsp(N, "N")
plotsp(Weight, "Weight")
plotsp(Weight/N, "Weight/N")
}
figu("Plot of variables in the TRCATCH file.", newpage="port")
detach(trcatch2)
rm(sus)
### trlf
head2("TRLF FILE")
trlf2 <- missings(trlf)
para(paste0("The TRCATCH file has ", dim(trlf2)[1], " rows and ", dim(trlf2)[2], " columns."))
tab <- qksmry(trlf2)
tabl("Quick summary table of variables in TRLF file.")
attach(trlf)
fig <- function() {
par(mfrow=c(5, 3), mar=c(3, 3, 2, 1))
plot.df(trlf)
}
figu("Plot of variables in the TRLF file.", newpage="port")
sus <- sort(unique(Species))
fig <- function() {
par(mfrow=n2mfrow(length(sus)), mar=c(3, 3, 2, 1), oma=c(2, 2, 1, 1), yaxs="i", cex=1)
for(i in seq(along=sus)) {
sel <- Species==sus[i]
a <- hist(rep(Length[sel], N[sel]), breaks=seq(-5, max(Length)+5, 5), plot=FALSE)
xr <- range(Length[sel])
hist(rep(Length[sel], N[sel]), xlim=xr+10*c(-1, 1), ylim=c(0, max(a$counts))*1.05, 
breaks=seq(-5, max(Length)+5, 5), col="blue", main=sus[i], las=1)
abline(v=xr, lwd=2, col="red")
box()
}
mtext("Length  (mm)", side=1, outer=TRUE, cex=1.5)
mtext("Frequency", side=2, outer=TRUE, cex=1.5)
}
figu("Length frequency histograms of species in the TRLF file.  Vertical red lines indicate the minimum and maximum lengths recorded.", 
newpage="port")
detach(trlf)
rm(trlf2, sus) #, i, sel, sul, a, xr)
### key106
head2("Alewife Age-Length Key")
m <- key106[-25, -(1:2)]
dimnames(m)[[1]] <- key106$mmgroup[-25]
dimnames(m)[[2]] <- substring(dimnames(m)[[2]], 4, 4)
m <- as.matrix(m)
m2 <- m[apply(m, 1, sum) > 0, apply(m, 2, sum) > 0]
para(paste0("The alewife age-length key has ", dim(m2)[1], " length categories and ", dim(m2)[2], " age categories."))
tab <- m2
tabl("Alewife age-length key.")
fig <- function() {
par(mar=c(4, 4, 2, 1), cex=1.5)
plot.matrix(m2, inc=0.2, xlab="Length  (mm)", ylab="Age", main="Alewife Age-Length Key")
}
figu("Alewife age-length key.  Circle size is proportional to probability of age, given length.",
"  Probabilities for all ages of a given length sum to one.", newpage="port")
rm(m, m2)
}
end.rtf()
{# save R data to file
rdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
alwayskeep <- c("LAKE", "YEAR", "maindir", "rdatname", "get.packages", "lakenames", "rain.n", "scale.02n", "myrecode", "explore", 
"start.rtf", "head1", "head2", "head3", "para", "tabl", "figu", "end.rtf", "explore.ac", "explore.ac")
keep1 <- c("trcatch", "optrop", "trlf", "key106")
keep10 <- c("sv2", "ts2")
if(explore==11) {
save(list=c(alwayskeep, keep10, keep1), file=paste0(maindir, rdatname))
} else {
if(explore==1) {
save(list=c(alwayskeep, keep1), file=paste0(maindir, rdatname))
} else {
if(explore==10) {
save(list=c(alwayskeep, keep10), file=paste0(maindir, rdatname))
}
}
}
rm(fig, tab)
}
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify whether alewife age key should be used (TRUE or FALSE)
use.alewife.ages <- FALSE
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in ha)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
para(if(use.alewife.ages) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area")
REG <- design$reg[design$lake==LAKE]
REG.AREA <- design$reg.area[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(use.alewife.ages & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination that occurs in the TS data but not in the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 1521.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(use.alewife.ages) {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & use.alewife.ages) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regarea <- REG.AREA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regarea + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regarea + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regarea + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regarea + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lakewide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regarea)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regarea)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regarea)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lakewide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- format(round(laketots.n), big.mark=",")
tabl("Lakewide estimates in number (millions) for each species group and 'new slices'.",
"  Groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- format(round(laketots.g), big.mark=",")
tabl("Lakewide biomass estimates (t) for each species group and 'new slices'.",
"  Groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
# regions used in laying out sampling design and corresponding areas (in ha)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
design
tab
tab <- format(round(laketots.g), big.mark=",")
tab
class(tab)
as.matrix(tab)
class(as.matrix(tab))
cleanup()
search()
load("C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/L3 Y2014 ACMT Data Daves Original.RData")
explore.ac <- "yes"
explore.tr <- "yes"
svdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Sv/"
tsdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/TS/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
ls()
rm(key106)
use.alewife.ages <- "no"
if(FALSE) {
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where output will be placed
# outputs include an Rdata file with data to be used for estimation and
# a summary of the data exploration in a Word document
# make sure you use forward slashes (/) not back slashes (\)
maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
### Do you want to explore acoustic data? ###
explore.ac <- "yes"
# if you DON'T want to explore acoustic data, you can ignore the next several lines 
# directory where all Sv data from Echoview are stored in csv files, one file per transect
svdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Sv/"
# directory where all single target frequencies by TS bin are stored in csv files, one for each transect
tsdir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/TS/"
### Do you want to explore trawl data? ###
explore.tr <- "yes"
# if you DON'T want to explore trawl data, you can ignore the next several lines 
# RVCAT data files
# midwater trawl op file which includes data from both OP and TR_OP
optrop.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron Midwater Trawl Op.csv"
# midwater trawl catch file
trcatch.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater catch.csv"
# midwater trawl tr_lf file
trlf.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/Lake Huron midwater LF.csv"
### Do you want to use an age-length key for alewife? ###
use.alewife.ages <- "no"
# if you DON'T want to use an age-length key for alewife, you can ignore the next couple of lines 
# alewife age-length key per year
key106.f <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/alewifeagelenkey.csv"
}
#########################################################################################################
{### FUNCTIONS
get.packages <- function(want) {
# install (if necessary) and attach wanted packages
have <- row.names(installed.packages())
need <- want[!(want %in% have)]
if(length(need)>0) install.packages(need, repos="http://cran.r-project.org")
lapply(want, require, character.only=TRUE)
invisible()
}
get.packages(c("lubridate", "rtf", "maps"))
lakenames <- c("Superior", "Michigan", "Huron", "Erie", "Ontario")
myrecode <- function(x, old, new, must.match=T) {
partial <- match(x, old)
if(must.match) new[partial] else ifelse(!is.na(partial), new[partial], x)
}
combine.csv <- function(mydir, add.source=TRUE) {
# combine all csv files in a given directory into a single data frame
# file names
filenames <- list.files(mydir)[grep(".csv$", list.files(mydir))]
nfiles <- length(filenames)
# create an empty list where all the files will be stored
files.list <- vector(mode="list", length=nfiles)
for(i in 1:nfiles) {
# read the data into a temporary file
temp <- read.csv(paste(mydir, filenames[i], sep=""), as.is=TRUE)
# add a new column identifying the source file
if(add.source) temp$source <- filenames[i]
# put the data into the list
files.list[[i]] <- temp
}
# combined each of the files from the list into one single file
do.call(rbind, files.list)
}
plotmap <- function(lat, long, colorz, main="", pch=1, cex=1.5, rtf=TRUE, xla=0, yla=0) {
# plot longitude versus latitude with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
map("usa", xlim=range(long, na.rm=TRUE) + c(-1, 1)*xla, ylim=range(lat, na.rm=TRUE) + c(-1, 1)*xla, mar=c(0, 0, 2, 0))
points(long, lat, pch=pch, cex=cex, col=colorz)
box()
mtext(main, side=3, cex=1.2)
}
mid <- function(x) {
# calculate the midpoint between the min and the max
(max(x, na.rm=TRUE) + min(x, na.rm=TRUE))/2
}
textg <- function(lt, ln, group, cex=1.5, ...) {
# add text to plot at group lat/long midpoints
sug <- sort(unique(group))
lng <- tapply(ln, group, mid)
ltg <- tapply(lt, group, mid)
text(lng, ltg, sug, cex=cex, ...)
}
plotil <- function(colorz, main="", rtf=TRUE) {
# plot interval versus layer with different colors for symbols
if(!rtf) windows(w=6.5, h=9)
par(mfrow=n2mfrow(length(sur)), mar=c(2, 2, 2, 1), oma=c(2, 2, 2, 0), cex=1)
for(i in order(lat.r, decreasing=T)) {
sel <- Region_name==sur[i]
plot(Interval[sel], -(Layer_depth_min + Layer_depth_max)[sel]/2, col=colorz[sel], pch=16,
ylim=range(-(Layer_depth_min + Layer_depth_max)/2, na.rm=TRUE), xlab="", ylab="", main=sur[i])
}
mtext("Interval", side=1, outer=T, line=1, cex=1.5)
mtext("Layer depth", side=2, outer=T, line=1, cex=1.5)
mtext(main, side=3, outer=T, cex=1.5)
}
plot3 <- function(lo, hi, betw, rtf=TRUE, order.matters=TRUE, varname="Varname", test=FALSE, ...) {
# plot lows, highs, and betweens for a given metric
x <- seq(hi)
if(!order.matters) {
newlo <- pmin(lo, hi, betw, na.rm=TRUE)
newhi <- pmax(lo, hi, betw, na.rm=TRUE)
lo <- newlo
hi <- newhi
}
ord <- order(hi)
lo. <- lo[ord]
be. <- betw[ord]
hi. <- hi[ord]
sel.lo <- (!is.na(lo.) & !is.na(be.) & lo. > be.) | (!is.na(lo.) & !is.na(hi.) & lo. > hi.)
sel.be <- !is.na(be.) & !is.na(hi.) & be. > hi.
problems <- sum(sum(sel.be, na.rm=TRUE), sum(sel.lo, na.rm=TRUE))
if(!test) {
yr <- range(lo., hi., be., na.rm=TRUE)
if(!rtf) windows(w=6.5, h=8.2)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(x, be., type="n", ylim=yr, las=1, xlab="Records ordered by max", ylab=varname, ...)
points(x[!sel.lo], lo.[!sel.lo], pch=6, col="blue")
points(x[!sel.be], be.[!sel.be])
points(x, hi., pch=2, col="red")
points(x[sel.lo], lo.[sel.lo], pch=6, col="cyan")
points(x[sel.be], be.[sel.be], pch=3, col="green")
legend("topleft", c(paste0("Mid > Max (", format(sum(sel.be, na.rm=TRUE), big.mark=","), " records)"), "Max", 
paste0("Min > Mid or Max (", format(sum(sel.lo, na.rm=TRUE), big.mark=","), " records)"), "Mid < Max", "Min < Mid and Max"), 
col=c("green", "red", "cyan", "black", "blue"), pch=c(3, 2, 6, 1, 6), cex=1.5)
}
if(is.na(problems) | problems < 1) return(FALSE)
}
plotsp <- function(y, ylabb, rtf=TRUE) {
if(!rtf) windows(w=6.5, h=9)
par(mar=c(4, 4, 1, 1))
plot(as.factor(Species), sqrt(y), xlab="Species", ylab=ylabb, axes=F)
axis(1, at=seq(sus), labels=sus)
axis(2, at=pretty(sqrt(y)), labels=pretty(sqrt(y))^2, las=1)
box()
}
missings <- function(df, misscodes=c(-9999, -999.9, -999, 999, 9999), misschars=c("NA", "NULL", ".", " ", "  ")) {
# assign NAs to missing value codes in numeric and character columns of data.frame
df.cur <- df
number.colz <- sapply(df, class) %in% c("integer", "numeric")
df[, number.colz] <- lapply(df[, number.colz], function(x) {
x[x %in% misscodes] <- NA
x
})
char.colz <- sapply(df, class) %in% c("character")
df[, char.colz] <- lapply(df[, char.colz, drop=FALSE], function(x) {
x[x %in% misschars] <- ""
xwe <- tryCatch.W.E(as.numeric(x))
if(is.null(xwe$warning)) x <- xwe$value
x
})
# cat("current = the original dataframe, target = the new dataframe\n")
# print(all.equal(current=df.cur, df))
df
}
qksmry <- function(df) {
no.unique <- sapply(df, function(x) length(table(x)))
no.entered <- sapply(df, function(x) if(is.character(x)) sum(x!="") else sum(!is.na(x)))
no.missing <- sapply(df, function(x) if(is.character(x)) sum(x=="") else sum(is.na(x)))
cbind(no.unique, no.entered, no.missing)
}
plot.df <- function(df, one=FALSE, mcex=1.2, cex=0.8, ...) {
if(one) windows(record=T)
par(cex=cex, ...)
# plot the columns of a data frame
for(i in 1:dim(df)[2]) {
x <- df[[i]]
if(sum(!is.na(x))>0) {
name <- names(df)[i]
if(is.factor(x)) plot(x, main=name, cex.main=mcex) else
if(is.character(x) & length(unique(x))<=50) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x) & sum(is.finite(x)) < 2) plot(1, 1, type="n", xlab="", ylab="", axes=F, 
main=paste(name, ": numeric", sep=""), cex.main=mcex) else
if(is.numeric(x) & length(unique(x))<=10) plot(as.factor(x), main=name, cex.main=mcex) else
if(is.numeric(x)) plot.default(x[is.finite(x)], main=name, cex.main=mcex) else
if(is.character(x)) plot(1, 1, type="n", xlab="", ylab="", axes=F, main=paste(name, ": character", sep=""), cex.main=mcex)
}
}
invisible()
}
plot.matrix <- function(m, inc=0.3, ...) {
# plots values of matrix as different sized circles
# dimension 1 is plotted on x-axis, dimension 2 on y-axis
uxy <- dimnames(m)
ux <- as.numeric(uxy[[1]])
uxlab <- NULL
if(any(is.na(ux))) {
ux <- seq(along=ux)
uxlab <- uxy[[1]]
}
uy <- as.numeric(uxy[[2]])
uylab <- NULL
if(any(is.na(uy))) {
uy <- seq(along=uy)
uylab <- uxy[[2]]
}
colorz <- c("cyan", "black", "red")[as.numeric(cut(as.vector(m), breaks=c(-Inf, -1e-7, 1e-7, Inf)))]
cush.x <- mean(abs(diff(ux)))
cush.y <- mean(abs(diff(uy)))
plot(ux[row(m)], uy[col(m)], type="n", xlim=range(ux) + cush.x*c(-1, 1), ylim=range(uy) + cush.y*c(-1, 1), axes=FALSE, ...)
symbols(ux[row(m)], uy[col(m)], circle=abs(as.vector(m)), inches=inc, fg=colorz, add=TRUE)
if(is.null(uxlab)) axis(1) else axis(1, at=ux, labels=uxlab)
if(is.null(uylab)) axis(2, las=1) else axis(2, at=uy, labels=uylab, las=1)
box()
}
tryCatch.W.E <- function(expr) {
# Martin Maechler, ETH Zurich, 9 Dec 2010, R-help subject: "How to catch both warnings and errors?"
W <- NULL
w.handler <- function(w) {
W <<- w
invokeRestart("muffleWarning")
}
list(value=withCallingHandlers(tryCatch(expr, error=function(e) e), warning=w.handler), warning=W)
}
scale.02n <- function(x, n=100) {
# scale a collection of values from 0 to some specified maximum
n*as.numeric(scale(x, center=min(x, na.rm=TRUE), scale=diff(range(x, na.rm=TRUE))))
}
rain.n <- function(x, n=100, start=0/6, end=4/6) {
# assign a specified number of rainbow colors to a collection of values
# default color range from blue (4/6) to red (0/6)
y <- round(scale.02n(x, n=n-1)) + 1
# reverse the direction of y (e.g., from 1:100 to 100:1)
y <- n + 1 - y
rainbow(n, start=start, end=end)[y]
}
start.rtf <- function(file=NULL, dir=getwd(), width=8.5, height=11, font.size=12, omi=c(1, 1, 1, 1), quiet=FALSE) {
# create a new RTF file readable by Word
# create two new variables to keep count of tables and figures
tabcount <<- 1
figcount <<- 1
if(is.null(file)) file <- paste0("RGeneratedDocument", Sys.Date())
dirfiledoc <- if(length(grep(".doc", file))>0) paste(dir, file, sep="/") else paste(dir, paste0(file, ".doc"), sep="/")
if(!quiet) cat(paste0("New RTF document created, ", dirfiledoc, "\n"))
RTF(dirfiledoc, width=width, height=height, font.size=font.size, omi=omi)
}
head1 <- function(words, rtf=doc, font.size=16) {
addHeader(this=rtf, title=words)
}
head2 <- function(words, rtf=doc, bold=TRUE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, words, bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
head3 <- function(..., rtf=doc, bold=FALSE, italic=TRUE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
para <- function(..., rtf=doc, bold=FALSE, italic=FALSE) {
startParagraph(this=rtf)
addText(this=rtf, ..., bold=bold, italic=italic)
endParagraph(this=rtf)
addNewLine(this=rtf)
}
tabl <- function(..., TAB=tab, rtf=doc, fontt=8, row.names=TRUE, tabc=tabcount, boldt=TRUE, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
startParagraph(this=rtf)
addText(this=rtf, paste0("Table ", tabc, ".  "), bold=boldt)
addText(this=rtf, ...)
addNewLine(this=rtf)
endParagraph(this=rtf)
addTable(this=rtf, TAB, font.size=fontt, row.names=row.names)
addNewLine(this=rtf)
addNewLine(this=rtf)
tabcount <<- tabc + 1
}
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, wf=6.5, hf=8.2, rf=300, newpage=c("none", "port", "land")[1], omi=rep(1, 4)) {
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
end.rtf <- function(rtf=doc, details=TRUE, ...) {
addPageBreak(rtf, ...)
addSessionInfo(rtf)
done(rtf)
}
}
if(FALSE) {
### CRUNCHING
if(casefold(substring(explore.ac, 1, 1))=="y") { # read in AC data
# 1.  Import data
# Concatenate all the files of type csv in a particular folder into one file for Sv, one for TS.  
# Trawl data will already be in one file per type of data (catch, lf, op).  
# Need to make R parse out year from the sampling date field.
### A.  Sv data from Echoview, one file per transect
sv <- combine.csv(svdir)
# create date variables that R recognizes as dates
sv$date.s <- ymd(sv$Date_S, quiet=TRUE)
sv$date.e <- ymd(sv$Date_E, quiet=TRUE)
sv$date.m <- ymd(sv$Date_M, quiet=TRUE)
# create variable for year
sv$year <- year(sv$date.m)
# assign NAs to missing value codes
sv2 <- missings(sv)
# eliminate records with missing values for Lat_M, Sv_mean, or PRC_ABC
sv2 <- sv2[!(is.na(sv2$Lat_M) | is.na(sv2$Sv_mean) | is.na(sv2$PRC_ABC)), ]
### B.  Single target frequencies by TS bin, one for each transect
ts <- combine.csv(tsdir)
# create date variables that R recognizes as dates
ts$date.s <- ymd(ts$Date_S, quiet=TRUE)
ts$date.e <- ymd(ts$Date_E, quiet=TRUE)
ts$date.m <- ymd(ts$Date_M, quiet=TRUE)
# create variable for year
ts$year <- year(ts$date.m)
# assign NAs to missing value codes
ts2 <- missings(ts)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# read in trawl data
# RVCAT data
optrop <- read.csv(optrop.f, as.is=TRUE, na.strings="NULL")
trcatch <- read.csv(trcatch.f, as.is=TRUE, na.strings="NULL")
trlf <- read.csv(trlf.f, as.is=TRUE, na.strings="NULL")
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {# read in alewife age length key
key106 <- read.csv(key106.f, as.is=TRUE)
}
}
}
{# exploratory plots - save output to *.doc file
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Explore ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
explore <- 10*(casefold(substring(explore.ac, 1, 1))=="y") + 1*(casefold(substring(explore.tr, 1, 1))=="y")
descr <- myrecode(explore, c(0, 1, 10, 11), c("No", "Trawl", "Acoustic", "Acoustic and Trawl"))
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Exploration of ", descr, " Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
rm(descr, docname)
}
if(casefold(substring(explore.ac, 1, 1))=="y") {# print Sv and TS info to doc
para(paste0("svdir = ", svdir, " = Sv data directory."))
para(paste0("tsdir = ", tsdir, " = single target frequencies data directory."))
}
para(paste0("maindir = ", maindir, " = main input/output directory."))
if(casefold(substring(explore.ac, 1, 1))=="y") {# explore Sv and TS files
### Sv
head2("SV FILES")
para(paste0("The Sv files have ", dim(sv2)[1], " rows and ", dim(sv2)[2], " columns."))
tab <- qksmry(sv2)
tabl("Quick summary table of variables in Sv files.")
para(paste0("Figures 1-9 are exploratory plots of the Sv data that can be examined to check for potential problems.  ",
"Any figures after that (but before the TS section) indicate problems in the data.  ",
"Minimum and maximum values were compared with middle or mean values for each of the following variables: Sv, Depth, Ping, Dist, Date, Lat, Long.  ",
"If any problems were indicated by these comparisons, they will appear as figures after Figure 9 but before the TS section.  "))
attach(sv2)
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, 1:35])
}
figu("Plot of the first 35 variables in the Sv files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 5), mar=c(3, 3, 2, 1))
plot.df(sv2[, -(1:35)])
}
figu("Plot of variables 36+ in the Sv files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Lat_M, Lon_M, as.numeric(as.factor(Region_name)), "Colors indicate Region")
textg(Lat_M, Lon_M, Region_name)
}
figu("Location of transects in Sv files.  Colors indicate Region.", newpage="port")
# close up look at each transect - lat/long
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=n2mfrow(length(sur)), mar=c(1, 0, 2, 0), cex=1.2)
for(i in order(lat.r, decreasing=F)) {
sel <- Region_name==sur[i]
map("usa", xlim=range(Lon_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, ylim=range(Lat_M[sel], na.rm=TRUE)+c(-1, 1)*0.01, mar=c(1, 0, 2, 0), type="n")
points(Lon_M[sel], Lat_M[sel], col=as.numeric(as.factor(Region_name))[sel])
mtext(sur[i], side=3, cex=1.5)
box()
}
}
figu("Close up look at each transect location in Sv files.", newpage="port")
# interval by layer plots
fig <- function() plotil(rain.n(-Depth_mean), "Colors indicate Depth_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Depth_mean.", newpage="port")
fig <- function() plotil(rain.n(Sv_mean), "Colors indicate Sv_mean")
figu("Interval by layer plots for Sv files.  Colors indicate Sv_mean.", newpage="port")
fig <- function() plotil(rain.n(PRC_ABC^0.2), "Colors indicate PRC_ABC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_ABC.", newpage="port")
fig <- function() plotil(rain.n(PRC_NASC^0.2), "Colors indicate PRC_NASC")
figu("Interval by layer plots for Sv files.  Colors indicate PRC_NASC.", newpage="port")
fig <- function() plotil(rain.n(Samples_In_Domain), "Colors indicate Samples")
figu("Interval by layer plots for Sv files.  Colors indicate Samples.", newpage="port")
# plots comparing extremes with middle values
fig <- function() plot3(Ping_S, Ping_E, Ping_M, varname="Ping")
np <- plot3(Ping_S, Ping_E, Ping_M, test=TRUE)
if(is.null(np)) figu("Comparing Ping_S, Ping_E, and Ping_M for Sv files.", newpage="port")
fig <- function() plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, varname="Dist")
np <- plot3(Dist_S, Dist_E, Dist_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Dist_S, Dist_E, and Dist_M for Sv files.", newpage="port")
fig <- function() plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), varname="Date")
np <- plot3(decimal_date(date.s), decimal_date(date.e), decimal_date(date.m), test=TRUE)
if(is.null(np)) figu("Comparing Date_S, Date_E, and Date_M for Sv files.", newpage="port")
fig <- function() plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, varname="Latitude")
np <- plot3(Lat_S, Lat_E, Lat_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lat_S, Lat_E, and Lat_M for Sv files.", newpage="port")
fig <- function() plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, varname="Longitude")
np <- plot3(Lon_S, Lon_E, Lon_M, order.matters=FALSE, test=TRUE)
if(is.null(np)) figu("Comparing Lon_S, Lon_E, and Lon_M for Sv files.", newpage="port")
detach(sv2)
rm(sur, lat.r, lon.r, np)
### TS
head2("TS FILES")
para(paste0("The TS files have ", dim(ts2)[1], " rows and ", dim(ts2)[2], " columns."))
tab <- qksmry(ts2)
tabl("Quick summary table of variables in TS files.")
attach(ts2)
sur <- sort(unique(Region_name))
lon.r <- tapply(Lon_M, Region_name, mean, na.rm=TRUE)
lat.r <- tapply(Lat_M, Region_name, mean, na.rm=TRUE)
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 1:28])
}
figu("Plot of the first 28 variables in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 28+(1:28)])
}
figu("Plot of variables 29-56 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, 56+(1:28)])
}
figu("Plot of variables 57-84 in the TS files.", newpage="port")
fig <- function() {
par(mfrow=c(7, 4), mar=c(3, 3, 2, 1))
plot.df(ts2[, -(1:84)])
}
figu("Plot of variables 85+ in the TS files.", newpage="port")
# interval by layer plots
start <- seq(16, 66, 10)
end <- start+10
for(i in seq(along=start)) {
colz <- paste("X.", start[i]:end[i], ".000000", sep="")
sumtargs <- apply(ts2[, colz], 1, sum)
title. <- paste("Colors indicate binned targets from, -", end[i], " to -", start[i], " dB", sep="")
fig <- function() plotil(rain.n(sqrt(sumtargs)), title.)
figu(paste("Interval by layer plots for TS files. ", title.), newpage="port")
}
detach(ts2)
rm(sur, lon.r, lat.r, start, end, i, colz, sumtargs, title.)
#####################################################################################################################################################
# compare interval gaps in Sv and TS files
head2("SV and TS FILES COMPARISON")
t1 <- table(sv2$Interval, sv2$Region_name)
t2 <- table(ts2$Interval, ts2$Region_name)
# create matrices with all the intervals and all the regions
iu <- union(rownames(t1), rownames(t2))
iu <- iu[order(as.numeric(iu))]
ju <- union(colnames(t1), colnames(t2))
bigt1 <- matrix(0, nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
bigt2 <- bigt1
bigt1[rownames(t1), colnames(t1)] <- t1
bigt2[rownames(t2), colnames(t2)] <- t2
results <- matrix("", nrow=length(iu), ncol=length(ju), dimnames=list(iu, ju))
# assign values when rows/columns don't match
results[bigt1 < 0.5 & bigt2 > 0.5] <- "Gap in Sv"
results[bigt1 > 0.5 & bigt2 < 0.5] <- "Gap in TS"
res <- results[apply(results!="", 1, sum) > 0, apply(results!="", 2, sum) > 0]
if(sum(dim(res)) > 0) {
tab <- res
tabl("Interval gaps in Sv and TS files don't match up.")
} else {
para("Interval gaps in Sv and TS files match up.")
}
rm(t1, t2, iu, ju, bigt1, bigt2, results, res)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# explore trawl files
### OPTROP
head2("OP and TROP FILES")
optrop2 <- missings(optrop)
para(paste0("The OP/TROP files have ", dim(optrop2)[1], " rows and ", dim(optrop2)[2], " columns."))
tab <- qksmry(optrop2)
tabl("Quick summary table of variables in OP/TROP files.")
attach(optrop2)
pcols <- c("Op.Id", "Vessel", "Cruise", "Serial", "Lake", "Port", 
"Beg.Depth", "End.Depth", "Distance", "Fishing_Temp", "Fishing_Depth", "Transect")
tab <- optrop2[is.na(Beg.Depth) | is.na(End.Depth) | is.na(Distance) | is.na(Fishing_Temp), pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with missing depth, distance, or temperature.", newpage="land")
} else {
para("All OP/TROP records have depth, distance, and temperature entered.")
}
set.time <- floor(Set_Time/100) + (Set_Time - 100*floor(Set_Time/100))/60
tod <- rep("night", length(set.time))
tod[set.time > 7 & set.time < 19] <- "day"
tt <- table(tod)
mostall <- names(which.max(table(tod)))
if(length(tt)<1.5) {
if(mostall=="night") {
para("All OP/TROP records were taken at night.")
} else {
para("All OP/TROP records were taken during the day.")
}
} else {
if(mostall=="night") {
tab <- optrop2[tod=="day", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken at night, but some were taken during the day.")
} else {
tab <- optrop2[tod=="night", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken during the day, but some were taken at night.")
}
}
tab <- optrop2[!is.na(Beg.Depth) & !is.na(End.Depth) & abs(Beg.Depth - End.Depth) > 20, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with > 20 m difference between beginning and ending bottom depth.")
} else {
para("All OP/TROP records have < 20 m difference between beginning and ending bottom depth.")
}
mind <- pmin(Beg.Depth, End.Depth, na.rm=T)
tab <- optrop2[!is.na(mind) & !is.na(Fishing_Depth) & Fishing_Depth > mind, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with fishing depth > beginning or ending bottom depth.")
} else {
para("All OP/TROP records have fishing depths < beginning and ending bottom depths.")
}
fig <- function() {
par(mfrow=c(6, 4), mar=c(3, 3, 2, 1))
plot.df(optrop2)
}
figu("Plot of variables in the OP/TROP files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Port))), "Colors indicate Port", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Port)
}
figu("Identification of ports in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Cruise))), "Colors indicate Cruise", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Cruise)
}
figu("Identification of cruises in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Transect))), "Colors indicate Transect", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Transect)
}
figu("Identification of transects in OP/TROP files.", newpage="port")
maxd <- pmax(Beg.Depth, End.Depth, na.rm=T)
fig <- function() {
plotmap(Latitude, Longitude, rain.n(-maxd), "Colors indicate Bottom Depth", pch=16, xla=0.15, yla=0.1)
}
figu("Bottom depths in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(Tow_Time), "Colors indicate Tow_Time", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tow_Time)
}
figu("Identification of tow times in OP/TROP files.", newpage="port")
if("Tr_Design" %in% names(optrop2)) {
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Tr_Design))), "Colors indicate Tr_Design", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tr_Design)
}
figu("Identification of trawl designs in OP/TROP files.", newpage="port")
}
detach(optrop2)
rm(set.time, tod, tt, mostall, mind, maxd)
### trcatch
addPageBreak(doc, width=11, height=8.5)
head2("TRCATCH FILE")
trcatch2 <- missings(trcatch)
para(paste0("The TRCATCH file has ", dim(trcatch2)[1], " rows and ", dim(trcatch2)[2], " columns."))
tab <- qksmry(trcatch2)
tabl("Quick summary table of variables in TRCATCH file.")
attach(trcatch2)
sus <- sort(unique(Species))
if("Beg.Depth" %in% names(trcatch2)) {
tab <- trcatch2[is.na(Beg.Depth) | is.na(End.Depth), 
c("Op.Id", "Year", "Vessel", "Serial", "Lake", "Species", "Port_Name", "Beg.Depth", "End.Depth", "N")]
if(dim(tab)[1] > 0) {
tabl("TRCATCH records with missing beginning or ending depth.")
} else {
para("All TRCATCH records have beginning and ending depth entered.")
}
}
fig <- function() {
par(mfrow=c(5, 4), mar=c(3, 3, 2, 1))
plot.df(trcatch2)
plotsp(N, "N")
plotsp(Weight, "Weight")
plotsp(Weight/N, "Weight/N")
}
figu("Plot of variables in the TRCATCH file.", newpage="port")
detach(trcatch2)
rm(sus)
### trlf
head2("TRLF FILE")
trlf2 <- missings(trlf)
para(paste0("The TRCATCH file has ", dim(trlf2)[1], " rows and ", dim(trlf2)[2], " columns."))
tab <- qksmry(trlf2)
tabl("Quick summary table of variables in TRLF file.")
attach(trlf)
fig <- function() {
par(mfrow=c(5, 3), mar=c(3, 3, 2, 1))
plot.df(trlf)
}
figu("Plot of variables in the TRLF file.", newpage="port")
sus <- sort(unique(Species))
fig <- function() {
par(mfrow=n2mfrow(length(sus)), mar=c(3, 3, 2, 1), oma=c(2, 2, 1, 1), yaxs="i", cex=1)
for(i in seq(along=sus)) {
sel <- Species==sus[i]
a <- hist(rep(Length[sel], N[sel]), breaks=seq(-5, max(Length)+5, 5), plot=FALSE)
xr <- range(Length[sel])
hist(rep(Length[sel], N[sel]), xlim=xr+10*c(-1, 1), ylim=c(0, max(a$counts))*1.05, 
breaks=seq(-5, max(Length)+5, 5), col="blue", main=sus[i], las=1)
abline(v=xr, lwd=2, col="red")
box()
}
mtext("Length  (mm)", side=1, outer=TRUE, cex=1.5)
mtext("Frequency", side=2, outer=TRUE, cex=1.5)
}
figu("Length frequency histograms of species in the TRLF file.  Vertical red lines indicate the minimum and maximum lengths recorded.", 
newpage="port")
detach(trlf)
rm(trlf2, sus) #, i, sel, sul, a, xr)
### key106
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
head2("Alewife Age-Length Key")
m <- key106[-25, -(1:2)]
dimnames(m)[[1]] <- key106$mmgroup[-25]
dimnames(m)[[2]] <- substring(dimnames(m)[[2]], 4, 4)
m <- as.matrix(m)
m2 <- m[apply(m, 1, sum) > 0, apply(m, 2, sum) > 0]
para(paste0("The alewife age-length key has ", dim(m2)[1], " length categories and ", dim(m2)[2], " age categories."))
tab <- m2
tabl("Alewife age-length key.")
fig <- function() {
par(mar=c(4, 4, 2, 1), cex=1.5)
plot.matrix(m2, inc=0.2, xlab="Length  (mm)", ylab="Age", main="Alewife Age-Length Key")
}
figu("Alewife age-length key.  Circle size is proportional to probability of age, given length.",
"  Probabilities for all ages of a given length sum to one.", newpage="port")
rm(m, m2)
}
}
end.rtf()
{# save R data to file
rdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
alwayskeep <- c("LAKE", "YEAR", "maindir", "rdatname", "get.packages", "lakenames", "rain.n", "scale.02n", "myrecode", "explore", 
"start.rtf", "head1", "head2", "head3", "para", "tabl", "figu", "end.rtf", "explore.ac", "explore.ac", "use.alewife.ages",
"svdir", "tsdir")
keep1 <- c("trcatch", "optrop", "trlf")
keep1.b <- "key106"
keep10 <- c("sv2", "ts2")
if(explore==11) {
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
save(list=c(alwayskeep, keep10, keep1, keep1.b), file=paste0(maindir, rdatname))
} else {
save(list=c(alwayskeep, keep10, keep1), file=paste0(maindir, rdatname))
}
} else {
if(explore==1) {
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
save(list=c(alwayskeep, keep1, keep1.b), file=paste0(maindir, rdatname))
} else {
save(list=c(alwayskeep, keep1), file=paste0(maindir, rdatname))
}
} else {
if(explore==10) {
save(list=c(alwayskeep, keep10), file=paste0(maindir, rdatname))
}
}
}
rm(fig, tab)
}
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
para(if(use.alewife.ages) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(casefold(substring(use.alewife.ages, 1, 1))=="y" & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 1521.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & use.alewife.ages) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lakewide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lakewide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- as.matrix(format(round(laketots.n), big.mark=","))
tabl("Lakewide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tabl("Lakewide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
aleage <- casefold(substring(use.alewife.ages, 1, 1))=="y"
para(if(aleage) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(aleage & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 1521.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(aleage) {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & use.alewife.ages) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lakewide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lakewide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- as.matrix(format(round(laketots.n), big.mark=","))
tabl("Lakewide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tabl("Lakewide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
aleage <- casefold(substring(use.alewife.ages, 1, 1))=="y"
para(if(aleage) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(aleage & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 1521.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(aleage) {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & aleage) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lakewide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lakewide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- as.matrix(format(round(laketots.n), big.mark=","))
tabl("Lakewide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tabl("Lakewide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
tabl
conflicts()
jvamisc::tabl
?rtf
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tab
tab <- round(laketots.g)
tab
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
aleage <- casefold(substring(use.alewife.ages, 1, 1))=="y"
para(if(aleage) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(aleage & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 1521.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(aleage) {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & aleage) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lake-wide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lake-wide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- round(laketots.n)
tabl("Lake-wide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- round(laketots.g)
tabl("Lake-wide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
cleanup()
search()
# C:\JVA\Lamprey\ChemControl\Resistance\Analysis\AnalyzeRaw.r
library(lubridate)
library(LW1949)
library(lattice)
load("C:/JVA/Lamprey/ChemControl/Resistance/Analysis/RawData.RData")
all.main$Year <- year(all.main$Start_Date)
mi <- match(all.sub$ID, all.main$ID)
all.sub$Year <- all.main$Year[mi]
all.main$fg <- paste(all.main$Folder, all.main$group)
all.sub$fg <- all.main$fg[mi]
rm(mi)
mytable(all.main$Folder)
with(all.main, tapply(ID, Folder, range))
with(all.main, tapply(group, Folder, range))
cleanup()
q()
source("C:/JVA/Lamprey/ChemControl/Resistance/Slaght/ReadSlaght.r")
ls()
rm(leavethese, sel, xxx)
ls()
head(Slaghtdat)
head(Slaghtdat)
dat <- Slaghtdat
rm(Slaghdat)
rm(Slaghtdat)
plotdf(dat)
.SavedPlots <- NULL
.SavedPlots <- NULL
stringin("ph", names(dat))
stringin
grep("s", dat$species, ignore.case=TRUE)
?grep
grepl("s", dat$species, ignore.case=TRUE)
sel <- grepl("s", dat$species, ignore.case=TRUE) & grepl("h", dat$water, ignore.case=TRUE)
plotdf(dat[sel, ])
head(dat)
plotdf(dat[!sel, ])
stringin
sel <- grepl("s", dat$species, ignore.case=TRUE, fixed=TRUE) & grepl("h", dat$water, ignore.case=TRUE, fixed=TRUE)
unique(dat$species)
grepl("s", unique(dat$species), ignore.case=TRUE)
summary(sub)
sel <- grepl("s", dat$species, ignore.case=TRUE) & grepl("h", dat$water, ignore.case=TRUE)
sub <- dat[sel, ]
plotdf(sub)
plotdf(sub)
attach(sub)
mytable(test.type)
mytable(water)
mytable(location)
mytable(lab)
mytable(acc..no.)
mytable(temp.)
mytable(chemical)
mytable(species)
mytable(total.no..tested)
mytable(Folder)
mytable(File)
mytable(Sheet)
mytable(ill)
mytable(dead)
mytable(cumulative..ill)
mytable(cumulative..dead)
mytable(temp..unit)
mytable(conc..unit)
mytable(aerated.)
mytable(general.comment..regarding.the.whole.test.)
mytable(specific.comment..regarding.just.the.corresponding.line.of.data.)
library(lubridate)
library(LW1949)
plot(start.date)
mytable(year(start.date))
sample(c(1:3, rep(0, 7)))
sample(c(1:3, rep(0, 7)))
sample(c(1:3, rep(0, 7)))
?which.max
x <- c(1:4, 0:5, 11)
which.min(x)
which.max(x)
x
x <- c(1:4, 0:4)
which.max(x)
table(x)
sort(table(x))
rev(table(x))
rev(table(x))[1]
nsim <- 10
res <- rep(NA, nsim)
for(i in 1:nsim) {
j1 <- sample(c(1:3, rep(0, 7)))
j2 <- sample(c(1:3, rep(0, 7)))
j3 <- sample(c(1:3, rep(0, 7)))
j4 <- sample(c(1:3, rep(0, 7)))
j <- j1 + j2 + j3 + j4
res[i] <- rev(table(j))[1]
}
res
hist(res)
nsim <- 1000
res <- rep(NA, nsim)
for(i in 1:nsim) {
j1 <- sample(c(1:3, rep(0, 7)))
j2 <- sample(c(1:3, rep(0, 7)))
j3 <- sample(c(1:3, rep(0, 7)))
j4 <- sample(c(1:3, rep(0, 7)))
j <- j1 + j2 + j3 + j4
res[i] <- rev(table(j))[1]
}
mytable(res)
100*mytable(res)/nsim
nsim <- 10000
res <- rep(NA, nsim)
for(i in 1:nsim) {
j1 <- sample(c(1:3, rep(0, 7)))
j2 <- sample(c(1:3, rep(0, 7)))
j3 <- sample(c(1:3, rep(0, 7)))
j4 <- sample(c(1:3, rep(0, 7)))
j <- j1 + j2 + j3 + j4
res[i] <- rev(table(j))[1]
}
100*mytable(res)/nsim
cleanup()
q()
?cheat
# C:\JVA\Admin\CenterDirector\StaffMeetings\Kostich Communications Survey\survey.r
wb <- loadWorkbook("C:/JVA/Admin/CenterDirector/StaffMeetings/Kostich Communications Survey/Communication Satisfaction Factors GLSC JVAmod.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
head(dat)
fill(dat$Group)
dat$Group <- fill(dat$Group)
head(dat)
dat[, -(1:3)]
range(dat[, -(1:3)])
range(unlist(dat[, -(1:3)]))
range(unlist(dat[, -(1:3)]), na.rm=TRUE)
(unlist(dat[, -(1:3)]))
range(unlist(dat[, -(1:3)]), na.rm=TRUE)
lapply(dat, class)
wb <- loadWorkbook("C:/JVA/Admin/CenterDirector/StaffMeetings/Kostich Communications Survey/Communication Satisfaction Factors GLSC JVAmod.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat$Group <- fill(dat$Group)
lapply(dat, class)
dat$Group <- fill(dat$Group)
dat$Fed <- as.numeric(dat$Fed)
dat$Nonfed <- as.numeric(dat$Nonfed)
dat$Mgmt <- as.numeric(dat$Mgmt)
dat$Nonmgmt <- as.numeric(dat$Nonmgmt)
sapply(dat, class)
search()
attach(dat)
range(dat[, -(1:3)], na.rm=TRUE)
mytable(dat$Group)
mytable(substring(dat$Group, 1, 10))
mytable(substring(dat$Group, 1, 9))
mytable(substring(dat$Group, 1, 8))
mytable(substring(dat$Group, 1, 7))
mytable(substring(dat$Group, 1, 6))
mytable(substring(dat$Group, 1, 5))
mytable(substring(dat$Group, 1, 4))
mytable(substring(dat$Group, 1, 1))
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
windows()
par(pty="s", mar=c(4, 4, 1, 1))
plot(Fed, Nonfed, type="n", xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
abline(0, 1, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
abline(0.6, 1, lty=2)
abline(-0.6, 1, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(0.6, 1, lty=2, lwd=3, col="lightgray")
abline(-0.6, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(0.6, 1, lty=2, lwd=3, col="lightgray")
abline(-0.6, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(Fed-Nonfed)>0.6)+1])
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
cut <- 0.56
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(0.6, 1, lty=2, lwd=3, col="lightgray")
abline(-0.6, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(Fed-Nonfed)>=cut)+1])
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
kut <- 0.56
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(Fed-Nonfed)>=kut)+1])
head(dat)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit(AA, Field, "AA", "Field")
plotit(Mgmt, Nonmgmt, "Mgmt", "Nonmgmt")
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit(Field, AA, "Field", "AA")
plotit(Nonmgmt, Mgmt, "Nonmgmt", "Mgmt")
sort(unique(Group))
legend("topleft", sort(unique(Group)), kol=1:9)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)))
substring(dat$Group, 1, 3)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(Group, 1, 1)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.5)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.7, font=2)
?legend
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.7, ptlwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.7, pt.lwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.6, pt.lwd=2)
plotit(Nonmgmt, Mgmt, "Nonmgmt", "Mgmt")
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.6, pt.lwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.59, pt.lwd=2)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(Group, 1, 1)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white", bty="n")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white", bty="n")
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="blue", bty="n")
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, box.col="white", bg="white")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(Group, 1, 1)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit(Field, AA, "Field", "AA")
plotit(Nonmgmt, Mgmt, "Nonmgmt", "Mgmt")
head(dat)
x <- dat[, -(1:3)]
apply(x, 1, mean, na.rm=TRUE)
apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
mnz <- apply(x, 1, mean, na.rm=TRUE)
rngz <- apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
plot(mnz, rngz)
plot(sort(mnz))
plot(sort(rngz))
locator()
plot(mnz, rngz)
abline(h=0.8, lty=2)
abline(v=4.5, lty=2)
plot(sort(mnz))
locator()
plot(sort(rngz))
locator()
plot(mnz, rngz)
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
x <- dat[, -(1:3)]
mnz <- apply(x, 1, mean, na.rm=TRUE)
rngz <- apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
windows()
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=)
axis(1, at=1:7, c("Very\ndissatisfied", "Dissatisfied", "Somewhat\ndissatisfied", "Indifferent",
"Somewhat\nsatisfied", "Satisfied", "Very\nsatisfied")
)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xaxt="n", xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(1, at=1:7, c("\nVery\ndissatisfied", "Dissatisfied", "Somewhat\ndissatisfied", "Indifferent",
"Somewhat\nsatisfied", "Satisfied", "Very\nsatisfied")
)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xaxt="n", xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(1, at=1:7, c("\nVery\ndissatisfied", "Dissatisfied", "Somewhat\ndissatisfied", "Indifferent",
"\nSomewhat\nsatisfied", "Satisfied", "Very\nsatisfied"))
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xaxt="n", xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(1, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", las=1, xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(4, at=c(0.2, 0.8), c("Consistent", "Discrepant")
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.2, 0.8), c("Consistent", "Discrepant")
axis(4, las=1)
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.2, 0.8), c("Consistent", "Discrepant"))
axis(4, las=1)
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.4, 0.85), c("Consistent", "Discrepant"))
axis(4, las=1)
head(dat)
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, paste0(char, Question), col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.4, 0.85), c("Consistent", "Discrepant"))
axis(4, las=1)
search()
detach()
# C:\JVA\Admin\CenterDirector\StaffMeetings\Kostich Communications Survey\survey.r
wb <- loadWorkbook("C:/JVA/Admin/CenterDirector/StaffMeetings/Kostich Communications Survey/Communication Satisfaction Factors GLSC JVAmod.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat$Group <- fill(dat$Group)
dat$Fed <- as.numeric(dat$Fed)
dat$Nonfed <- as.numeric(dat$Nonfed)
dat$Mgmt <- as.numeric(dat$Mgmt)
dat$Nonmgmt <- as.numeric(dat$Nonmgmt)
attach(dat)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
char2 <- paste0(char, Question)
x <- dat[, -(1:3)]
mnz <- apply(x, 1, mean, na.rm=TRUE)
rngz <- apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
windows()
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char2, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.4, 0.85), c("Consistent", "Discrepant"))
axis(4, las=1)
dat[mnz < 4.4, ]
cbind(mnz, dat)[mnz < 4.4, ]
a <- cbind(mnz, dat)[mnz < 4.4, ]
a[order(a[, 1])
, ]
a$Question[order(a[, 1])]
a <- cbind(mnz, dat)[mnz < 4.4, ]
a$Question[order(a[, 1])]
a <- cbind(rngz, dat)[rngz > 0.85, ]
a$Question[order(-a[, 1])]
q()
cleanup()
cleanup()
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
library(devtools)
install_github("httr")
install_github("twitteR", username="geoffjentry")
library(twitteR)
utils:::menuInstallPkgs()
library(twitteR)
?setup_twitter_oauth
??setup_twitter_oauth
utils:::menuInstallPkgs()
library(httr)
?install
cleanup()
q()
library(httr)
utils:::menuInstallPkgs()
library(httr)
library(twitteR)
library(RCurl)
?setup_twitter_oauth
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
install.packages(c("devtools", "rjson", "bit64", "httr"))
q()
library(devtools)
utils:::menuInstallPkgs()
library(devtools)
install_github("twitteR", username="geoffjentry")
?install_github
install_github("geoffjentry/twitteR")
library(twitteR)
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
updateStatus
?updateStatus
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
library(jvamisc)
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
m
m[4, ]
m[4, 2:1]
paste(m[4, 2:1], collapse=". ")
tweets <- apply(m[, 2:1], 1, paste, collapse=". ")
tweets
nchar(tweets)
updateStatus(tweets[4])
more.codes
rm(tweets)
adj <- getUser("AntigoDJ")
df <- twListToDF(tweets)
?getUser
adj <- getUser("AntigoDJ")
userTimeline(adj, n=20, excludeReplies=TRUE)
updateStatus(tweets[3])
updateStatus(totweet[3])
totweet <- apply(m[, 2:1], 1, paste, collapse=". ")
totweet
updateStatus(totweet[3])
userTimeline(adj, n=20, excludeReplies=TRUE)
unlist(userTimeline(adj, n=20, excludeReplies=TRUE))
a <- userTimeline(adj, n=20, excludeReplies=TRUE)
class(a)
unlist(a)
length(a)
a[[1]]
unlist(unlist(a))
as.vector(a)
sapply(a, "[", 1)
do.call(c, a)
do.call(rbind, a)
a <- userTimeline(adj, n=20, excludeReplies=TRUE)
a
twListToDF(a)
twListToDF(a)$text
totweet
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
totweet <- apply(m[, 2:1], 1, paste, collapse=". ")
# pull up old tweets
adj <- getUser("AntigoDJ")
tweetsdf <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
stringin(substring(totweet, 1, 10), tweetsdf$text)
substring(totweet, 1, 10)
tweetsdf$text
substring(totweet, 1, 30) %in% substring(tweetsdf$text, 1, 30)
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
# pull up old tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
# tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
totweet
?updateStatus
newtweets
m
m[3, 3]
updateStatus(newtweets[3], lat=45.141473, long=-89.152339, mediaPath=m[3, 3])
m[3, 3]
?url
oldtweets
nchar(oldtweets$text)
paste(newtweets[3], m[3, 3])
updateStatus(paste(newtweets[3], m[3, 3]), lat=45.141473, long=-89.152339)
totweet
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339))
rev(totweet)
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
oldtweets
oldtweets$created
max(oldtweets$created)
date(max(oldtweets$created))
format(max(oldtweets$created), "%M/%D/%Y")
?as.Date
date()
?date
Sys.Date
Sys.Date()
Sys.time()
format(max(oldtweets$created), tz="CST")
format(max(oldtweets$created), tz="CT")
?Sys.timezone
Sys.timezone()
format(max(oldtweets$created), tz=Sys.timezone())
max(oldtweets$created)
format(max(oldtweets$created), "%a", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b %e", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b %e %I", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone())
cleanup()
q()
library(twitteR)
library(RCurl)
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
?setup_twitter_oauth
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
cleanup()
library(twitteR)
library(RCurl)
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
### grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl, show=TRUE) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
if(show) cat(paste0("\n\n***  ", head, "\n", thisurl, "  ***\n"))
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
if(show) cat(paste0(photo.url, "\n"))
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
if(show) cat(paste0(paste(strwrap(cap, 60, indent=3), collapse="\n")), "\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
newtweets
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
totweet
if(length(totweet) > 0) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nNo new headlines posted to website since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
oldtweets
oldtweets
?updateStatus
updateStatus("this is a test", , lat=45.141473, long=-89.152339)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
oldtweets
q()
cleanup()
library(twitteR)
library(RCurl)
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
rm(api_key, api_secret, access_token, access_token_secret)
### grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl, show=TRUE) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
if(show) cat(paste0("\n\n***  ", head, "\n", thisurl, "  ***\n"))
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
if(show) cat(paste0(photo.url, "\n"))
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
if(show) cat(paste0(paste(strwrap(cap, 60, indent=3), collapse="\n")), "\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nNo new headlines posted to website since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
if(FALSE) {
install_github("geoffjentry/twitteR")
#install_github("httr")
#library(httr)
}
source("C:/JVA/Exp/ADJTweet.r")
m
m[1, 1]
download.file
?download.file
m[1, 1]
download.file(m[1, 1], "c:/temp/photo1.jpg")
download.file(m[1, 1], "c:/temp/photo1.jpg", mode="wb")
download.file(m[1, 1], "c:/temp/photo1.jpg", mode="wb")
download.file(m[1, 1], "c:/temp/photo1.jpg", method="curl")
link = "http://29.media.tumblr.com/tumblr_m0q2g8mhGK1qk6uvyo1_500.png"
download.file(link,basename(link))
getwd()
?updateStatus
updateStatus("this is a test", mediaPath="c:/temp/photo1.jpg")
updateStatus("this is a test")
cleanup()
q()
library(twitteR)
credentials <- c(
  "twitter_api_key=PQWmazlRxSLrKSW0ysq6Qe8iJ",
  "twitter_api_secret=yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3",
  "twitter_access_token=2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF",
  "twitter_access_token_secret=flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
  )
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(credentials, fname)
browseURL(fname)
normalizePath("~/")
getwd
getwd()
?normalizePath
fname <- paste0(getwd(), ".Renviron")
writeLines(credentials, fname)
browseURL(fname)
file.path(getwd(), ".Rprofile") 
file.path(getwd(), ".Renviron") 
fname <- file.path(getwd(), ".Renviron")
writeLines(credentials, fname)
fname
browseURL(fname)
q()
api_key <- Sys.getenv("twitter_api_key")
api_key
?Sys.getenv
Sys.getenv("twitter_api_key")
q()
cleanup()
Sys.getenv("twitter_api_key")
cleanup()
q()
?source
source("https://raw.githubusercontent.com/JVAdams/jvamisc/master/R/calcr2.r")
source.url("https://raw.githubusercontent.com/JVAdams/jvamisc/master/R/calcr2.r")
??source.url
library(devtools)
source_url("https://raw.githubusercontent.com/JVAdams/jvamisc/master/R/calcr2.r")
ls()
calcr2
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, newtweets=newtweets, totweet=totweet)
}
tweetadj(FALSE)
library(twitteR)
library(Rcurl)
library(RCurl)
tweetadj(FALSE)
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, newtweets=newtweets, totweet=totweet)
}
tweetadj(FALSE)
tweetadj(FALSE)
tweetadj()
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
return(list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet))
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
}
tweetadj(FALSE)
tweetadj()
a <- tweetadj()
a
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweetadj()
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweetadj(FALSE)
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweetadj()
cleanup()
q()
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- website
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(username)
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
library(RCurl)
library(twitteR)
tweethead()
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- website
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
website
base.url
username
credentials
base.url <- Sys.getenv("website")
base.url
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
a <- tweethead()
b <- a$oldtweets
b
b[, c(1, 3, 5, 12)]
b[, c(1, 3, 12, 5)]
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead
tweethead()
b
b$created
format(b$created, tz=)
format(b$created, tz=Sys.timezone())
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
oldtweets$created <- format(oldtweets$created, tz=Sys.timezone())
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
b
b$created <- format(b$created, tz=Sys.timezone())
b
class(b$created)
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)==created] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$createdUTC), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)=="created"] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$createdUTC), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot  and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
long.codes <- (min(more.codes)-2):(max(more.codes)+2)
more.urls <- paste0(base.url, "index.php?ID=", long.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)=="created"] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$createdUTC), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead(FALSE)
more.codes
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
long.codes <- (min(more.codes)-2):(max(more.codes)+2)
more.codes
more.codes <- as.numeric(substring(stringin("more", links2), 1, 5))
long.codes <- (min(more.codes)-2):(max(more.codes)+2)
more.urls <- paste0(base.url, "index.php?ID=", long.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)=="created"] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
totweet
q()
cleanup()
q()
?get
#' Incorporate ASCII Fish with Messages
#'
#' Incorporate ASCII fish with messages.
#' @param what A character scalar, your message, default "".
#' @param typeA character scalar, one of "message" (default), "warning", "error", or "string".
#' @return A message of the specified type with an ASCII fish.
#' @export
#' @references 
#' This function is based on the function say() from the cowsay package \href{https://github.com/sckott/cowsay}.
#' @seealso\code{\link{message}}, \code{\link{warning}}, \code{\link{error}}.
asciifish <- function(what="", type="message"){
fish <- paste0(
"#       O            \n"
"#        o           \n"
"#  ><(((>   artFISHal\n"
)
switch(type,
message = message(sprintf(fish, what)),
warning = warning(sprintf(fish, what)),
warning = error(sprintf(fish, what)),
string = sprintf(fish, what)
)
}
asciifish <- function(what="", type="message"){
fish <- paste0(
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n"
)
switch(type,
message = message(sprintf(fish, what)),
warning = warning(sprintf(fish, what)),
warning = error(sprintf(fish, what)),
string = sprintf(fish, what)
)
}
asciifish("Hello")
asciifish(what="Hello")
sprintf
?sprintf
?warning
asciifish(what="Hello", type="warning")
asciifish <- function(what="", type="message"){
fish <- paste0(
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n"
)
fishplus <- paste(fish, what, sep="\n")
switch(type,
message = message(fishplus),
warning = warning(fishplus),
warning = error(fishplus),
string = fishplus
)
}
asciifish(what="Hello")
asciifish(what="Hello", type="warning")
asciifish <- function(what="", type="message"){
fish <- paste0(
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n"
)
fishplus <- paste(fish, what, sep="\n")
switch(type,
message = message(fishplus),
warning = warning(paste0("\n", fishplus)),
warning = error(paste0("\n", fishplus)),
string = fishplus
)
}
asciifish(what="Hello", type="warning")
asciifish <- function(what="", type="message"){
fishplus <- paste0(
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n",
"#                    \n",
what
)
switch(type,
message = message(fishplus),
warning = warning(paste0("\n", fishplus)),
warning = error(paste0("\n", fishplus)),
string = fishplus
)
}
asciifish(what="Hello", type="warning")
#' Incorporate ASCII Fish with Messages
#'
#' Incorporate ASCII fish with messages.
#' @param what A character scalar, your message, default "".
#' @param typeA character scalar, one of "message" (default), "warning", "error", or "string".
#' @return A message of the specified type with an ASCII fish.
#' @export
#' @references 
#' This function is based on the function say() from the cowsay package \href{https://github.com/sckott/cowsay}.
#' @seealso\code{\link{message}}, \code{\link{warning}}, \code{\link{error}}.
asciifish <- function(what="", type="message"){
fishplus <- paste0(
"#                    \n",
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n",
"#                    \n",
what
)
switch(type,
message = message(fishplus),
warning = warning(paste0("\n", fishplus)),
warning = error(paste0("\n", fishplus)),
string = fishplus
)
}
asciifish("Hello.")
asciifish("Problem here.", "warning")
asciifish("Uh oh.", "error")
asciifish("Just a string.", "string")
?error
?warning
#' Incorporate ASCII Fish with Messages
#'
#' Incorporate ASCII fish with messages.
#' @param what A character scalar, your message, default "".
#' @param typeA character scalar, one of "message" (default), "warning", "error", or "string".
#' @return A message of the specified type with an ASCII fish.
#' @export
#' @references 
#' This function is based on the function say() from the cowsay package \href{https://github.com/sckott/cowsay}.
#' @seealso\code{\link{message}}, \code{\link{warning}}, \code{\link{error}}.
asciifish <- function(what="", type="message"){
fishplus <- paste0(
"#                    \n",
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n",
"#                    \n",
what
)
switch(type,
message = message(fishplus),
warning = warning(paste0("\n", fishplus)),
warning = stop(paste0("\n", fishplus)),
string = fishplus
)
}
asciifish("Hello.")
asciifish("Problem here.", "warning")
asciifish("Uh oh.", "error")
asciifish("Just a string.", "string")
#' Incorporate ASCII Fish with Messages
#'
#' Incorporate ASCII fish with messages.
#' @param what A character scalar, your message, default "".
#' @param typeA character scalar, one of "message" (default), "warning", "error", or "string".
#' @return A message of the specified type with an ASCII fish.
#' @export
#' @references 
#' This function is based on the function say() from the cowsay package \href{https://github.com/sckott/cowsay}.
#' @seealso\code{\link{message}}, \code{\link{warning}}, \code{\link{error}}.
asciifish <- function(what="", type="message"){
fishplus <- paste0(
"#                    \n",
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n",
"#                    \n",
what
)
switch(type,
message = message(fishplus),
warning = warning(paste0("\n", fishplus)),
error = stop(paste0("\n", fishplus)),
string = fishplus
)
}
asciifish("Hello.")
asciifish("Problem here.", "warning")
asciifish("Uh oh.", "error")
asciifish("Just a string.", "string")
stop(asciifish("There must be ... "))
stop(asciifish("There must be ... "), "string")
stop("a", "b")
stop("There must be")
fishplus <- paste0(
"#                    \n",
"#       O            \n",
"#        o           \n",
"#  ><(((>   artFISHal\n",
"#                    \n")
fishplus
stop(fishplus, "There must be ...")
stop("\n", fishplus, "There must be ...")
warning("\n", fishplus, "There must be ...")
warning(fishplus, "There must be ...")
# ASCII fish for warning and error messages
fishwarn <- paste0(
"#                     \n",
"#       O             \n",
"#        o            \n",
"#  ><(((>   artiFISHal\n",
"#                     \n")
fisherr <- paste0(
"\n",
"#                     \n",
"#       O             \n",
"#        o            \n",
"#  ><(((>   artiFISHal\n",
"#                     \n")
stop(fisherr, "You can't do that.)
stop(fisherr, "You can't do that.")
warning(fishwarn, "You shouldn't do that.")
cleanup()
# ASCII fish for warning and error messages
fishwarn <- paste0(
"#                     \n",
"#       O             \n",
"#        o            \n",
"#  ><(((>   artiFISHal\n",
"#                     \n")
fisherr <- paste0(
"\n",
"#                     \n",
"#       O             \n",
"#        o            \n",
"#  ><(((>   artiFISHal\n",
"#                     \n")
save(list=ls(all=FALSE), file="C:/JVA/GitHub/artiFISHal/data/misc.RData")
cleanup()
# ASCII fish for warning and error messages
fishwarn <- paste0(
"#                     \n",
"#       O             \n",
"#        o            \n",
"#  ><(((>   artiFISHal\n",
"#                     \n")
fisherr <- paste0(
"\n",
"#                     \n",
"#       O             \n",
"#        o            \n",
"#  ><(((>   artiFISHal\n",
"#                     \n")
save(list=ls(all=FALSE), file="C:/JVA/GitHub/artiFISHal/data/misc.RData")
fisherr
#' Combine Acoustic and Midwater Trawl Survey Data
#'
#' Combine survey data from acoustic transects and midwater trawl tows (created by \code{\link{SampFish}}).  
#' Apply availability to the acoustic data and catchability (availability and selectivity) to the midwater trawl catch.
#'
#' @param SimPop A list with elements \code{LakeInfo}, \code{FishInfo}, \code{FishParam}, \code{FishPop}, typically output from \code{\link{SimFish}}.
#' See \code{\link{SimFish}} for details on the list elements.
#' @param AcMtSurv A list with elements \code{Targets}, \code{AcSummaryCell}, \code{AcSummaryColumn}, \code{MtCatch}, 
#' typically output from \code{\link{SampFish}}.
#' @param AcExclA numeric vector of length 2, depth of acoustic "dead" zones at the surface and at the bottom (in m), 
#' default of c(0, 0) represents 100\% acoustic availability of fish.
#' @param MtExclA numeric vector of length 2, depth of zones unfishable with the midwater trawl at the surface and at the bottom (in m), 
#' default of c(0, 0) represents 100\% midwater trawl availability of fish.
#' @param PanelProps A numeric vector of length 4, size of the different mesh panel zones of the midwater trawl, 
#' mouth (outermost), middle, aft, and cod (inner), default c(0.4, 0.3, 0.2, 0.1).
#' Sizes are expressed as proportions of the distance from the outer edge of the trawl to the trawl center in both the vertical and horizontal directions,
#' and they should add up to 1.  Use \code{\link{ViewZones}} to visualize the mesh panel zones.
#' @param SelecParamA data frame with 6 columns in which each row provides the midwater trawl selectivity parameters for
#' a given fish group and mesh panel zone.
#' All columns must be completely filled in (no missing values).
#' Selectivity is assumed to be 100\% for any group-zone combination not represented as a row in the data frame.
#' For 100\% selectivity of small fish, use \code{MtL50Small = -Inf} and any slope.  
#' For 100\% selectivity of large fish, use \code{MtL50Large = Inf} and any slope. 
#' Column names and descriptions:
#' \itemize{
#'   \item \code{G} = character, a one-letter nickname for the group (e.g., fish species and life stage) used in plotting
#'   \item \code{Zone} = character, mesh panel zone, one of "mouth", "middle", "aft", or "cod"
#'   \item \code{MtL50Small} = numeric, the length (in mm) at which small fish have a 50\% probability of being captured by the trawl
#'   \item \code{MtSlopeSmall} = numeric, the (inverse) slope at which small fish probability of capture increases with length, smaller values are steeper
#'   \item \code{MtL50Large} = numeric, the length (in mm) at which large fish have a 50\% probability of being captured by the trawl
#'   \item \code{MtSlopeLarge} = numeric, the (absolute value of the inverse) slope at which large fish probability of capture decreases with length, smaller values are steeper
#' }
#' @param SeedAn integer scalar, starting seed for stochasticity incorporated in acoustic and midwater trawl catchability.  
#' Use \code{Seed} to ensure the same individual fish are included in the surveys with each call to \code{CatchComb}.  
#' Otherwise, if set to NULL, the default, a random seed is used, resulting in a different fish selection with each call to \code{CatchComb}.  
#'
#' @returnA data frame with estimated fish density (in number per ha) and biomass (in kg per ha) for each sampling event and group (species, lifestage).
#'
#' @details
#'
#' A classification tree is used to relate the catch composition of the midwater trawl to the location of the trawl in the lake 
#' (e.g., MTReast, ACnorth, MTRd2sh, MTRbdep).  This tree is then used to assign a single midwater trawl catch to each acoustic cell
#' (interval x layer), such that the estimated acoustic densities can be assigned to specific fish groups (species, life stages).  See,
#' for example, Yule et al. (2013).
#'
#' @export
#' @importrpart
#' @seealso \code{\link{SimFish}}, \code{\link{SampFish}}, \code{\link{ViewZones}}, \code{\link{TuneSelec}}.
#' @references 
#' 
#' Yule, DL, JV Adams, DM Warner, TR Hrabik, PM Kocovsky, BC Weidel, LG Rudstam, and PJ Sullivan.  2013.  
#' Evaluating analytical approaches for estimating pelagic fish biomass using simulated fish communities. 
#' Canadian Journal of Fisheries and Aquatic Sciences 70:1845-1857.
#' \emph{http://www.nrcresearchpress.com/doi/abs/10.1139/cjfas-2013-0072#.U1KYxPldXTQ}
#' 
#' @examples
#'
#' # parameters for small (a) and large (A) alewife as input to the simulator
#' fishp <- data.frame(
#' G = c("a", "A", "A"), 
#' Z = c(50, 140, 140), ZE = c(0.25, 0.2, 0.2), 
#' LWC1 = 0.000014, LWC2 = 2.8638, LWCE = 0.18, 
#' TSC1 = -64.2, TSC2 = 20.5, TSCE = c(0.02, 0.07, 0.07), 
#' PropN = c(0.55, 0.25, 0.20), 
#' E = c(NA, 900, 2800), EE = c(NA, 4.5, 0.3), 
#' N = NA, NE = NA, 
#' WD = c(5, 15, 15), WDE = c(0.5, 0.7, 0.7), 
#' D2B = NA, D2BE = NA)
#' 
#' # simulate the fish population
#' res <- SimFish(LakeName="Clear Lake", LkWidth=3000, LkLength=2000, 
#'BotDepMin=20, BotDepMax=100, FishParam=fishp, TotNFish=50000)
#'
#' # survey the population
#' surv <- SampFish(SimPop=res, NumEvents=2, AcNum=5, AcInterval=3000, 
#'AcLayer=10, AcAngle=7, MtNum=25, MtHt=10, MtWd=10, MtLen=200)
#'
#' selec <- data.frame(
#' G = c("A", "a", "A", "a", "A", "a"), 
#' Zone = c("mouth", "mouth", "middle", "middle", "aft", "aft"), 
#' MtL50Small = c(100, 100, 60, 60, 30, 30), 
#' MtSlopeSmall = c(40, 40, 30, 30, 20, 20), 
#' MtL50Large = c(180, 180, Inf, Inf, Inf, Inf), 
#' MtSlopeLarge = c(20, 20, 100, 100, 100, 100))
#'
#' AcMtEst(SimPop=res, AcMtSurv=surv, Seed=927)
#' AcMtEst(SimPop=res, AcMtSurv=surv, AcExcl=c(5, 10), 
#'MtExcl=c(2, 2), SelecParam=selec, Seed=204)
#'
AcMtEst <- function(SimPop, AcMtSurv, AcExcl=c(0, 0), MtExcl=c(0, 0), PanelProps=c(0.4, 0.3, 0.2, 0.1), SelecParam=NULL, Seed=NULL) {
# SimPop=res
# AcMtSurv=surv
# SelecParam=selec
# PanelProps=c(0.4, 0.3, 0.2, 0.1)
# AcExcl=c(5, 10)
# MtExcl=c(0, 0)
# Seed=245
if(!is.null(Seed)) set.seed(Seed)
mtr <- AcMtSurv$MtCatch
srv <- AcMtSurv$SurvParam
ac <- AcMtSurv$Targets
# check validity of the trawl zone proportions that were input
names(PanelProps) <- c("mouth", "middle", "aft", "cod")
panelprops <- rev(PanelProps)
if(round(sum(panelprops), 0.0000001) != 1) stop(fisherr, "cod proportions should sum to 1")
# availability function, = 0 at surface and bottom and = 1 in the middle
dblcut <- function(wdep, surfacecut, d2bot, bottomcut) {
as.numeric(wdep > surfacecut & d2bot > bottomcut)
}
# acoustic availability
ac$keep <- with(ac, dblcut(wdep=f.wdep, surfacecut=AcExcl[1], d2bot=f.d2bot, bottomcut=AcExcl[2]))
# summarize by cell (interval x layer)
acs <- AcSmry(AcTarg=ac[ac$keep==1, ], LakeInfo=SimPop$LakeInfo, SurvParam=AcMtSurv$SurvParam)$AcCell
# midwater trawl availability
if(is.null(SelecParam)) {
SelecParam <- data.frame(
G = character(),
Zone = character(),
MtL50Small = numeric(0),
MtSlopeSmall = numeric(0),
MtL50Large = numeric(0),
MtSlopeLarge = numeric(0))
}
# check validity of zones
suz <- c("mouth", "middle", "aft", "cod")
uz <- unique(SelecParam$Zone)
badzones <- setdiff(uz, suz)
if(length(badzones) > 0) stop('Zones must be one of "mouth", "middle", "aft", or "cod".')
# check for missings
missings <- sum(is.na(SelecParam))
if(missings > 0) stop("SelectParam data frame may not have any missing values.")
# fill in 100% selectivities for group-zones with no parameters
sug <- sort(unique(AcMtSurv$MtCatch$G))
full <- expand.grid(G=sug, Zone=suz)
selec2 <- merge(SelecParam, full, all=TRUE)
sel100 <- is.na(selec2$MtL50Small)
selec2$MtL50Small[sel100] <- -Inf
selec2$MtSlopeSmall[sel100] <- 100
selec2$MtL50Large[sel100] <- Inf
selec2$MtSlopeLarge[sel100] <- 200
# for each fish, determine its maximum vertical or horizontal distance from the center of the trawl
# as a proportion of the trawl dimensions
mtr$maxdist <- with(mtr, pmax(abs(f.wdep - MTRwdep)/srv["MtHt"], abs(f.north - ACnorth)/srv["MtWd"]))
# use this distance to assign each fish to a zone of the trawl
mtr$Zone <- cut(mtr$maxdist, breaks=c(0, cumsum(panelprops)), include.lowest=TRUE, labels=names(panelprops))
mtrsel <- merge(mtr, selec2, all.x=TRUE)
# Think about trawl availability ... when we are cutting off trawls near the surface or the bottom, shouldn't this constraint happen
# during the survey itself, where trawls that encompass those "dead" zones can be eliminated?
mtrsel$p.avail <- with(mtrsel, dblcut(wdep=f.wdep, surfacecut=MtExcl[1], d2bot=f.d2bot, bottomcut=MtExcl[2]))
mtrsel$p.selec <- with(mtrsel, logit2(x=len, x50a=MtL50Small, slopea=MtSlopeSmall, x50b=MtL50Large, slopeb=-MtSlopeLarge))
mtrsel$p.catch <- mtrsel$p.avail*mtrsel$p.selec
# apply catchability (selectivity AND availability) functions to "perfect" MTR catch
mtrsel$keep <- sapply(mtrsel$p.catch, function(p) sample(0:1, size=1, replace=TRUE, prob=c(1-p, p)))
sue <- sort(unique(acs$Event))
results <- expand.grid(G=sug, Event=sue, nperha=NA, kgperha=NA)
for(k in sue) {
# subset the MT data
mtk <- mtrsel[mtrsel$Event==k & mtrsel$keep==1, ]
# only do these calculations if there were "keep" fish in the midwater trawl
# without "keep" fish, no species-specific density and biomass can be estimated
if(dim(mtk)[1] > 0) {
# subset the AC data
ack <- acs[acs$Event==k, ]
# make the variable names in mtk the same as in those in ack for tree prediction
names(mtk)[match(c("MTReast", "MTRd2sh", "MTRbdep", "MTRwdep", "MTRd2bot"), names(mtk))] <- c("interval", "d2sh", "botdep", "layer", "d2bot")
# fit a classification tree to the MTR data for Event k
treek <- rpart(as.factor(G) ~ interval + ACnorth + d2sh + botdep + layer + d2bot, data=mtk, control=list(cp=0.05, minsplit=10, minbucket=5))
# mean density for each species
# suffixes:  e = event, a = ac transect, i = interval, l = layer
# use the fitted tree to predict species composition of each AC interval/layer
pred.props <- predict(treek, newdata=ack)
dens.eail <- ack$nperha*pred.props
dens.eai <- aggregate(dens.eail, ack[, c("ACid", "interval")], sum)
dens.e <- apply(dens.eai[, names(dens.eai) %in% sug], 2, mean)
# mean biomass for each species
# calculate the mean weight of each group at each node of the fitted tree
mwt <- tapply(mtk$wt, list(treek$where, mtk$G), mean)
mwt[is.na(mwt)] <- 0
# proportions corresponding to each node
pred.node <- prednode(treek, newdata=ack)
# mean weigth expanded to full dimensions of ack, by matching node number
mwt.eail <- mwt[match(pred.node, row.names(mwt)), ]
bio.eail <- mwt.eail * dens.eail
bio.eai <- aggregate(bio.eail, ack[, c("ACid", "interval")], sum)
bio.e <- apply(bio.eai[, names(bio.eai) %in% sug], 2, mean)
for(g in seq(sug)) {
sel <- results$Event==k & results$G==sug[g]
results$nperha[sel] <- dens.e[sug[g]]
results$kgperha[sel] <- bio.e[sug[g]]/1000
}
}
}
results$nperha[is.na(results$nperha)] <- 0
results$kgperha[is.na(results$kgperha)] <- 0
results[, c("Event", "G", "nperha", "kgperha")]
}
tweethead()
#' Visualize the Mesh Panel Zones of a Midwater Trawl
#'
#' Draw a diagram displaying the sizes of the mesh panel zones of a midwater trawl.
#'
#' @param PanelProps A numeric vector of length 4, size of the different mesh panel zones of the midwater trawl, 
#' mouth (outermost), middle, aft, and cod (inner), default c(0.4, 0.3, 0.2, 0.1).
#' Sizes are expressed as proportions of the distance from the outer edge of the trawl to the trawl center in both the vertical and horizontal directions,
#' and they should add up to 1.
#'
#' @details
#'
#' A diagram is produced giving a view of the midwater trawl mesh panel zones (drawn to scale) from the perspective a fish 
#' located directly in the center of the oncoming trawl path.
#'
#' @export
#' @import MASS
#' @seealso \code{\link{AcMtEst}}
#' @examples
#'
#' \dontrun{
#' ViewZones()
#' ViewZones(c(0.4, 0.4, 0.1, 0.1))
#' }
#'
ViewZones <- function(PanelProps=c(0.4, 0.3, 0.2, 0.1)) {
# check validity of the trawl zone proportions that were input
if(round(sum(PanelProps), 0.0000001) != 1) stop(fisherr, "Panel proportions should sum to 1.")
mouth.edge <- 1
middle.edge <- 1 - PanelProps[1]
aft.edge <- middle.edge - PanelProps[2]
cod.edge <- aft.edge - PanelProps[3]
dev.new(rescale="fit")
par(mar=rep(0.1, 4), cex=2)
eqscplot(0, 0, xlim=c(-1, 1), ylim=c(-1, 1), type="n", axes=FALSE, xlab="", ylab="")
polygon(mouth.edge*c(-1, 1, 1, -1), mouth.edge*c(-1, -1, 1, 1), col=gray(0.4))
polygon(middle.edge*c(-1, 1, 1, -1), middle.edge*c(-1, -1, 1, 1), col=gray(0.6))
polygon(aft.edge*c(-1, 1, 1, -1), aft.edge*c(-1, -1, 1, 1), col=gray(0.8))
polygon(cod.edge*c(-1, 1, 1, -1), cod.edge*c(-1, -1, 1, 1), col=gray(1))
text(-middle.edge, middle.edge, "Mouth", pos=3, offset=0.1, col=gray(1))
text(-aft.edge, aft.edge, "Middle", pos=3, offset=0.1, col=gray(0))
text(-cod.edge, cod.edge, "Aft", pos=3, offset=0.1, col=gray(0.2))
text(0, 0, "Cod", col=gray(0.4))
}
ViewZones(c(0.4, 0.4, 0.1, 0.1))
ViewZones(c(0.4, 0.4, 0.1, 0.2))
ViewZones(c(0.4, 0.4, 0.1, 0.5))
ViewZones(c(0.4, 0.4, 0.1, 0.0))
ViewZones
fisherr
ViewZones
stop(fisherr, "Panel proportions should sum to 1.")
ViewZones(PanelProps=c(0.4, 0.4, 0.1, 0.0))
ViewZones(PanelProps=c(0.4, 0.2, 0.1, 0.0))
PanelProps=c(0.4, 0.4, 0.1, 0.0)
round(sum(PanelProps), 0.0000001) != 1
round(sum(PanelProps), 0.0000001)
round(sum(PanelProps), 7)
#' Visualize the Mesh Panel Zones of a Midwater Trawl
#'
#' Draw a diagram displaying the sizes of the mesh panel zones of a midwater trawl.
#'
#' @param PanelProps A numeric vector of length 4, size of the different mesh panel zones of the midwater trawl, 
#' mouth (outermost), middle, aft, and cod (inner), default c(0.4, 0.3, 0.2, 0.1).
#' Sizes are expressed as proportions of the distance from the outer edge of the trawl to the trawl center in both the vertical and horizontal directions,
#' and they should add up to 1.
#'
#' @details
#'
#' A diagram is produced giving a view of the midwater trawl mesh panel zones (drawn to scale) from the perspective a fish 
#' located directly in the center of the oncoming trawl path.
#'
#' @export
#' @import MASS
#' @seealso \code{\link{AcMtEst}}
#' @examples
#'
#' \dontrun{
#' ViewZones()
#' ViewZones(c(0.4, 0.4, 0.1, 0.1))
#' }
#'
ViewZones <- function(PanelProps=c(0.4, 0.3, 0.2, 0.1)) {
# check validity of the trawl zone proportions that were input
if(round(sum(PanelProps), 7) != 1) stop(fisherr, "Panel proportions should sum to 1.")
mouth.edge <- 1
middle.edge <- 1 - PanelProps[1]
aft.edge <- middle.edge - PanelProps[2]
cod.edge <- aft.edge - PanelProps[3]
dev.new(rescale="fit")
par(mar=rep(0.1, 4), cex=2)
eqscplot(0, 0, xlim=c(-1, 1), ylim=c(-1, 1), type="n", axes=FALSE, xlab="", ylab="")
polygon(mouth.edge*c(-1, 1, 1, -1), mouth.edge*c(-1, -1, 1, 1), col=gray(0.4))
polygon(middle.edge*c(-1, 1, 1, -1), middle.edge*c(-1, -1, 1, 1), col=gray(0.6))
polygon(aft.edge*c(-1, 1, 1, -1), aft.edge*c(-1, -1, 1, 1), col=gray(0.8))
polygon(cod.edge*c(-1, 1, 1, -1), cod.edge*c(-1, -1, 1, 1), col=gray(1))
text(-middle.edge, middle.edge, "Mouth", pos=3, offset=0.1, col=gray(1))
text(-aft.edge, aft.edge, "Middle", pos=3, offset=0.1, col=gray(0))
text(-cod.edge, cod.edge, "Aft", pos=3, offset=0.1, col=gray(0.2))
text(0, 0, "Cod", col=gray(0.4))
}
ViewZones(c(0.4, 0.4, 0.1, 0.1))
ViewZones(c(0.4, 0.4, 0.1, 0.2))
#' Visualize the Mesh Panel Zones of a Midwater Trawl
#'
#' Draw a diagram displaying the sizes of the mesh panel zones of a midwater trawl.
#'
#' @param PanelProps A numeric vector of length 4, size of the different mesh panel zones of the midwater trawl, 
#' mouth (outermost), middle, aft, and cod (inner), default c(0.4, 0.3, 0.2, 0.1).
#' Sizes are expressed as proportions of the distance from the outer edge of the trawl to the trawl center in both the vertical and horizontal directions,
#' and they should add up to 1.
#'
#' @details
#'
#' A diagram is produced giving a view of the midwater trawl mesh panel zones (drawn to scale) from the perspective a fish 
#' located directly in the center of the oncoming trawl path.
#'
#' @export
#' @import MASS
#' @seealso \code{\link{AcMtEst}}
#' @examples
#'
#' \dontrun{
#' ViewZones()
#' ViewZones(c(0.4, 0.4, 0.1, 0.1))
#' }
#'
ViewZones <- function(PanelProps=c(0.4, 0.3, 0.2, 0.1)) {
# check validity of the trawl zone proportions that were input
if(round(sum(PanelProps), 7) != 1) stop("Panel proportions should sum to 1.", fisherr)
mouth.edge <- 1
middle.edge <- 1 - PanelProps[1]
aft.edge <- middle.edge - PanelProps[2]
cod.edge <- aft.edge - PanelProps[3]
dev.new(rescale="fit")
par(mar=rep(0.1, 4), cex=2)
eqscplot(0, 0, xlim=c(-1, 1), ylim=c(-1, 1), type="n", axes=FALSE, xlab="", ylab="")
polygon(mouth.edge*c(-1, 1, 1, -1), mouth.edge*c(-1, -1, 1, 1), col=gray(0.4))
polygon(middle.edge*c(-1, 1, 1, -1), middle.edge*c(-1, -1, 1, 1), col=gray(0.6))
polygon(aft.edge*c(-1, 1, 1, -1), aft.edge*c(-1, -1, 1, 1), col=gray(0.8))
polygon(cod.edge*c(-1, 1, 1, -1), cod.edge*c(-1, -1, 1, 1), col=gray(1))
text(-middle.edge, middle.edge, "Mouth", pos=3, offset=0.1, col=gray(1))
text(-aft.edge, aft.edge, "Middle", pos=3, offset=0.1, col=gray(0))
text(-cod.edge, cod.edge, "Aft", pos=3, offset=0.1, col=gray(0.2))
text(0, 0, "Cod", col=gray(0.4))
}
ViewZones(c(0.4, 0.4, 0.1, 0.2))
warning("Acoustic transects are too close together.\nTry again with fewer acoustic transects.", fishwarn)
warning("Acoustic transects are too close together.\nTry again with fewer acoustic transects.", fisherr)
cleanup()
# ASCII fish for warning and error messages
fisherr <- paste0(
"\n",
"#                     \n",
"#       O             \n",
"#        o            \n",
"#  ><(((>   artiFISHal\n",
"#                     \n")
save(list=ls(all=FALSE), file="C:/JVA/GitHub/artiFISHal/data/misc.RData")
?cheat
pkgup("jvamisc")
q()
