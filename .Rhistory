a <- read.csv("C:\\JVA\\Lamprey\\Adults\\SpawnDisModel\\2009\\StreamPEDynamic.csv", as.is=T)
b <- fread("C:\\JVA\\Lamprey\\Adults\\SpawnDisModel\\2009\\StreamPEDynamic.csv")
a <- read.csv("C:\\JVA\\Lamprey\\Adults\\SpawnDisModel\\2009\\StreamPEDynamic.csv", as.is=T)
b <- fread("C:\\JVA\\Lamprey\\Adults\\SpawnDisModel\\2009\\StreamPEDynamic.csv")
a <- read.csv("C:\\JVA\\Lamprey\\Adults\\SpawnDisModel\\2009\\StreamPEDynamic.csv", as.is=T)
b <- fread("C:\\JVA\\Lamprey\\Adults\\SpawnDisModel\\2009\\StreamPEDynamic.csv")
dim(a)
dim(b)
head(a)
head(b)
all.equal(a, b)
cleanup()
q()
search()
.First()
.First <- function() {
# this function is in the Rprofile.site text file
options(chmhelp=FALSE, scipen=4, stringsAsFactors=F)
.SavedPlots <- NULL
# attach packages
library(rJava)
library(XLConnect)
library(maps)
library(mapproj)
library(RColorBrewer)
library(abind)
library(boot)
library(mgcv)
library(MASS) 
library(jvamisc)
# my personal functions
already <- ls(".GlobalEnv")
if(FALSE) {
fdir <- "c:\\JVA\\R\\My R functions"
fvec <- list.files(fdir)
lapply(paste(fdir, fvec, sep="\\"), source)
now <- ls(".GlobalEnv")
save(list=now[!is.element(now, already)], file="c:\\JVA\\R\\.RData")
load(file="c:\\JVA\\R\\.RData", envir=attach(NULL, name="JVA.fcns"))
rm(list=ls("JVA.fcns"), envir=.GlobalEnv)
#vectorz()
}
print(already)
}
.First()
q()
search()
q()
# C:\JVA\Consult\Stapanian\Amphib\Next Subm 2\Three Group Approach 2.r
# which environmental variables best predict this index of amphibian biotic integrity
# relevant emails:
# 15 May 2013 - https://mail.google.com/mail/u/0/?shva=1#search/amphibian/13ea801401e50a84
#  8 Aug 2013 - https://mail.google.com/mail/u/1/?shva=1#inbox/1405defbc5969b40
# 27 Aug 2013 - https://mail.google.com/mail/u/1/?shva=1#inbox/140c042c5ba851fc
# bring in data and create initial figures
source("C:/JVA/Consult/Stapanian/Amphib/Next Subm 2/Explore Figs 2.r")
select.varz <- function(varnames, mydat, max.no.ind.var) {
# keep only those varnames that have at least two unique non-missing values
varnames <- varnames[apply(mydat[, varnames], 2, function(x) {
y <- var(x[!is.na(x)])
!is.na(y) & y>0
})]
m <- allcombs(length(varnames), 0, max.no.ind.var)
dimnames(m)[[2]] <- varnames
# right side of the formula
rightside <- apply(m, 1, function(row) paste(varnames[row==1], collapse=" + "))
# if no predictors, use 1 to represent intercept only model
rightside[rightside==""] <- "1"
form <- paste("amphibi ~", rightside)
fits <- lapply(form, function(f) lm(formula(f), dat=mydat))
# AIC table
aictab <- AICc(fits)
bigtab <- cbind(formula=rightside[aictab$model], aictab)
nmodzfit <- dim(bigtab)[1]
# top models
topmods <- bigtab[bigtab$daicc < 2, ]
# add coefficients to top model
coef <- lapply(fits[topmods$model], coef)
xmod <- unique(unlist(lapply(coef, names)))
topcoef <- t(sapply(coef, function(x) 
{
blank <- rep(0, length(xmod))
blank[match(names(x), xmod)] <- x
blank
}))
dimnames(topcoef)[[2]] <- xmod
topmods <- cbind(signif(topcoef, 3), round(topmods[, c("n", "p", "rmse", "aicc", "daicc", "aiccw")], 2))
# single best model
bestnum <- bigtab$model[1]
bestfit <- fits[[bestnum]]
xvarz <- varnames[m[bestnum, ]==1]
# output
out <- list(xvarz=xvarz, nmodzfit=nmodzfit, bestfit=bestfit, topmods=topmods, allfits=fits, allmods=bigtab, allpreds=m)
print(out[1:4])
out
}
assess.fit <- function(fit, newd) {
p <- predict(fit, newdata=newd)
y <- newd$amphibi
r <- p - y
rmse <- sqrt(mean(r^2))
r2 <- calcr2(fitted=p, observed=y, nparam=length(labels(terms(freshfita)))+1)
list(cbind(p=p, y=y, r=r), rmse=rmse, r2=r2)
}
# assign each observation to one of three groups
# ensure even distribution of groups across vegetation classes and latitudes
dat3 <- dat2[order(dat2$veg.class, dat2$lat.dd), ]
dat3$group <- rep(1:3, length.out=dim(dat2)[1])
df <- dat3
### map
attach(df)
suv <- sort(unique(veg.class))
windows(h=6.5, w=6.5)
map("state", mar=c(1, 1, 1, 1), region= "ohio", col="darkgray", lwd=2)
for(i in seq(suv)) {
sel <- veg.class==suv[i]
points(lon.dd[sel], lat.dd[sel], cex=2, lwd=2, pch=c(2, 1)[i], col=group[sel]+1)
}
#legend("bottomright", capwords(suv), pch=c(2, 1), col=c("black", "gray"), pt.lwd=2, pt.cex=2, cex=1.15, title="Veg. Class")
par(usr=c(-126, 94, 21, 193), xpd=NA)
polygon(c(-128, -128, -64, -64, -128), c(22, 52, 52, 22, 22), col="white", lwd=2)
map("state", add=T, mar=c(0, 0, 0, 0)) 
map("state", region="ohio", fill=T, add=T) 
detach(df)
# step 1, select variables
# limit the number of variables in each model, such that no more than n/10 coefficients are estimated, following Burnham and Anderson (2002)
maxxvar <- round(dim(df)[1]/3/10)
ga <- select.varz(varnames=varz1, mydat=df[df$group==1, ], max.no.ind.var=maxxvar)
gb <- select.varz(varnames=varz1, mydat=df[df$group==2, ], max.no.ind.var=maxxvar)
gc <- select.varz(varnames=varz1, mydat=df[df$group==3, ], max.no.ind.var=maxxvar)
round(ct[rev(order(ct[, "r"])), ], 4)
0.05/length(varz1)
#varnames=varz1; mydat=df[df$group==3, ]; max.no.ind.var=maxxvar;
cleanup()
q()
q <- function() quit(
quit
q <- function() quit(save=TRUE)
?quit
q()
q <- function() quit(save="yes")
q()
q
?quit
q()
# After restarting R, attach/install packages
library(devtools)
library(roxygen2)
# install from local folder
setwd("C:/JVA/GitHub")
install("jvamisc")
setwd("C:/JVA/R/Working Directory")
library(jvamisc)
q()
search()
# C:\JVA\Video\TFM\StreamAnimate.r
# READ IN XLS FILES
library(XLConnect)
wb <- loadWorkbook("C:/JVA/Video/TFM/NodesLinks.xlsx")
nodes <- readWorksheet(wb, sheet="Nodes")
links <- readWorksheet(wb, sheet="Links")
tfm <- readWorksheet(wb, sheet="TFM")
nodes <- merge(nodes, tfm, all=TRUE)
getSheets(wb)
# C:\JVA\Video\TFM\StreamAnimate.r
# READ IN XLS FILES
library(XLConnect)
wb <- loadWorkbook("C:/JVA/Video/TFM/NodesLinks.xlsx")
nodes <- readWorksheet(wb, sheet="Nodes")
links <- readWorksheet(wb, sheet="Links")
tfm <- readWorksheet(wb, sheet="TFM")
nodes <- merge(nodes, tfm, all=TRUE)
nodes
nodes[is.na(nodes$amount)] <- 0
nodes
# C:\JVA\Video\TFM\StreamAnimate.r
# READ IN XLS FILES
library(XLConnect)
wb <- loadWorkbook("C:/JVA/Video/TFM/NodesLinks.xlsx")
nodes <- readWorksheet(wb, sheet="Nodes")
links <- readWorksheet(wb, sheet="Links")
tfm <- readWorksheet(wb, sheet="TFM")
nodes <- merge(nodes, tfm, all=TRUE)
nodes$amount[is.na(nodes$amount)] <- 0
nodes
links
nodes[links$from, c("x", "y")]
nodes[links$to, c("x", "y")]
dist
dist
?dist
# calculate the distance between the links
(nodes[links$from, c("x", "y")] - nodes[links$to, c("x", "y")])^2
nodes[links$from, c("x", "y")]
nodes[links$to, c("x", "y")]
sqrt(apply((nodes[links$from, c("x", "y")] - nodes[links$to, c("x", "y")])^2, 1, sum))
interpolate
??interpolate
?approx
seq(3, 10, length=2)
seq(3, 10, length=3)
seq(3, 10, length=4)
# C:\JVA\Video\TFM\StreamAnimate.r
library(XLConnect)
# resolution in time and space
reshr <- 1
resxy <- 1
# read in the data
wb <- loadWorkbook("C:/JVA/Video/TFM/NodesLinks.xlsx")
nodes <- readWorksheet(wb, sheet="Nodes")
links <- readWorksheet(wb, sheet="Links")
tfm <- readWorksheet(wb, sheet="TFM")
rm(wb)
# combine nodes and tfm info
nodes <- merge(nodes, tfm, all=TRUE)
nodes$amount[is.na(nodes$amount)] <- 0
rm(tfm)
# create additional segments between links based on spatial resolution specified
fromxy <- nodes[links$from, c("x", "y")]
toxy <- nodes[links$to, c("x", "y")]
# calculate the distance for the links
links$dist <- sqrt(apply((fromxy - toxy)^2, 1, sum))
approx(c(fromxy[1], toxy[1]), c(fromxy[2], toxy[2]), n=round(links$dist/resxy))
fromxy[1]
fromxy
# create additional segments between links based on spatial resolution specified
fromxy <- nodes[links$from, c("x", "y")]
toxy <- nodes[links$to, c("x", "y")]
# calculate the distance for the links
links$dist <- sqrt(apply((fromxy - toxy)^2, 1, sum))
lapply(1:length(links), function(i) approx(c(fromxy[i, 1], toxy[i, 1]), c(fromxy[i, 2], toxy[i, 2]), n=round(links$dist[i]/resxy))
)
nodes
links
?approx
# create additional segments between links based on spatial resolution specified
fromxy <- nodes[links$from, c("x", "y")]
toxy <- nodes[links$to, c("x", "y")]
# calculate the distance for the links
links$dist <- sqrt(apply((fromxy - toxy)^2, 1, sum))
links$nsegs <- round(links$dist/resxy)
segz <- lapply(1:length(links), function(i) {
ftx <- c(fromxy[i, 1], toxy[i, 1])
npts <- links$nsegs[i]+1
xy <- approx(ftx, c(fromxy[i, 2], toxy[i, 2]), n=npts)
x <- xy$x
y <- xy$y
vel <- approx(ftx, nodes$velocity[c(links$from[i], links$to[i])], n=npts)
wid <- approx(ftx, nodes$width[c(links$from[i], links$to[i])], n=npts)
dep <- approx(ftx, nodes$depth[c(links$from[i], links$to[i])], n=npts)
cbind(x=x, y=y, velocity=vel, width=wid, depth=dep)
})
i <- 1
ftx <- c(fromxy[i, 1], toxy[i, 1])
npts <- links$nsegs[i]+1
xy <- approx(ftx, c(fromxy[i, 2], toxy[i, 2]), n=npts)
vel <- approx(ftx, nodes$velocity[c(links$from[i], links$to[i])], n=npts)
wid <- approx(ftx, nodes$width[c(links$from[i], links$to[i])], n=npts)
dep <- approx(ftx, nodes$depth[c(links$from[i], links$to[i])], n=npts)
cbind(x=xy$x, y=xy$y, velocity=vel, width=wid, depth=dep)
ftx <- c(fromxy[i, 1], toxy[i, 1])
npts <- links$nsegs[i]+1
xy <- approx(ftx, c(fromxy[i, 2], toxy[i, 2]), n=npts)
vel <- approx(ftx, nodes$velocity[c(links$from[i], links$to[i])], n=npts)$y
wid <- approx(ftx, nodes$width[c(links$from[i], links$to[i])], n=npts)$y
dep <- approx(ftx, nodes$depth[c(links$from[i], links$to[i])], n=npts)$y
cbind(x=xy$x, y=xy$y, velocity=vel, width=wid, depth=dep)
# create additional segments between links based on spatial resolution specified
fromxy <- nodes[links$from, c("x", "y")]
toxy <- nodes[links$to, c("x", "y")]
# calculate the distance for the links
links$dist <- sqrt(apply((fromxy - toxy)^2, 1, sum))
links$nsegs <- round(links$dist/resxy)
segz <- lapply(1:length(links), function(i) {
ftx <- c(fromxy[i, 1], toxy[i, 1])
npts <- links$nsegs[i]+1
xy <- approx(ftx, c(fromxy[i, 2], toxy[i, 2]), n=npts)
vel <- approx(ftx, nodes$velocity[c(links$from[i], links$to[i])], n=npts)$y
wid <- approx(ftx, nodes$width[c(links$from[i], links$to[i])], n=npts)$y
dep <- approx(ftx, nodes$depth[c(links$from[i], links$to[i])], n=npts)$y
cbind(x=xy$x, y=xy$y, velocity=vel, width=wid, depth=dep, dist=seq(0, links$dist, length=npts))
})
# create additional segments between links based on spatial resolution specified
fromxy <- nodes[links$from, c("x", "y")]
toxy <- nodes[links$to, c("x", "y")]
# calculate the distance for the links
links$dist <- sqrt(apply((fromxy - toxy)^2, 1, sum))
links$nsegs <- round(links$dist/resxy)
segz <- lapply(1:length(links), function(i) {
ftx <- c(fromxy[i, 1], toxy[i, 1])
npts <- links$nsegs[i]+1
xy <- approx(ftx, c(fromxy[i, 2], toxy[i, 2]), n=npts)
vel <- approx(ftx, nodes$velocity[c(links$from[i], links$to[i])], n=npts)$y
wid <- approx(ftx, nodes$width[c(links$from[i], links$to[i])], n=npts)$y
dep <- approx(ftx, nodes$depth[c(links$from[i], links$to[i])], n=npts)$y
cbind(x=xy$x, y=xy$y, velocity=vel, width=wid, depth=dep, dist=seq(0, links$dist[i], length=npts))
})
segz
links
# create additional segments between links based on spatial resolution specified
fromxy <- nodes[links$from, c("x", "y")]
toxy <- nodes[links$to, c("x", "y")]
# calculate the distance for the links
links$dist <- sqrt(apply((fromxy - toxy)^2, 1, sum))
links$nsegs <- round(links$dist/resxy)
segz <- lapply(1:dim(links)[1], function(i) {
ftx <- c(fromxy[i, 1], toxy[i, 1])
npts <- links$nsegs[i]+1
xy <- approx(ftx, c(fromxy[i, 2], toxy[i, 2]), n=npts)
vel <- approx(ftx, nodes$velocity[c(links$from[i], links$to[i])], n=npts)$y
wid <- approx(ftx, nodes$width[c(links$from[i], links$to[i])], n=npts)$y
dep <- approx(ftx, nodes$depth[c(links$from[i], links$to[i])], n=npts)$y
cbind(x=xy$x, y=xy$y, velocity=vel, width=wid, depth=dep, dist=seq(0, links$dist[i], length=npts))
})
segz
nodes
nodes$start+nodes$duration
max(nodes$start+nodes$duration, na.rm=TRUE)
q()
df.clip <- function(...) read.table("clipboard", header=TRUE, ...)
dfclip <- function(...) read.table("clipboard", header=TRUE, ...)
dfclip
dfclip()
?POSIXct
as.POSIXlt(Sys.time())
dfclip()
dfclip(sep=",")
a <- dfclip(sep=",")
a
lapply(a, class)
as.Posix(a$Vollzeit)
as.PosixCT(a$Vollzeit)
as.Posixct(a$Vollzeit)
as.POSIXct(a$Vollzeit)
a$Vollzeit <- as.POSIXct(a$Vollzeit)
a
a <- dfclip(sep=",")
a <- dfclip(sep=",")
a
b <- a
as.POSIXct(a$Vollzeit)
as.POSIXct(strptime(a$Vollzeit))
as.POSIXct(strptime(a$Vollzeit, "%Y-%m-%d %H:%M:%S")))
as.POSIXct(strptime(a$Vollzeit, "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime(a$Vollzeit, "%m/%d/%Y %H:%M"))
b$Vollzeit <- 
as.POSIXct(strptime(a$Vollzeit, "%m/%d/%Y %H:%M"))
as.POSIXct(strptime(a$Zugnacht, "%m/%d/%Y"))
b$Zugnacht <- as.POSIXct(strptime(a$Zugnacht, "%m/%d/%Y"))
a
b
b[, c(1, 3)]
dput(b[, c(1, 3)])
treat <- structure(list(Vollzeit = structure(c(1378775700, 1378776600, 
1378777500, 1378778400, 1378779300, 1378780200), class = c("POSIXct", 
"POSIXt"), tzone = ""), Zugnacht = structure(c(1378702800, 1378702800, 
1378702800, 1378702800, 1378702800, 1378702800), class = c("POSIXct", 
"POSIXt"), tzone = "")), .Names = c("Vollzeit", "Zugnacht"), class = "data.frame", row.names = c(NA, 
-6L))
treat
tapply(treat$Vollzeit, treat$Zugnacht, range)
?ddply
libray(plyr)
library(plyr)
?ddply
ddply(treat, .(Zugnacht), summarize, su=min(Vollzeit), sa=max(Vollzeit))
library(plyr)
treat <- structure(list(Vollzeit = structure(c(1378775700, 1378776600, 
1378777500, 1378778400, 1378779300, 1378780200), class = c("POSIXct", 
"POSIXt"), tzone = ""), Zugnacht = structure(c(1378702800, 1378702800, 
1378702800, 1378702800, 1378702800, 1378702800), class = c("POSIXct", 
"POSIXt"), tzone = "")), .Names = c("Vollzeit", "Zugnacht"), class = "data.frame", row.names = c(NA, 
-6L))
minmax <- ddply(treat, .(Zugnacht), summarize, su=min(Vollzeit), sa=max(Vollzeit))
merge(treat, minmax, all=TRUE)
cleanup()
search()
library(plyr)
treat <- structure(list(Vollzeit = structure(c(1378775700, 1378776600, 
1378777500, 1378778400, 1378779300, 1378780200), class = c("POSIXct", 
"POSIXt"), tzone = ""), Zugnacht = structure(c(1378702800, 1378702800, 
1378702800, 1378702800, 1378702800, 1378702800), class = c("POSIXct", 
"POSIXt"), tzone = "")), .Names = c("Vollzeit", "Zugnacht"), class = "data.frame", row.names = c(NA, 
-6L))
minmax <- ddply(treat, .(Zugnacht), summarize, su=min(Vollzeit), sa=max(Vollzeit))
treat2 <- merge(treat, minmax, all=TRUE)
treat2
q()
library(plyr)
treat <- structure(list(Vollzeit = structure(c(1378775700, 1378776600, 
1378777500, 1378778400, 1378779300, 1378780200), class = c("POSIXct", 
"POSIXt"), tzone = ""), Zugnacht = structure(c(1378702800, 1378702800, 
1378702800, 1378702800, 1378702801, 1378702801), class = c("POSIXct", 
"POSIXt"), tzone = "")), .Names = c("Vollzeit", "Zugnacht"), class = "data.frame", row.names = c(NA, 
-6L))
minmax <- ddply(treat, .(Zugnacht), summarize, su=min(Vollzeit), sa=max(Vollzeit))
treat2 <- merge(treat, minmax, all=TRUE)
treat2
treat2
search()
ls(4)
q
q()
quit
q(status=0)
find("q")
q
jvamisc::q
base::q
q()
quit()
library(XLConnect)
wb <- loadWorkbook("C:/JVA/GLFC/People/Lawrence/Chi-square question.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1])
head(dat)
library(XLConnect)
wb <- loadWorkbook("C:/JVA/GLFC/People/Lawrence/Chi-square question.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=2)
head(dat)
with(dat, table(L2, F1))
?chi.square.test
?chisq.test
M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
dimnames(M) <- list(gender = c("M", "F"),
                    party = c("Democrat", "Independent", "Republican"))
M
dput(M)
#' Chi-squared test
#'
#' Performs chi-squared contingency table tests with informative output.
#' @param x a numeric matrix
#' @param rpct a numeric scalar indicating the rounding used for printed output, default 0
#' @return a list with class "htest" containing the components described in \link{\code{chisq.test}}
#' @export
#' @seealso \link{\code{chisq.test}}
#' @examples 
#' ## From Agresti(2007) p.39
#' M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
#' dimnames(M) <- list(gender = c("M", "F"), party = c("Democrat", "Independent", "Republican"))
#' mychi(M)
mychi <- function(x, rpct=0) {
sum1 <- apply(x, 1, sum)
sum2 <- apply(x, 2, sum)
exp <- outer(sum1, sum2)/sum(x) # contribution to chi square
cont <- (x - exp)^2/exp
cutoff <- qchisq(p=0.95, df=prod(dim(x)-1))
cont.ord <- sort(as.vector(cont))
major <- rev(cont.ord[cumsum(cont.ord)>cutoff])
cat("\nExpected percentages\n")
print(round((100 * exp)/apply(exp, 1, sum), rpct))
cat("\nObserved percentages\n")
print(round((100 * x)/sum1, rpct))
cat("\nContribution to chi square\n")
print(round(cont, 1))
plot(sort(cont), pch = 16)
cat("\nCutoff\n")
print(cutoff)
cat("\nMajor players\n")
print(major)
chisq.test(x)
}
m
library(XLConnect)
wb <- loadWorkbook("C:/JVA/GLFC/People/Lawrence/Chi-square question.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=2)
# chi square test of L2 and F1
m <- with(dat, table(L2, F1))
m
mychi(m)
with(dat, chisq.test(L2, F1))
chisq.test(as.factor(dat$L2), as.factor(dat$F1))
t1 <- chisq.test(as.factor(dat$L2), as.factor(dat$F1))
t1
t1$observed   # observed counts (same as M)
t1$expected   # expected counts under the null
t1$residuals  # Pearson residuals
t1$stdres     # standardized residuals
sel <- dat$L2 < 5 & dat$F1 < 5
t1 <- chisq.test(dat$L2[sel], dat$F1[sel])
t1
t1$observed   # observed counts (same as M)
t1$expected   # expected counts under the null
t1$residuals  # Pearson residuals
t1$stdres     # standardized residuals
# C:\JVA\GLFC\People\Lawrence\chisquare.r
library(XLConnect)
wb <- loadWorkbook("C:/JVA/GLFC/People/Lawrence/Chi-square question.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=2)
attach(dat)
# chi square test for independence, L2 vs. F1 using all levels
t1 <- chisq.test(L2, F1)
t1
t1$observed
t1$expected
# chi square test for independence, L2 vs. F1 discarding "unsure" observations
sel <- L2 < 5 & F1 < 5
t2 <- chisq.test(L2[sel], F1[sel])
t2
t2$observed
t2$expected
detach(dat)
?dist
dist
?dist
?outer
setwd("C:/JVA/GitHub/jvamisc")
document()
??document
setwd("C:/JVA/GitHub/jvamisc")
library(devtools)
document()
q()
