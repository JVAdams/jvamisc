tab <- res
tabl("Interval gaps in Sv and TS files don't match up.")
} else {
para("Interval gaps in Sv and TS files match up.")
}
rm(t1, t2, iu, ju, bigt1, bigt2, results, res)
}
if(casefold(substring(explore.tr, 1, 1))=="y") {# explore trawl files
### OPTROP
head2("OP and TROP FILES")
optrop2 <- missings(optrop)
para(paste0("The OP/TROP files have ", dim(optrop2)[1], " rows and ", dim(optrop2)[2], " columns."))
tab <- qksmry(optrop2)
tabl("Quick summary table of variables in OP/TROP files.")
attach(optrop2)
pcols <- c("Op.Id", "Vessel", "Cruise", "Serial", "Lake", "Port", 
"Beg.Depth", "End.Depth", "Distance", "Fishing_Temp", "Fishing_Depth", "Transect")
tab <- optrop2[is.na(Beg.Depth) | is.na(End.Depth) | is.na(Distance) | is.na(Fishing_Temp), pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with missing depth, distance, or temperature.", newpage="land")
} else {
para("All OP/TROP records have depth, distance, and temperature entered.")
}
set.time <- floor(Set_Time/100) + (Set_Time - 100*floor(Set_Time/100))/60
tod <- rep("night", length(set.time))
tod[set.time > 7 & set.time < 19] <- "day"
tt <- table(tod)
mostall <- names(which.max(table(tod)))
if(length(tt)<1.5) {
if(mostall=="night") {
para("All OP/TROP records were taken at night.")
} else {
para("All OP/TROP records were taken during the day.")
}
} else {
if(mostall=="night") {
tab <- optrop2[tod=="day", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken at night, but some were taken during the day.")
} else {
tab <- optrop2[tod=="night", c(pcols, "Set_Time")]
tabl("Most OP/TROP records were taken during the day, but some were taken at night.")
}
}
tab <- optrop2[!is.na(Beg.Depth) & !is.na(End.Depth) & abs(Beg.Depth - End.Depth) > 20, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with > 20 m difference between beginning and ending bottom depth.")
} else {
para("All OP/TROP records have < 20 m difference between beginning and ending bottom depth.")
}
mind <- pmin(Beg.Depth, End.Depth, na.rm=T)
tab <- optrop2[!is.na(mind) & !is.na(Fishing_Depth) & Fishing_Depth > mind, pcols]
if(dim(tab)[1] > 0) {
tabl("OP/TROP records with fishing depth > beginning or ending bottom depth.")
} else {
para("All OP/TROP records have fishing depths < beginning and ending bottom depths.")
}
fig <- function() {
par(mfrow=c(6, 4), mar=c(3, 3, 2, 1))
plot.df(optrop2)
}
figu("Plot of variables in the OP/TROP files.", newpage="port")
# lat/long plots
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Port))), "Colors indicate Port", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Port)
}
figu("Identification of ports in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Cruise))), "Colors indicate Cruise", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Cruise)
}
figu("Identification of cruises in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Transect))), "Colors indicate Transect", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Transect)
}
figu("Identification of transects in OP/TROP files.", newpage="port")
maxd <- pmax(Beg.Depth, End.Depth, na.rm=T)
fig <- function() {
plotmap(Latitude, Longitude, rain.n(-maxd), "Colors indicate Bottom Depth", pch=16, xla=0.15, yla=0.1)
}
figu("Bottom depths in OP/TROP files.", newpage="port")
fig <- function() {
plotmap(Latitude, Longitude, rain.n(Tow_Time), "Colors indicate Tow_Time", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tow_Time)
}
figu("Identification of tow times in OP/TROP files.", newpage="port")
if("Tr_Design" %in% names(optrop2)) {
fig <- function() {
plotmap(Latitude, Longitude, rain.n(as.numeric(as.factor(Tr_Design))), "Colors indicate Tr_Design", pch=16, xla=0.15, yla=0.1)
textg(Latitude, Longitude, Tr_Design)
}
figu("Identification of trawl designs in OP/TROP files.", newpage="port")
}
detach(optrop2)
rm(set.time, tod, tt, mostall, mind, maxd)
### trcatch
addPageBreak(doc, width=11, height=8.5)
head2("TRCATCH FILE")
trcatch2 <- missings(trcatch)
para(paste0("The TRCATCH file has ", dim(trcatch2)[1], " rows and ", dim(trcatch2)[2], " columns."))
tab <- qksmry(trcatch2)
tabl("Quick summary table of variables in TRCATCH file.")
attach(trcatch2)
sus <- sort(unique(Species))
if("Beg.Depth" %in% names(trcatch2)) {
tab <- trcatch2[is.na(Beg.Depth) | is.na(End.Depth), 
c("Op.Id", "Year", "Vessel", "Serial", "Lake", "Species", "Port_Name", "Beg.Depth", "End.Depth", "N")]
if(dim(tab)[1] > 0) {
tabl("TRCATCH records with missing beginning or ending depth.")
} else {
para("All TRCATCH records have beginning and ending depth entered.")
}
}
fig <- function() {
par(mfrow=c(5, 4), mar=c(3, 3, 2, 1))
plot.df(trcatch2)
plotsp(N, "N")
plotsp(Weight, "Weight")
plotsp(Weight/N, "Weight/N")
}
figu("Plot of variables in the TRCATCH file.", newpage="port")
detach(trcatch2)
rm(sus)
### trlf
head2("TRLF FILE")
trlf2 <- missings(trlf)
para(paste0("The TRCATCH file has ", dim(trlf2)[1], " rows and ", dim(trlf2)[2], " columns."))
tab <- qksmry(trlf2)
tabl("Quick summary table of variables in TRLF file.")
attach(trlf)
fig <- function() {
par(mfrow=c(5, 3), mar=c(3, 3, 2, 1))
plot.df(trlf)
}
figu("Plot of variables in the TRLF file.", newpage="port")
sus <- sort(unique(Species))
fig <- function() {
par(mfrow=n2mfrow(length(sus)), mar=c(3, 3, 2, 1), oma=c(2, 2, 1, 1), yaxs="i", cex=1)
for(i in seq(along=sus)) {
sel <- Species==sus[i]
a <- hist(rep(Length[sel], N[sel]), breaks=seq(-5, max(Length)+5, 5), plot=FALSE)
xr <- range(Length[sel])
hist(rep(Length[sel], N[sel]), xlim=xr+10*c(-1, 1), ylim=c(0, max(a$counts))*1.05, 
breaks=seq(-5, max(Length)+5, 5), col="blue", main=sus[i], las=1)
abline(v=xr, lwd=2, col="red")
box()
}
mtext("Length  (mm)", side=1, outer=TRUE, cex=1.5)
mtext("Frequency", side=2, outer=TRUE, cex=1.5)
}
figu("Length frequency histograms of species in the TRLF file.  Vertical red lines indicate the minimum and maximum lengths recorded.", 
newpage="port")
detach(trlf)
rm(trlf2, sus) #, i, sel, sul, a, xr)
### key106
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
head2("Alewife Age-Length Key")
m <- key106[-25, -(1:2)]
dimnames(m)[[1]] <- key106$mmgroup[-25]
dimnames(m)[[2]] <- substring(dimnames(m)[[2]], 4, 4)
m <- as.matrix(m)
m2 <- m[apply(m, 1, sum) > 0, apply(m, 2, sum) > 0]
para(paste0("The alewife age-length key has ", dim(m2)[1], " length categories and ", dim(m2)[2], " age categories."))
tab <- m2
tabl("Alewife age-length key.")
fig <- function() {
par(mar=c(4, 4, 2, 1), cex=1.5)
plot.matrix(m2, inc=0.2, xlab="Length  (mm)", ylab="Age", main="Alewife Age-Length Key")
}
figu("Alewife age-length key.  Circle size is proportional to probability of age, given length.",
"  Probabilities for all ages of a given length sum to one.", newpage="port")
rm(m, m2)
}
}
end.rtf()
{# save R data to file
rdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
alwayskeep <- c("LAKE", "YEAR", "maindir", "rdatname", "get.packages", "lakenames", "rain.n", "scale.02n", "myrecode", "explore", 
"start.rtf", "head1", "head2", "head3", "para", "tabl", "figu", "end.rtf", "explore.ac", "explore.ac", "use.alewife.ages",
"svdir", "tsdir")
keep1 <- c("trcatch", "optrop", "trlf")
keep1.b <- "key106"
keep10 <- c("sv2", "ts2")
if(explore==11) {
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
save(list=c(alwayskeep, keep10, keep1, keep1.b), file=paste0(maindir, rdatname))
} else {
save(list=c(alwayskeep, keep10, keep1), file=paste0(maindir, rdatname))
}
} else {
if(explore==1) {
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
save(list=c(alwayskeep, keep1, keep1.b), file=paste0(maindir, rdatname))
} else {
save(list=c(alwayskeep, keep1), file=paste0(maindir, rdatname))
}
} else {
if(explore==10) {
save(list=c(alwayskeep, keep10), file=paste0(maindir, rdatname))
}
}
}
rm(fig, tab)
}
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
para(if(use.alewife.ages) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(casefold(substring(use.alewife.ages, 1, 1))=="y" & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 15–21.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(casefold(substring(use.alewife.ages, 1, 1))=="y") {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & use.alewife.ages) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lakewide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lakewide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- as.matrix(format(round(laketots.n), big.mark=","))
tabl("Lakewide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tabl("Lakewide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
aleage <- casefold(substring(use.alewife.ages, 1, 1))=="y"
para(if(aleage) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(aleage & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 15–21.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(aleage) {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & use.alewife.ages) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lakewide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lakewide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- as.matrix(format(round(laketots.n), big.mark=","))
tabl("Lakewide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tabl("Lakewide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
aleage <- casefold(substring(use.alewife.ages, 1, 1))=="y"
para(if(aleage) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(aleage & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 15–21.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(aleage) {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & aleage) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lakewide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lakewide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- as.matrix(format(round(laketots.n), big.mark=","))
tabl("Lakewide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tabl("Lakewide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
tabl
conflicts()
jvamisc::tabl
?rtf
tab <- as.matrix(format(round(laketots.g), big.mark=","))
tab
tab <- round(laketots.g)
tab
# C:\JVA\Consult\Warner\Nearest Trawl\Troubleshoot 2015-01-22\Estimate works 20 Jan 2015 JVAmod.R
#########################################################################################################
###  INPUTS
#########################################################################################################
# specify lake (2=Michigan, 3=Huron) and year of interest
LAKE <- 3
YEAR <- 2014
# directory where input (*.Rdata) is stored and output will be placed
# outputs include an Excel workbook with estimates and
# a summary of the estimation in a Word document
# maindir <- "F:/DATA/Acoustics/Huron/Acoustic/2014/Explore/"
maindir <- "C:/JVA/Consult/Warner/Nearest Trawl/Troubleshoot 2015-01-22/"
# set the TS range of interest, minimum and maximum in dB
# Lake Michigan is typically c(-60, -30)
# Lake Huron is typically c(-64, -30)
ts.range <- c(-64, -30)
# set the transducer-specific two-way equivalent beam angle in steradians
psi <- 0.007997566
# specify species of interest by lake
# input lake number, followed by vector of species codes, separated by commas
soi <- list(list(lake=2, spsel=c(106, 109, 204)),
list(lake=3, spsel=c(106, 109, 129, 130, 202, 203, 204, 504)))
# regions used in laying out sampling design and corresponding areas (in km2)
# lake code, region name, region area
design <- scan(what=list(1, "", 1))
# length cut offs (lcut) and length-weight relations, Wg = lwa * Lmm ^ lwb;
# use lcut=NA for species with NO length cut off
# species code, species name, lcut, lwa, and lwb separated by commas
lwr <- scan(what=list(1, "", 1, 1, 1), sep=",")
#########################################################################################################
{### FUNCTIONS
define.slice <- function(lake, fdp, lat, bdp) {
# 2012-12-19 widened metalimnion from 38.5-60.7 to 28.5-60.7
if(length(lake)!=1) stop("Input lake as a vector of length one.")
if(!(lake %in% 2:3)) stop("Apportionment slices are only defined for Lakes Michigan (2) and Huron (3).")
if(lake==2) {
# Lake Michigan slices
fdplabs=c("epi", "meta", "hypo")
fdpcuts=c(0, 28.5, 70.7, 1000)
latlabs <- c("s", "m", "n")
latcuts <- c(0, 44.06, 44.93, 90)
bdplabs=c("near", "off")
bdpcuts=c(0, 60.7, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(lati, shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("m.near.meta", "m.off.meta", "n.near.meta", "n.off.meta")] <- "N.Meta"
levels(slice)[levels(slice) %in% c("s.near.meta", "s.off.meta")] <- "S.Meta"
levels(slice)[levels(slice) %in% c("s.near.hypo", "s.off.hypo", "m.near.hypo", "m.off.hypo", "n.near.hypo", "n.off.hypo")] <- "Hypo"
levels(slice)[levels(slice) %in% c("n.near.epi")] <- "N.Near.Epi"
levels(slice)[levels(slice) %in% c("s.off.epi", "m.off.epi", "n.off.epi")] <- "Off.Epi"
levels(slice)[levels(slice) %in% c("s.near.epi", "m.near.epi")] <- "S.Near.Epi"
} else {
# Lake Huron slices
fdplabs=c("Epi", "Hypo")
fdpcuts=c(0, 40, 1000)
latlabs <- c("a")
latcuts <- c(0, 90)
bdplabs=c("Near", "Off")
bdpcuts=c(0, 61.5, 1000)
# define slice
lati <- cut(lat, latcuts, labels=latlabs, right=FALSE)
shor <- cut(bdp, bdpcuts, labels=bdplabs, right=FALSE)
limn <- cut(fdp, fdpcuts, labels=fdplabs, right=FALSE)
slice <- interaction(shor, limn, lex.order=TRUE)
# metalimnetic and hypolimnetic slice are the same for both lakes
levels(slice)[levels(slice) %in% c("Near.Hypo", "Off.Hypo")] <- "Hypo"
}
data.frame(lati, shor, limn, slice)
}
plotbygrp <- function(xph.int) {
# come up with break points that divide the nonzero data into 7 groups on a log scale
v <- unlist(xph.int[, match(sp.grps, names(xph.int))])
v2 <- v[v>0]
mybrks <- 10^quantile(log10(v2), seq(0, 1, length=8))
symsize <- seq(0.5, 2.5, length=7)
npanels <- length(grp.sp) + length(unique(paste(grp.sp, grp.type))) - 1
if(LAKE==2) {
nrows <- 3
ncols <- ceiling(npanels/3)
} else {
nrows <- 4
ncols <- ceiling(npanels/4)
}
par(mfrow=c(nrows, ncols), mar=c(0, 0, 3, 0))
for(i in seq(sp.grps)) {
if(i>1) if(grp.sp[i]!=grp.sp[i-1] | grp.type[i]!=grp.type[i-1]) frame()
selcol <- match(sp.grps[i], names(xph.int))
selrow <- xph.int[, selcol] > 0
quant9 <- as.numeric(cut(xph.int[selrow, selcol], breaks=mybrks, include.lowest=TRUE))
map("usa", xlim=range(xph.int$Lon_M) + 0.1*c(-1, 1), ylim=range(xph.int$Lat_M) + 0.1*c(-1, 1), mar=c(0, 0, 3, 0), col="gray")
mtext(sp.grps[i], side=3)
points(xph.int$Lon_M[selrow], xph.int$Lat_M[selrow], cex=symsize[quant9], col=mypalette[quant9])
}
}
myscale <- function(x, newr=0:1) {
        # rescale a vector to take on values in a new defined range
        xr <- range(x, na.rm=T)
        xp <- (x-xr[1])/diff(xr)
        xp*diff(newr) + newr[1]
        }
}
### CRUNCHING
{# 1.  Initial stuff
options(stringsAsFactors=F, survey.lonely.psu="remove")
newrdatname <- paste0("L", LAKE, " Y", YEAR, " ACMT Data.RData")
newlake <- LAKE
newyear <- YEAR
newdir <- maindir
rm(LAKE, YEAR, maindir)
# bring in the data that was saved during the data exploration run (ACMT Explore.r)
# includes objects: sv2, ts2, trcatch, optrop, trlf, alekey
thisdata <- paste0(newdir, newrdatname)
load(thisdata)
ts <- ts2
sv <- sv2
maindir <- newdir
rm(ts2, sv2, newdir, newrdatname, newlake, newyear)
get.packages(c("class", "rgdal", "RColorBrewer", "survey", "maps", "mapdata","lubridate", "rtf"))
# make sure lake, year, and directory match up with lake, year, directory run through ACMT Explore.r
if(explore!=11) error("Must first run ACMT Explore program on both acoustic and trawl data.")
# make sure selected lake and year is represented in data provided
if(!(LAKE %in% optrop$Lake)) warning(paste0("\nNo information from ", lakenames[LAKE], " in RVCAT data in ", thisdata, ".\n\n"))
if(!(YEAR %in% optrop$Year)) warning(paste0("\nNo information from ", YEAR, " in RVCAT data in ", thisdata, ".\n\n"))
rm(thisdata, explore)
# create rtf document to save printed output (tables and figures)
docname <- paste0("L", LAKE, " Y", YEAR, " ACMT Estimate ", today(), ".doc")
doc <- start.rtf(file=docname, dir=maindir)
head1(paste0(YEAR, " Lake ", lakenames[LAKE], " Estimation from Acoustic and Trawl Data   ", today()))
para("R code written by Jean Adams for Dave Warner.")
para(paste0(docname, " = this document."))
head2("INPUTS")
para(paste0("maindir = ", maindir, " = main input/output directory."))
para(paste0("ts.range = ", ts.range[1], " to ", ts.range[2], " = TS range of interest."))
para(paste0("psi = ", psi, " = the transducer-specific two-way equivalent beam angle in steradians."))
aleage <- casefold(substring(use.alewife.ages, 1, 1))=="y"
para(if(aleage) "Alewife ages WILL be used." else "Alewife ages will NOT be used.")
# get lake- and species-specific information
SPSEL <- soi[[match(LAKE, sapply(soi, "[[", "lake"))]]$spsel
design <- data.frame(design)
names(design) <- c("lake", "reg", "reg.area.km2")
design$reg.area.ha <- 100*design$reg.area.km2
REG <- design$reg[design$lake==LAKE]
REG.AREA.HA <- design$reg.area.ha[design$lake==LAKE]
lwr <- data.frame(lwr)
names(lwr) <- c("sp", "spname", "lcut", "lwa", "lwb")
lwr$lcut[is.na(lwr$lcut)] <- 0
rm(soi, design)
# make sure we have age-length keys for the species that need it
if(aleage & !("key106" %in% ls())) warning("\nNo age length key available for alewife.\n\n")
}
{# 2.  Estimate sigma for each cell using TS frequency dist file
# Sigma is estimated as the mean of the linearized TS (sigma) weighted by the number of targets in each dB bin
tsbin.colz <- grep("X[[:punct:]]", names(ts))
db <- -as.numeric(substring(names(ts)[tsbin.colz], 3, 20))
lin.TS <- 10^(db/10)
in.range <- db >= ts.range[1] & db <= ts.range[2]
ts$sigma <- apply(ts[, tsbin.colz[in.range]], 1, function(w) weighted.mean(lin.TS[in.range], w))
rm(tsbin.colz, db, lin.TS, in.range)
}
{# 3.  Merge Sv and sigma data
# use region.interval.layer as unique identifier
sv$UID <- interaction(gsub(" ", "", sv$Region_name), sv$Interval, sv$Layer)
dim(sv)[1]
length(unique(sv$UID))
sv$source.sv <- sv$source
ts$UID <- interaction(gsub(" ", "", ts$Region_name), ts$Interval, ts$Layer)
dim(ts)[1]
length(unique(ts$UID))
ts$source.ts <- ts$source
# merge sv and ts files
svts <- merge(sv[, c("UID", "Region_name", "Interval", "Layer", "Layer_depth_min", "Layer_depth_max", "Lat_M", "Lon_M", "year", 
"date.m", "Sv_min", "Sv_max", "Sv_mean", "Depth_mean", "PRC_ABC", "source.sv")],
ts[, c("UID", "source.ts", "sigma")],
by="UID", all=TRUE)
# get rid of blanks in Region_name
svts$Region_name <- gsub(" ", "", svts$Region_name)
# if there are more rows in the merged data frame than in the original sv file, somethings wrong
if(dim(svts)[1] > dim(sv)[1]) {
sel <- is.na(svts$Interval)
tab <- ts[ts$UID %in% svts$UID[sel], c("Region_name", "Interval", "Layer", "source.ts")]
tabl("There is at least one region-interval-layer combination in the TS data that is missing from the SV data.",
"  These data will be removed from further calculations.")
svts <- svts[!sel, ]
}
# before making changes to sigma, keep the original value for later reference
svts$sigma.orig <- svts$sigma
# assign the value of zero to sigmas where there were no single targets
# There will be cells without single targets, so not all rows of Sv can get sigma.  
# I assign these a fish density of zero, because I never have zero targets because of high-density inability ot detect targets.
svts$sigma[is.na(svts$sigma)] <- 0
}
{# 4.  Estimate Nv
# Sawada, K., Furusawa, M., and Williamson, N.J. 1993. 
# Conditions for the precise measurement of fish target strength in situ. 
# Fish. Sci. (Tokyo), 20: 15–21.
# nv = c*tau*psi*R^2*n1/2
# where
# c = sound speed in m/s
# tau = pulse length in seconds
# psi = two-way equivalent beam angle in steradians.  
# This varies from transducer to transducer.  
# Should be able to input it as a constant (psi=) and in the formula below refer to psi or something.
# R = range to target 
# n1 = volumetric fish density
svts$n1 <- (10^(svts$Sv_mean/10))/svts$sigma
svts$nv <- (1450*0.0004*psi*(svts$Depth_mean^2)*svts$n1)/2
}
{# 5.  Replace "biased" sigmas where Nv>0.1 with mean "unbiased" sigma from cells in the same layer and (if possible) transect
# calculate mean of "unbiased" sigmas by year-transect-layer
svts.unbiased <- svts[svts$nv <= 0.1 & !is.na(svts$nv), ]
tranlay <- aggregate(sigma ~ year + Region_name + Layer, mean, data=svts.unbiased)
names(tranlay)[names(tranlay)=="sigma"] <- "sigunb.tranlay"
lay <- aggregate(sigma ~ year + Layer, mean, data=svts.unbiased)
names(lay)[names(lay)=="sigma"] <- "sigunb.lay"
svts2 <- merge(svts, tranlay, by=c("year", "Region_name", "Layer"), all=TRUE)
svts3 <- merge(svts2, lay, by=c("year", "Layer"), all=TRUE)
# if Nv > 0.1 (or Nv is missing), replace sigma with transect-layer mean of unbiased sigma
svts3$sigma[svts3$nv > 0.1 | is.na(svts3$nv)] <- svts3$sigunb.tranlay[svts3$nv > 0.1 | is.na(svts3$nv)]
# if Nv > 0.1 (or Nv is missing) and there is no transect-layer mean, replace sigma with layer mean of unbiased sigma
svts3$sigma[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)] <- 
svts3$sigunb.lay[(svts3$nv > 0.1 | is.na(svts3$nv)) & is.na(svts3$sigunb.tranlay)]
sel <- is.na(svts3$sigma)
if(sum(sel)>0) {
look3 <- svts3[sel, ]
tab <- table(look3$Region_name, look3$Layer)
tabl("Frequency of observations with missing sigmas by transect (row) and layer (column).",
"  These are layers that had no targets in any transect.",
"  They will be removed from further calculations.",
"  ")
svts3 <- svts3[!sel, ]
rm(look3)
}
rm(svts, svts.unbiased, tranlay, svts2, lay, sel)
}
{# 6.  Recalculate Nv and estimate density
svts3$n1 <- (10^(svts3$Sv_mean/10))/svts3$sigma
svts3$nv <- (1450*0.0004*psi*(svts3$Depth_mean^2)*svts3$n1)/2
svts3$fish_ha <- ((svts3$PRC_ABC / svts3$sigma) * 10000)
}
{# 7.  Add classifiers to acoustic data
# bottom depth range in each interval
depth.botmin <- aggregate(Layer_depth_min ~ Interval + Region_name, max, data=svts3)
names(depth.botmin)[names(depth.botmin)=="Layer_depth_min"] <- "depth.botmin"
depth.botmax <- aggregate(Layer_depth_max ~ Interval + Region_name, max, data=svts3)
names(depth.botmax)[names(depth.botmax)=="Layer_depth_max"] <- "depth.botmax"
depth.bot <- merge(depth.botmin, depth.botmax, all=TRUE)
svts4 <- merge(svts3, depth.bot, all=TRUE)
svts4$depth_botmid <- (svts4$depth.botmin + svts4$depth.botmax)/2
# define slice
svts5 <- data.frame(svts4, define.slice(lake=LAKE, fdp=svts4$Depth_mean, lat=svts4$Lat_M, bdp=svts4$depth_botmid))
rm(depth.botmin, depth.botmax, depth.bot, svts3, svts4)
}
{# 8.  Add classifiers to trawl data so they match those in acoustic data
# vertical layer
optrop$layer <- cut(optrop$Fishing_Depth, seq(0, 240, 10), right=FALSE)
# bottom depth interval
optrop$depth.botmin <- 10*floor(pmin(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth.botmax <- 10*ceiling(pmax(optrop$Beg.Depth, optrop$End.Depth)/10)
optrop$depth_botmid <- (optrop$Beg.Depth + optrop$End.Depth)/2
# define slice
optrop <- data.frame(optrop, define.slice(lake=LAKE, fdp=optrop$Fishing_Depth, lat=optrop$Latitude, bdp=optrop$depth_botmid))
}
{# 9.  Calculate mean proportion and mean weight of catch for trawl data
# summarize trcatch by species and op.id
trcatch2 <- aggregate(cbind(N, Weight) ~ Op.Id + Species, sum, data=trcatch)
# estimate weight from length for each fish
indx <- match(trlf$Species, lwr$sp)
trlf$estfw <- lwr$lwa[indx] * trlf$Length ^ lwr$lwb[indx]
# eliminate jumbo alewife
trlf <- trlf[!(trlf$Species==106 & trlf$Length>400), ]
rm(indx)
# calculate proportion of catch and mean weight for each MT and each species-age-length group
# determine ages of measured fish first, if necessary
if(aleage) {
allspsel <- c("106", SPSEL)
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL)+1)
names(sum.n) <- allspsel
mean.w <- sum.n
add.sp <- 1
# tally up lengths by mmgroup
lf106 <- trlf[trlf$Species==106, ]
lf106$mmgroup <- 10*round((lf106$Length+5)/10)-5
# total count and mean weight
g106 <- aggregate(cbind(N, estfw) ~ Op.Id + mmgroup, sum, data=lf106)
gkey106 <- merge(g106, key106, all.x=TRUE)
# rename ages
agecolz <- grep("Age", names(gkey106))
names(gkey106)[agecolz] <- paste0("106.A", substring(names(gkey106)[agecolz], 4, 10))
# apply probabilities from key to both counts and weights
# total numbers and mean weight by age group
tot.n <- apply(gkey106$N * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)
m.w <- apply(gkey106$estfw * gkey106[, agecolz], 2, tapply, gkey106$Op.Id, sum)/tot.n
tidyup <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y)[[1]] <- uniq
y[is.na(y)] <- 0
y[, apply(y, 2, sum)>0]
}
sum.n[[1]] <- tidyup(tot.n, allops)
mean.w[[1]] <- tidyup(m.w, allops)
rm(allspsel, lf106, g106, gkey106, agecolz, tot.n, m.w, tidyup)
} else {
allspsel <- SPSEL
# create vector of ops for all selected species
allops <- aggregate(N ~ Op.Id, sum, data=trcatch2[trcatch2$Species %in% allspsel, ])$Op.Id
# create list for results of all selected species
sum.n <- vector("list", length(SPSEL))
names(sum.n) <- SPSEL
mean.w <- sum.n
add.sp <- 0
rm(allspsel)
}
# determine groupings of other fish
for(i in seq(SPSEL)) {
sp <- SPSEL[i]
lc <- lwr$lcut[lwr$sp==sp]
# tally up lengths by length group
lf <- trlf[trlf$Species==sp, ]
lf$mmgroup <- lc*(lf$Length > lc)
# total up numbers and weights by length group
tot.n <- tapply(lf$N, list(lf$Op.Id, lf$mmgroup), sum)
m.w <- tapply(lf$estfw, list(lf$Op.Id, lf$mmgroup), sum)/tot.n
tidyup2 <- function(x, uniq) {
y <- x[match(uniq, dimnames(x)[[1]]), , drop=FALSE]
dimnames(y) <- list(uniq, paste0(sp, ".L", dimnames(y)[[2]]))
y[is.na(y)] <- 0
y
}
sum.n[[add.sp+i]] <- tidyup2(tot.n, allops)
mean.w[[add.sp+i]] <- tidyup2(m.w, allops)
rm(sp, lc, lf, tot.n, m.w, i, tidyup2)
}
# Report the proportion of "other" by number and weight for each trawl ... in case it's too large
sumbyspec <- tapply(trcatch2$N, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$N, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the number in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- names(propother)[sel]
}
sumbyspec <- tapply(trcatch2$Weight, list(trcatch2$Op.Id, trcatch2$Species %in% SPSEL), sum)
propother <- sumbyspec[, 1]/apply(sumbyspec, 1, sum)
sel <- propother>0.1 & !is.na(propother)
if(sum(sel)>0) {
look <- trcatch2[trcatch2$Op.Id %in% names(propother)[sel], ]
look$Other <- ifelse(look$Species %in% SPSEL, "", "*")
tab <- look[order(look$Op.Id, look$Other=="", -look$Weight, look$Species), c("Op.Id", "Other", "Species", "N", "Weight")]
tabl("Species other than those selected (", paste(SPSEL, collapse=", "), 
") are ignored when calculating proportions, but other species make up > 10% of the weight in at least one trawl haul.",
"  The locations of these tows are highlighted in Figure 1.")
rm(look)
mtops <- if(exists("mtops")) c(mtops, names(propother)[sel]) else names(propother)[sel]
}
# bring together total counts and mean weights
counts <- do.call(cbind, sum.n)
mnwts <- do.call(cbind, mean.w)
# calculate proportions by number
# don't double count the alewife if they're in by both length and age
# define the group type for each column of counts and wts as "A" for age and "L" for length
sp.grps <- dimnames(counts)[[2]]
grp.sp <- sapply(strsplit(sp.grps, "\\."), "[", 1)
grp.type <- substring(sapply(strsplit(sp.grps, "\\."), "[", 2), 1, 1)
sum.counts <- if(106 %in% SPSEL & aleage) apply(counts[, grp.type=="L"], 1, sum) else apply(counts, 1, sum)
nprops <- sweep(counts, 1, sum.counts, "/")
nprops[is.na(nprops)] <- 0
rm(add.sp, sum.n, mean.w, counts, sum.counts, sumbyspec, propother)
}
{# 10. Find the nearest midwater trawl to each acoustic cell within slice
# subset only the MT data with selected species captured
opsub <- optrop[match(allops, optrop$Op.Id), ]
# convert from lat/long to UTM
# use zone 16 for Lakes Superior and Michigan, and zone 17 for Huron, Erie, Ontario
projj <- if(LAKE < 2.5) "+proj=utm +zone=16 ellps=WGS84" else "+proj=utm +zone=17 ellps=WGS84"
MTutm <- project(as.matrix(opsub[, c("Longitude", "Latitude")]), projj)
ACutm <- project(as.matrix(svts5[, c("Lon_M", "Lat_M")]), projj)
# unique slice in AC and MT data
sus <- sort(unique(svts5$slice))
sus2 <- sort(unique(opsub$slice))
# determine nearest trawl
svts5$nearmt <- NA
for(i in seq(sus)) {
# select records from the selected slice
# exclude any records with missing slice or missing lat/long info
selm <- opsub$slice==sus[i] & !is.na(opsub$slice) & !apply(is.na(MTutm), 1, any)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)& !apply(is.na(ACutm), 1, any)
# determine the nearest MT
if(sum(selm)) {
if(sum(selm) > 1) {
svts5$nearmt[sela] <- as.numeric(as.character(knn1(MTutm[selm, ], ACutm[sela, ], allops[selm])))
} else {
svts5$nearmt[sela] <- allops[selm]
}
}
}
{# plot of apportionment
figu <- function(..., FIG=fig, rtf=doc, figc=figcount, boldt=TRUE, w=NULL, h=NULL, rf=300, newpage=c("none", "port", "land")[1], omi=c(1, 1, 1, 1)) {
wf <- if(is.null(w)) 6.5
hf <- if(is.null(h)) 8
if(newpage=="none") addNewLine(this=rtf)
if(newpage=="port") addPageBreak(this=rtf, width=8.5, height=11, omi=omi)
if(newpage=="land") {
wf <- if(is.null(w)) 9
hf <- if(is.null(h)) 5.5
addPageBreak(this=rtf, width=11, height=8.5, omi=omi)
}
addPlot(this=rtf, plot.fun=FIG, width=wf, height=hf, res=rf)
addNewLine(this=rtf)
addNewLine(this=rtf)
startParagraph(this=rtf)
addText(this=rtf, paste0("Figure ", figc, ".  "), bold=boldt)
addText(this=rtf, ...)
endParagraph(this=rtf)
addNewLine(this=rtf)
addNewLine(this=rtf)
figcount <<- figc + 1
}
# assign colors so that like colors are geographically separated
loc <- cmdscale(dist(opsub[, c("Latitude", "Longitude")]), k=1)
separate <- rep(1:3, length.out=length(loc))
colz1 <- rain.n(1:(dim(opsub)[1]), n=dim(opsub)[1], start=2/6, end=6/6)[order(loc)[order(separate)]]
colz2 <- myrecode(svts5$nearmt, opsub$Op.Id, colz1)
if(LAKE==2) {
mf <- c(2, 3)
iord <- c(6, 1, 4, 5, 2, 3)
} else {
mf <- c(2, 2)
iord <- c(1, 3, 2)
}
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
lowhigh <- if(is.null(mtops)) 1 else ((opsub$Op.Id[selm] %in% mtops) + 1)
par(xpd=NA)
text(opsub$Longitude[selm], opsub$Latitude[selm], opsub$Op.Id[selm], col=colz1[selm], cex=lowhigh, font=lowhigh)
par(xpd=FALSE)
mtext(sus[i], side=3)
} else {
mtext(paste(sus[i], "- No Tows"), side=3, col="brown")
}
}
}
figu("Location of midwater trawl hauls in 'new slices'.",
"  Numbers identify the OP_ID of each tow.  Colors are the same as in the next figure.",
"  Tows with > 10% of their catch (by number or weight) in 'other' species are shown in large, bold font.", hf=8, newpage="port")
fig <- function() {
par(mfrow=mf)
for(i in iord) {
selm <- opsub$slice==sus[i] & !is.na(opsub$slice)
sela <- svts5$slice==sus[i] & !is.na(svts5$slice)
map("usa", xlim=range(opsub$Longitude, svts5$Lon_M, na.rm=TRUE), ylim=range(opsub$Latitude, svts5$Lat_M, na.rm=TRUE), 
mar=c(0, 0, 2.5, 0), col="gray")
box(col="gray")
if(sum(selm)>0) {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col=colz2[sela], pch=3)
# add convex hull for each trawl haul
sut <- sort(unique(svts5$nearmt[sela]))
for(j in seq(along=sut)) {
selz <- sela & svts5$nearmt==sut[j]
hpts <- chull(svts5$Lon_M[selz], svts5$Lat_M[selz])
hpts <- c(hpts, hpts[1])
lines(svts5$Lon_M[selz][hpts], svts5$Lat_M[selz][hpts], lty=3)
}
mtext(sus[i], side=3)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col=colz1[selm], cex=2)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, cex=1.5)
points(opsub$Longitude[selm], opsub$Latitude[selm], pch=16, col="white", cex=1)
} else {
points(svts5$Lon_M[sela], svts5$Lat_M[sela], col="brown", pch=4)
mtext(paste(sus[i], "- No Trawls"), side=3, col="brown")
}
}
}
figu("Apportionment using 'new slices'.",
"  Each MT tow is shown as a white circle (o).",
"  Each AC interval is shown as a colored plus sign (+).",
"  Dotted lines encircle all the AC intervals (given the same color) that used each MT tow for apportionment.", hf=8, newpage="port")
}
{# plot of AC and MT data by slice
if(LAKE==2) {
mf <- c(3, 2)
orient <- "port"
} else {
mf <- c(1, 2)
orient <- "land"
}
sul <- rev(sort(unique(c(levels(svts5$lati), levels(opsub$lati)))))
fig <- function() {
par(mfrow=mf, mar=c(0, 0, 3, 3), oma=c(1.5, 2, 1.5, 2))
for(i in seq(sul)) {
# plot AC data
sel <- svts5$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(jitter(svts5$depth_botmid)[sel], -jitter(svts5$Depth_mean)[sel], col=svts5$slice[sel])
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
mtext(paste("Latitude", sul[i]), side=2, cex=1.2)
if(i==1) mtext("AC", side=3, line=2.5, cex=1.2)
# plot MT data
sel2 <- opsub$lati==sul[i]
plot(1, 1, xlim=range(svts5$depth_botmid, opsub$depth_botmid, na.rm=TRUE), ylim=range(-svts5$Depth_mean, -opsub$Fishing_Depth, na.rm=TRUE), type="n", xlab="", ylab="", axes=FALSE)
abline(0, -1, col="gray")
points(opsub$depth_botmid[sel2], -opsub$Fishing_Depth[sel2], col=opsub$slice[sel2], lwd=2, cex=2)
axis(3, col="gray", col.axis="gray", cex.axis=1.5)
axis(4, las=1, at=axTicks(4), labels=-axTicks(4), col="gray", col.axis="gray", cex.axis=1.5)
box(bty="7", col="gray")
if(i==1) mtext("MT", side=3, line=2.5, cex=1.2)
if(i==2) mtext("Bottom depth  (m)", side=3, line=2.5, col="darkgray", cex=1.2)
}
mtext("Water depth  (m)", side=4, outer=TRUE, line=0.5, col="darkgray", cex=1.2)
# levels in AC that are NOT in MT
misslev <- sus[!(sus %in% sus2)]
if(length(misslev)>0) {
mtext(paste("Slices not sampled by midwater trawls:", paste(misslev, collapse=", ")), side=1, outer=TRUE)
warning(paste("\nSlices not sampled by midwater trawls:", paste(misslev, collapse=", "), "\n\n"))
}
}
figu("Acoustic (left) and midwater trawl (right) data by 'new slices'.",
"  Color is used to uniquely identify each of the 'new slices'.", newpage=orient)
}
rm(projj, MTutm, ACutm, sus, sus2, selm, sela, sul, sel, i, iord, mf, loc, separate, colz1, colz2)
}
{# 11. Assign transects to regions (design strata) using transect names
svts5$region <- substring(svts5$Region_name, 1, 2)
svts5$regareaha <- REG.AREA.HA[match(svts5$region, REG)]
# make sure that design strata match up with sampled strata
sur <- sort(unique(svts5$region))
if(!identical(sort(REG), sur)) warning(paste0("\nStrata used in laying out the sampling design (", paste(sort(REG), collapse=", "), 
") do not match up with the strata actually sampled (", paste(sur, collapse=", "), ").\n\n"))
rm(sur)
rcol <- as.numeric(as.factor(svts5$region))
fig <- function() {
map("usa", xlim=range(svts5$Lon_M, na.rm=TRUE) + 0.1*c(-1, 1), ylim=range(svts5$Lat_M, na.rm=TRUE) + 0.1*c(-1, 1), mar=c(0, 0, 0, 0), col="gray")
points(svts5$Lon_M, svts5$Lat_M, col=rcol)
text(tapply(svts5$Lon_M, svts5$region, mean), tapply(svts5$Lat_M, svts5$region, mean), names(tapply(svts5$Lon_M, svts5$region, mean)), cex=2,
col=tapply(rcol, svts5$region, mean))
}
figu("Acoustic transect data, color coded by design-based strata.", newpage="port")
look <- tapply(svts5$Region_name, svts5$region, function(x) sort(unique(x)))
if(sum(sapply(tab, length) < 2)) {
tab <- cbind(names(look), sapply(look, paste, collapse=", "))
tabl("Only one transect in at least one region.  Variance will be estimated with this region(s) removed.")
}
rm(rcol, look)
}
{# 12. Generate estimates for the species groups.
# apply species group proportions to AC densities
nph <- svts5$fish_ha * nprops[match(svts5$nearmt, allops), ]
gph <- nph * mnwts[match(svts5$nearmt, allops), ]
# summary of density by interval (summed densities over layers)
nph.int <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(nph.int)[is.na(names(nph.int))] <- sp.grps
gph.int <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M, sum, data=svts5)
names(gph.int)[is.na(names(gph.int))] <- sp.grps
nph.int.domain <- aggregate(nph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(nph.int.domain)[is.na(names(nph.int.domain))] <- sp.grps
gph.int.domain <- aggregate(gph ~ region + regareaha + Region_name + Interval + shor + depth_botmid + lati + Lat_M + Lon_M + 
limn + slice, sum, data=svts5)
names(gph.int.domain)[is.na(names(gph.int.domain))] <- sp.grps
# a palette of 7 colors for non-zero data
mypalette <- brewer.pal(9, "GnBu")[-(1:2)]
fig <- function() plotbygrp(xph.int=nph.int)
figu("Acoustic density for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate higher density.", newpage="port")
fig <- function() plotbygrp(xph.int=gph.int)
figu("Acoustic biomass for each species group.  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Darker and larger circles indicate greater biomass.", newpage="port")
rm(nph, gph, mypalette)
}
{# 13. Calculate lake-wide totals based on stratified cluster sampling design
# stratified cluster design ... regions are strata, transects are clusters (nested in regions)
SCD.n <- svydesign(id=~Region_name, strata=~region, variables=nph.int[, grep("\\.", names(nph.int))], data=nph.int, nest=TRUE, 
weights=~regareaha)
SCD.n2 <- as.data.frame(svytotal(as.matrix(nph.int[, grep("\\.", names(nph.int))]/1000000), SCD.n))
SCD.n2ph <- as.data.frame(svymean(as.matrix(nph.int[, grep("\\.", names(nph.int))]), SCD.n))
SCD.g <- svydesign(id=~Region_name, strata=~region, variables=gph.int[, grep("\\.", names(gph.int))], data=gph.int, nest=TRUE, 
weights=~regareaha)
SCD.g2 <- as.data.frame(svytotal(as.matrix(gph.int[, grep("\\.", names(gph.int))]/1000000), SCD.g))
SCD.g2ph <- as.data.frame(svymean(as.matrix(gph.int[, grep("\\.", names(gph.int))]), SCD.g))
domainest <- function(dat, type="total") {
d <- NA
if(dim(dat)[1]>0) {
scd <- svydesign(id=~Region_name, variables=dat[, grep("\\.", names(dat))], data=dat, nest=TRUE, weights=~regareaha)
if(type=="total") d <- as.data.frame(svytotal(as.matrix(dat[, grep("\\.", names(dat))]/1000000), scd))[, type]
if(type=="mean") d <- as.data.frame(svymean(as.matrix(dat[, grep("\\.", names(dat))]), scd))[, type]
}
d
}
# summarize by the new "slices" (domains) ... ignore old "strata", and don't attempt to calculate variances
SCD.n.d <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="total")
SCD.n.d2 <- do.call(cbind, SCD.n.d)
SCD.n.dph <- lapply(split(nph.int.domain, nph.int.domain$slice), domainest, type="mean")
SCD.n.d2ph <- do.call(cbind, SCD.n.dph)
SCD.g.d <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="total")
SCD.g.d2 <- do.call(cbind, SCD.g.d)
SCD.g.dph <- lapply(split(gph.int.domain, gph.int.domain$slice), domainest, type="mean")
SCD.g.d2ph <- do.call(cbind, SCD.g.dph)
# combine information
laketots.n <- cbind(SCD.n.d2, SCD.n2, rse=100*SCD.n2$SE / SCD.n2$total)
lakemeans.n <- cbind(SCD.n.d2ph, SCD.n2ph, rse=100*SCD.n2ph$SE / SCD.n2ph$mean)
laketots.g <- cbind(SCD.g.d2, SCD.g2, rse=100*SCD.g2$SE / SCD.g2$total)
lakemeans.g <- cbind(SCD.g.d2ph, SCD.g2ph, rse=100*SCD.g2ph$SE / SCD.g2ph$mean)
# Save estimates to csv files
fourtypes <- c("millions", "nph", "t", "gph")
outfiles <- paste0(maindir, "L", LAKE, " Y", YEAR, " ACMT Estimates ", fourtypes, " ", today(), ".csv")
write.csv(laketots.n, outfiles[1])
write.csv(lakemeans.n, outfiles[2])
write.csv(laketots.g, outfiles[3])
write.csv(lakemeans.g, outfiles[4])
rm(nph.int, gph.int, nph.int.domain, gph.int.domain, SCD.n, SCD.n2, SCD.g, SCD.g2, SCD.n.d, SCD.n.d2, SCD.g.d, SCD.g.d2, 
SCD.n2ph, SCD.g2ph, SCD.n.dph, SCD.n.d2ph, SCD.g.dph, SCD.g.d2ph, fourtypes, outfiles)
mypalette <- brewer.pal(6, "Set3") 
fig <- function() {
par(mar=c(4, 5, 0, 1), oma=c(0, 0, 2, 0), mfrow=c(1, 2), cex=1.2)
barplot(t(as.matrix(laketots.n[, 1:(length(laketots.n)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Number of fish  (millions)")
barplot(t(as.matrix(laketots.g[, 1:(length(laketots.g)-3)])), col=mypalette, horiz=TRUE, las=1, xlab="Biomass of fish  (t)", 
legend.text=TRUE, args.legend=list(x="topright"))
}
figu("Acoustic survey lake-wide estimates in number (left) and biomass (right) for each species group.",
"  Groups are defined by length cut offs (L) in mm or ages (A).",
"  Colors are used to identify contributions from different 'new slices'.", hf=5.8, wf=9, newpage="land")
rm(mypalette)
# numbers in millions
tab <- round(laketots.n)
tabl("Lake-wide estimates in number (millions) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
# biomass in metric tons (t)
tab <- round(laketots.g)
tabl("Lake-wide biomass estimates (t) for each species group incorporating survey design information",
" (stratification of sampling effort in each region), then sliced into different lake communities.",
"  Species groups are defined by length cut offs (L) in mm or ages (A).")
}
end.rtf()
rm(tab, fig)
cleanup()
search()
# C:\JVA\Lamprey\ChemControl\Resistance\Analysis\AnalyzeRaw.r
library(lubridate)
library(LW1949)
library(lattice)
load("C:/JVA/Lamprey/ChemControl/Resistance/Analysis/RawData.RData")
all.main$Year <- year(all.main$Start_Date)
mi <- match(all.sub$ID, all.main$ID)
all.sub$Year <- all.main$Year[mi]
all.main$fg <- paste(all.main$Folder, all.main$group)
all.sub$fg <- all.main$fg[mi]
rm(mi)
mytable(all.main$Folder)
with(all.main, tapply(ID, Folder, range))
with(all.main, tapply(group, Folder, range))
cleanup()
q()
source("C:/JVA/Lamprey/ChemControl/Resistance/Slaght/ReadSlaght.r")
ls()
rm(leavethese, sel, xxx)
ls()
head(Slaghtdat)
head(Slaghtdat)
dat <- Slaghtdat
rm(Slaghdat)
rm(Slaghtdat)
plotdf(dat)
.SavedPlots <- NULL
.SavedPlots <- NULL
stringin("ph", names(dat))
stringin
grep("s", dat$species, ignore.case=TRUE)
?grep
grepl("s", dat$species, ignore.case=TRUE)
sel <- grepl("s", dat$species, ignore.case=TRUE) & grepl("h", dat$water, ignore.case=TRUE)
plotdf(dat[sel, ])
head(dat)
plotdf(dat[!sel, ])
stringin
sel <- grepl("s", dat$species, ignore.case=TRUE, fixed=TRUE) & grepl("h", dat$water, ignore.case=TRUE, fixed=TRUE)
unique(dat$species)
grepl("s", unique(dat$species), ignore.case=TRUE)
summary(sub)
sel <- grepl("s", dat$species, ignore.case=TRUE) & grepl("h", dat$water, ignore.case=TRUE)
sub <- dat[sel, ]
plotdf(sub)
plotdf(sub)
attach(sub)
mytable(test.type)
mytable(water)
mytable(location)
mytable(lab)
mytable(acc..no.)
mytable(temp.)
mytable(chemical)
mytable(species)
mytable(total.no..tested)
mytable(Folder)
mytable(File)
mytable(Sheet)
mytable(ill)
mytable(dead)
mytable(cumulative..ill)
mytable(cumulative..dead)
mytable(temp..unit)
mytable(conc..unit)
mytable(aerated.)
mytable(general.comment..regarding.the.whole.test.)
mytable(specific.comment..regarding.just.the.corresponding.line.of.data.)
library(lubridate)
library(LW1949)
plot(start.date)
mytable(year(start.date))
sample(c(1:3, rep(0, 7)))
sample(c(1:3, rep(0, 7)))
sample(c(1:3, rep(0, 7)))
?which.max
x <- c(1:4, 0:5, 11)
which.min(x)
which.max(x)
x
x <- c(1:4, 0:4)
which.max(x)
table(x)
sort(table(x))
rev(table(x))
rev(table(x))[1]
nsim <- 10
res <- rep(NA, nsim)
for(i in 1:nsim) {
j1 <- sample(c(1:3, rep(0, 7)))
j2 <- sample(c(1:3, rep(0, 7)))
j3 <- sample(c(1:3, rep(0, 7)))
j4 <- sample(c(1:3, rep(0, 7)))
j <- j1 + j2 + j3 + j4
res[i] <- rev(table(j))[1]
}
res
hist(res)
nsim <- 1000
res <- rep(NA, nsim)
for(i in 1:nsim) {
j1 <- sample(c(1:3, rep(0, 7)))
j2 <- sample(c(1:3, rep(0, 7)))
j3 <- sample(c(1:3, rep(0, 7)))
j4 <- sample(c(1:3, rep(0, 7)))
j <- j1 + j2 + j3 + j4
res[i] <- rev(table(j))[1]
}
mytable(res)
100*mytable(res)/nsim
nsim <- 10000
res <- rep(NA, nsim)
for(i in 1:nsim) {
j1 <- sample(c(1:3, rep(0, 7)))
j2 <- sample(c(1:3, rep(0, 7)))
j3 <- sample(c(1:3, rep(0, 7)))
j4 <- sample(c(1:3, rep(0, 7)))
j <- j1 + j2 + j3 + j4
res[i] <- rev(table(j))[1]
}
100*mytable(res)/nsim
cleanup()
q()
?cheat
# C:\JVA\Admin\CenterDirector\StaffMeetings\Kostich Communications Survey\survey.r
wb <- loadWorkbook("C:/JVA/Admin/CenterDirector/StaffMeetings/Kostich Communications Survey/Communication Satisfaction Factors GLSC JVAmod.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
head(dat)
fill(dat$Group)
dat$Group <- fill(dat$Group)
head(dat)
dat[, -(1:3)]
range(dat[, -(1:3)])
range(unlist(dat[, -(1:3)]))
range(unlist(dat[, -(1:3)]), na.rm=TRUE)
(unlist(dat[, -(1:3)]))
range(unlist(dat[, -(1:3)]), na.rm=TRUE)
lapply(dat, class)
wb <- loadWorkbook("C:/JVA/Admin/CenterDirector/StaffMeetings/Kostich Communications Survey/Communication Satisfaction Factors GLSC JVAmod.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat$Group <- fill(dat$Group)
lapply(dat, class)
dat$Group <- fill(dat$Group)
dat$Fed <- as.numeric(dat$Fed)
dat$Nonfed <- as.numeric(dat$Nonfed)
dat$Mgmt <- as.numeric(dat$Mgmt)
dat$Nonmgmt <- as.numeric(dat$Nonmgmt)
sapply(dat, class)
search()
attach(dat)
range(dat[, -(1:3)], na.rm=TRUE)
mytable(dat$Group)
mytable(substring(dat$Group, 1, 10))
mytable(substring(dat$Group, 1, 9))
mytable(substring(dat$Group, 1, 8))
mytable(substring(dat$Group, 1, 7))
mytable(substring(dat$Group, 1, 6))
mytable(substring(dat$Group, 1, 5))
mytable(substring(dat$Group, 1, 4))
mytable(substring(dat$Group, 1, 1))
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
windows()
par(pty="s", mar=c(4, 4, 1, 1))
plot(Fed, Nonfed, type="n", xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
abline(0, 1, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(h=4, lty=2)
abline(v=4, lty=2)
abline(0.6, 1, lty=2)
abline(-0.6, 1, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(0.6, 1, lty=2, lwd=3, col="lightgray")
abline(-0.6, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2)
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(0.6, 1, lty=2, lwd=3, col="lightgray")
abline(-0.6, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(Fed-Nonfed)>0.6)+1])
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
cut <- 0.56
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(0.6, 1, lty=2, lwd=3, col="lightgray")
abline(-0.6, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(Fed-Nonfed)>=cut)+1])
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
kut <- 0.56
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(Fed, Nonfed, type="n", las=1, xlim=xyr, ylim=xyr, xlab="Fed", ylab="Non-Fed")
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(Fed, Nonfed, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(Fed-Nonfed)>=kut)+1])
head(dat)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(dat$Group, 1, 1)
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit(AA, Field, "AA", "Field")
plotit(Mgmt, Nonmgmt, "Mgmt", "Nonmgmt")
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit(Field, AA, "Field", "AA")
plotit(Nonmgmt, Mgmt, "Nonmgmt", "Mgmt")
sort(unique(Group))
legend("topleft", sort(unique(Group)), kol=1:9)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)))
substring(dat$Group, 1, 3)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(Group, 1, 1)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.5)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.7, font=2)
?legend
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.7, ptlwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.7, pt.lwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.6, pt.lwd=2)
plotit(Nonmgmt, Mgmt, "Nonmgmt", "Mgmt")
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.6, pt.lwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2)
legend("topleft", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.59, pt.lwd=2)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(Group, 1, 1)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white", bty="n")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white", bty="n")
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="blue", bty="n")
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, box.col="white", bg="white")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char <- substring(Group, 1, 1)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
kut <- 0.56
plotit <- function(x, y, xl, yl) {
windows()
par(pty="s", mar=c(4, 4, 1, 1), cex=1.5)
plot(x, y, type="n", las=1, xlim=xyr, ylim=xyr, xlab=xl, ylab=yl)
abline(kut, 1, lty=2, lwd=3, col="lightgray")
abline(-kut, 1, lty=2, lwd=3, col="lightgray")
abline(h=4, lty=2)
abline(v=4, lty=2)
text(x, y, char, col=kol, font=2, cex=c(0.75, 1.5)[(abs(x-y)>=kut)+1])
legend("bottomright", sort(unique(Group)), col=1:9, pch=sort(unique(char)), cex=0.58, pt.lwd=2, bg="white")
}
plotit(Fed, Nonfed, "Fed", "Nonfed")
plotit(Field, AA, "Field", "AA")
plotit(Nonmgmt, Mgmt, "Nonmgmt", "Mgmt")
head(dat)
x <- dat[, -(1:3)]
apply(x, 1, mean, na.rm=TRUE)
apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
mnz <- apply(x, 1, mean, na.rm=TRUE)
rngz <- apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
plot(mnz, rngz)
plot(sort(mnz))
plot(sort(rngz))
locator()
plot(mnz, rngz)
abline(h=0.8, lty=2)
abline(v=4.5, lty=2)
plot(sort(mnz))
locator()
plot(sort(rngz))
locator()
plot(mnz, rngz)
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
x <- dat[, -(1:3)]
mnz <- apply(x, 1, mean, na.rm=TRUE)
rngz <- apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
windows()
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=)
axis(1, at=1:7, c("Very\ndissatisfied", "Dissatisfied", "Somewhat\ndissatisfied", "Indifferent",
"Somewhat\nsatisfied", "Satisfied", "Very\nsatisfied")
)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xaxt="n", xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(1, at=1:7, c("\nVery\ndissatisfied", "Dissatisfied", "Somewhat\ndissatisfied", "Indifferent",
"Somewhat\nsatisfied", "Satisfied", "Very\nsatisfied")
)
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xaxt="n", xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(1, at=1:7, c("\nVery\ndissatisfied", "Dissatisfied", "Somewhat\ndissatisfied", "Indifferent",
"\nSomewhat\nsatisfied", "Satisfied", "Very\nsatisfied"))
par(mar=c(4, 4, 1, 1), cex=1.5)
plot(mnz, rngz, type="n", las=1, xaxt="n", xlab="Mean Score", ylab="Score Range")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(1, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", las=1, xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(4, at=c(0.2, 0.8), c("Consistent", "Discrepant")
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.2, 0.8), c("Consistent", "Discrepant")
axis(4, las=1)
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.2, 0.8), c("Consistent", "Discrepant"))
axis(4, las=1)
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.4, 0.85), c("Consistent", "Discrepant"))
axis(4, las=1)
head(dat)
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, paste0(char, Question), col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.4, 0.85), c("Consistent", "Discrepant"))
axis(4, las=1)
search()
detach()
# C:\JVA\Admin\CenterDirector\StaffMeetings\Kostich Communications Survey\survey.r
wb <- loadWorkbook("C:/JVA/Admin/CenterDirector/StaffMeetings/Kostich Communications Survey/Communication Satisfaction Factors GLSC JVAmod.xlsx")
dat <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat$Group <- fill(dat$Group)
dat$Fed <- as.numeric(dat$Fed)
dat$Nonfed <- as.numeric(dat$Nonfed)
dat$Mgmt <- as.numeric(dat$Mgmt)
dat$Nonmgmt <- as.numeric(dat$Nonmgmt)
attach(dat)
xyr <- range(dat[, -(1:3)], na.rm=TRUE)
char[Group=="Relationship with Supervisor"] <- "S"
kol <- as.numeric(as.factor(char))
char2 <- paste0(char, Question)
x <- dat[, -(1:3)]
mnz <- apply(x, 1, mean, na.rm=TRUE)
rngz <- apply(x, 1, function(y) abs(diff(range(y, na.rm=TRUE))))
windows()
par(mar=c(4, 4, 3, 3), cex=1.5)
plot(mnz, rngz, type="n", yaxt="n", xlab="Average Score", ylab="Biggest Difference in Scores")
abline(h=0.85, lty=2)
abline(v=4.4, lty=2)
text(mnz, rngz, char2, col=kol, font=2)
axis(3, at=1:7, c("Very dissatisfied", "Dissatisfied", "Somewhat dissatisfied", "Indifferent",
"Somewhat satisfied", "Satisfied", "Very satisfied"))
axis(2, at=c(0.4, 0.85), c("Consistent", "Discrepant"))
axis(4, las=1)
dat[mnz < 4.4, ]
cbind(mnz, dat)[mnz < 4.4, ]
a <- cbind(mnz, dat)[mnz < 4.4, ]
a[order(a[, 1])
, ]
a$Question[order(a[, 1])]
a <- cbind(mnz, dat)[mnz < 4.4, ]
a$Question[order(a[, 1])]
a <- cbind(rngz, dat)[rngz > 0.85, ]
a$Question[order(-a[, 1])]
q()
cleanup()
cleanup()
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
library(devtools)
install_github("httr")
install_github("twitteR", username="geoffjentry")
library(twitteR)
utils:::menuInstallPkgs()
library(twitteR)
?setup_twitter_oauth
??setup_twitter_oauth
utils:::menuInstallPkgs()
library(httr)
?install
cleanup()
q()
library(httr)
utils:::menuInstallPkgs()
library(httr)
library(twitteR)
library(RCurl)
?setup_twitter_oauth
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
install.packages(c("devtools", "rjson", "bit64", "httr"))
q()
library(devtools)
utils:::menuInstallPkgs()
library(devtools)
install_github("twitteR", username="geoffjentry")
?install_github
install_github("geoffjentry/twitteR")
library(twitteR)
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
updateStatus
?updateStatus
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
library(jvamisc)
library(RCurl)
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
cat("\n\n")
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
cat("***  ")
cat(head)
cat("\n")
cat(thisurl)
cat("  ***")
cat("\n")
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
cat(photo.url)
cat("\n")
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
cat(paste(strwrap(cap, 60, indent=3), collapse="\n"))
cat("\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
m <- do.call(rbind, lapply(more.urls, pull))
m
m[4, ]
m[4, 2:1]
paste(m[4, 2:1], collapse=". ")
tweets <- apply(m[, 2:1], 1, paste, collapse=". ")
tweets
nchar(tweets)
updateStatus(tweets[4])
more.codes
rm(tweets)
adj <- getUser("AntigoDJ")
df <- twListToDF(tweets)
?getUser
adj <- getUser("AntigoDJ")
userTimeline(adj, n=20, excludeReplies=TRUE)
updateStatus(tweets[3])
updateStatus(totweet[3])
totweet <- apply(m[, 2:1], 1, paste, collapse=". ")
totweet
updateStatus(totweet[3])
userTimeline(adj, n=20, excludeReplies=TRUE)
unlist(userTimeline(adj, n=20, excludeReplies=TRUE))
a <- userTimeline(adj, n=20, excludeReplies=TRUE)
class(a)
unlist(a)
length(a)
a[[1]]
unlist(unlist(a))
as.vector(a)
sapply(a, "[", 1)
do.call(c, a)
do.call(rbind, a)
a <- userTimeline(adj, n=20, excludeReplies=TRUE)
a
twListToDF(a)
twListToDF(a)$text
totweet
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
totweet <- apply(m[, 2:1], 1, paste, collapse=". ")
# pull up old tweets
adj <- getUser("AntigoDJ")
tweetsdf <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
stringin(substring(totweet, 1, 10), tweetsdf$text)
substring(totweet, 1, 10)
tweetsdf$text
substring(totweet, 1, 30) %in% substring(tweetsdf$text, 1, 30)
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
# pull up old tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
# tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
totweet
?updateStatus
newtweets
m
m[3, 3]
updateStatus(newtweets[3], lat=45.141473, long=-89.152339, mediaPath=m[3, 3])
m[3, 3]
?url
oldtweets
nchar(oldtweets$text)
paste(newtweets[3], m[3, 3])
updateStatus(paste(newtweets[3], m[3, 3]), lat=45.141473, long=-89.152339)
totweet
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339))
rev(totweet)
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
oldtweets
oldtweets$created
max(oldtweets$created)
date(max(oldtweets$created))
format(max(oldtweets$created), "%M/%D/%Y")
?as.Date
date()
?date
Sys.Date
Sys.Date()
Sys.time()
format(max(oldtweets$created), tz="CST")
format(max(oldtweets$created), tz="CT")
?Sys.timezone
Sys.timezone()
format(max(oldtweets$created), tz=Sys.timezone())
max(oldtweets$created)
format(max(oldtweets$created), "%a", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b %e", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b %e %I", tz=Sys.timezone())
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone())
cleanup()
q()
library(twitteR)
library(RCurl)
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
?setup_twitter_oauth
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
cleanup()
library(twitteR)
library(RCurl)
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
### grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl, show=TRUE) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
if(show) cat(paste0("\n\n***  ", head, "\n", thisurl, "  ***\n"))
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
if(show) cat(paste0(photo.url, "\n"))
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
if(show) cat(paste0(paste(strwrap(cap, 60, indent=3), collapse="\n")), "\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
newtweets
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
totweet
if(length(totweet) > 0) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nNo new headlines posted to website since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
oldtweets
oldtweets
?updateStatus
updateStatus("this is a test", , lat=45.141473, long=-89.152339)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
oldtweets
q()
cleanup()
library(twitteR)
library(RCurl)
### connect to Twitter
api_key <- "PQWmazlRxSLrKSW0ysq6Qe8iJ"
api_secret <- "yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3"
access_token <- "2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF"
access_token_secret <- "flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
rm(api_key, api_secret, access_token, access_token_secret)
### grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl, show=TRUE) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
if(show) cat(paste0("\n\n***  ", head, "\n", thisurl, "  ***\n"))
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
if(show) cat(paste0(photo.url, "\n"))
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
if(show) cat(paste0(paste(strwrap(cap, 60, indent=3), collapse="\n")), "\n")
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nNo new headlines posted to website since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
if(FALSE) {
install_github("geoffjentry/twitteR")
#install_github("httr")
#library(httr)
}
source("C:/JVA/Exp/ADJTweet.r")
m
m[1, 1]
download.file
?download.file
m[1, 1]
download.file(m[1, 1], "c:/temp/photo1.jpg")
download.file(m[1, 1], "c:/temp/photo1.jpg", mode="wb")
download.file(m[1, 1], "c:/temp/photo1.jpg", mode="wb")
download.file(m[1, 1], "c:/temp/photo1.jpg", method="curl")
link = "http://29.media.tumblr.com/tumblr_m0q2g8mhGK1qk6uvyo1_500.png"
download.file(link,basename(link))
getwd()
?updateStatus
updateStatus("this is a test", mediaPath="c:/temp/photo1.jpg")
updateStatus("this is a test")
cleanup()
q()
library(twitteR)
credentials <- c(
  "twitter_api_key=PQWmazlRxSLrKSW0ysq6Qe8iJ",
  "twitter_api_secret=yOzDPTZJ2A03OnB67CFETb2QoTjf26i5VHjVe8AAZKMUVEwbH3",
  "twitter_access_token=2996369957-NKicE61ZMVTpS3eJxC0QslkjZ9shbxjihy7xWNF",
  "twitter_access_token_secret=flWIGwSbFy8BMK2WtGRuyljFd6fzJTeW2Fph4Ds08F9rJ"
  )
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(credentials, fname)
browseURL(fname)
normalizePath("~/")
getwd
getwd()
?normalizePath
fname <- paste0(getwd(), ".Renviron")
writeLines(credentials, fname)
browseURL(fname)
file.path(getwd(), ".Rprofile") 
file.path(getwd(), ".Renviron") 
fname <- file.path(getwd(), ".Renviron")
writeLines(credentials, fname)
fname
browseURL(fname)
q()
api_key <- Sys.getenv("twitter_api_key")
api_key
?Sys.getenv
Sys.getenv("twitter_api_key")
q()
cleanup()
Sys.getenv("twitter_api_key")
cleanup()
q()
?source
source("https://raw.githubusercontent.com/JVAdams/jvamisc/master/R/calcr2.r")
source.url("https://raw.githubusercontent.com/JVAdams/jvamisc/master/R/calcr2.r")
??source.url
library(devtools)
source_url("https://raw.githubusercontent.com/JVAdams/jvamisc/master/R/calcr2.r")
ls()
calcr2
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, newtweets=newtweets, totweet=totweet)
}
tweetadj(FALSE)
library(twitteR)
library(Rcurl)
library(RCurl)
tweetadj(FALSE)
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
newtweets <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- newtweets[!(substring(newtweets, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, newtweets=newtweets, totweet=totweet)
}
tweetadj(FALSE)
tweetadj(FALSE)
tweetadj()
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=20, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
return(list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet))
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
}
tweetadj(FALSE)
tweetadj()
a <- tweetadj()
a
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\nThis is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\nNo new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweetadj()
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweetadj(FALSE)
#' Tweet ADJ
#'
#' Tweet the latest headlines from ADJ.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, will use credentials stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsTo store credentials in local .Renviron file, use \code{writeLines(c("twitter_api_key=xxx", "twitter_api_secret=xxx", 
#'"twitter_access_token=xxx", "twitter_access_token_secret=xxx"), file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweetadj(FALSE)
# headlines without "More"s
# github
# photos
tweetadj <- function(tweet=TRUE, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- "http://www.antigodailyjournal.com/"
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser("AntigoDJ")
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), "\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweetadj()
cleanup()
q()
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- website
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(username)
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
library(RCurl)
library(twitteR)
tweethead()
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- website
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
website
base.url
username
credentials
base.url <- Sys.getenv("website")
base.url
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste("\n\n***  No new headlines since last tweet,", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
a <- tweethead()
b <- a$oldtweets
b
b[, c(1, 3, 5, 12)]
b[, c(1, 3, 12, 5)]
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead
tweethead()
b
b$created
format(b$created, tz=)
format(b$created, tz=Sys.timezone())
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
oldtweets$created <- format(oldtweets$created, tz=Sys.timezone())
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$created), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
b
b$created <- format(b$created, tz=Sys.timezone())
b
class(b$created)
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)==created] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$createdUTC), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
more.urls <- paste0(base.url, "index.php?ID=", more.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
rm(base.url, base.html, links, links2, more.codes, more.urls)
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)=="created"] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$createdUTC), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead()
#' Tweet Headlines
#'
#' Tweet the latest headlines from the specified website.
#' @param tweetA logical scalar indicating if tweets should be posted, default TRUE.
#' @param usernameA character scalar, giving the name of the twitter user.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param websiteA character scalar, giving the name of the website, from which to pull headlines.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @param credentialsA character vector of length four, giving the twitter_api_key, the twitter_api_secret, 
#'the twitter_access_token, and the twitter_access_token_secret.
#' The default, NULL, uses information stored in local .Renviron file (see Details).
#' @return A named vector with the \code{Mean}, lower and upper confidence limits (\code{L} and \code{U}), 
#' and the number of observations \code{N}.
#' @detailsThis function is customized to work on a particular website.  It's not for general use. 
#'To store information in local .Renviron file, use \code{writeLines(c("username=xxx", "website=xxx", 
#'"twitter_api_key=xxx", "twitter_api_secret=xxx", "twitter_access_token=xxx", "twitter_access_token_secret=xxx"), 
#'file.path(getwd(), ".Renviron"))}.
#' @importtwitteR, RCurl
#' @export
#' @references
#'
#' Simon Munzert.  19 Jan 2015.
#' Programming a Twitter bot – and the rescue from procrastination.
#' \emph{http://www.r-datacollection.com/blog/Programming-a-Twitter-bot/}
#'
#' Simon Munzert.  21 Dec 2014.
#' How to conduct a tombola with R.
#' \emph{www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/}
#' @examples 
#' tweethead(FALSE)
# headlines without "More"s
# github
# photos
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
tweethead <- function(tweet=TRUE, username=NULL, website=NULL, credentials=NULL) {
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
long.codes <- (min(more.codes)-2):(max(more.codes)+2)
more.urls <- paste0(base.url, "index.php?ID=", long.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)=="created"] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
if(length(totweet) > 0) {
if(tweet) {
lapply(rev(totweet), updateStatus, lat=45.141473, long=-89.152339)
} else {
cat(paste("\n\n***  This is what would be posted if tweet=TRUE.\n\n"))
print(totweet)
cat("\n\n")
}
} else {
cat(paste0("\n\n***  No new headlines since last tweet, ", 
format(max(oldtweets$createdUTC), "%a %b %e %I:%M %p", tz=Sys.timezone()), ".\n\n\n"))
}
list(oldtweets=oldtweets, currentheads=currentheads, totweet=totweet)
}
tweethead(FALSE)
more.codes
tweet=TRUE
username=NULL
website=NULL
credentials=NULL
if(is.null(credentials)) {
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
} else {
api_key <- credentials[1]
api_secret <- credentials[2]
access_token <- credentials[3]
access_token_secret <- credentials[4]
}
# connect to Twitter
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
1
# grab headlines from website
# read in html source code
base.url <- Sys.getenv("website")
base.html <- getURLContent(base.url)[[1]]
# pull off links that say "More"
links <- strsplit(base.html, "ID=")[[1]]
links2 <- sapply(strsplit(links, "</a>"), "[", 1)[-1]
more.codes <- substring(stringin("more", links2), 1, 5)
long.codes <- (min(more.codes)-2):(max(more.codes)+2)
more.codes
more.codes <- as.numeric(substring(stringin("more", links2), 1, 5))
long.codes <- (min(more.codes)-2):(max(more.codes)+2)
more.urls <- paste0(base.url, "index.php?ID=", long.codes)
# pull off headline, photo url, photo caption
pull <- function(thisurl) {
thishtml <- getURLContent(thisurl)[[1]]
# headline
headlong <- strsplit(thishtml, "<font size=6><b>")[[1]][2]
head <- strsplit(headlong, "</b>")[[1]][1]
# photo url
photolong <- strsplit(thishtml, "<img src='./Photos/")[[1]]
if(length(photolong)>1) {
photolong <- photolong[2]
photo <- strsplit(photolong, "'><br>")[[1]][1]
photo.url <- paste0(base.url, "Photos/", photo)
} else {
photo.url <- ""
}
# photo caption
caplong <- strsplit(thishtml, "<br><font size=2><b>")[[1]]
if(length(caplong)>1) {
caplong <- caplong[2]
cap <- strsplit(caplong, "</b>")[[1]][1]
} else {
cap <- ""
}
c(article.url=thisurl, headline=head, photo.url=photo.url, photo.caption=cap)
}
# get new tweets ready
m <- do.call(rbind, lapply(more.urls, pull))
currentheads <- apply(m[, 2:1], 1, paste, collapse=". ")
### grab latest tweets
adj <- getUser(Sys.getenv("username"))
oldtweets <- twListToDF(userTimeline(adj, n=5, excludeReplies=TRUE))[, c("text", "favoriteCount", "retweetCount", "created")]
names(oldtweets)[names(oldtweets)=="created"] <- "createdUTC"
### tweet all new tweets that haven't been tweeted before
totweet <- currentheads[!(substring(currentheads, 1, 30) %in% substring(oldtweets$text, 1, 30))]
totweet
q()
cleanup()
q()
?dump
df <- data.frame(a=1:3, b=4:6)
df
dput(df)
dump(df)
dump("df")
dump("df", "")
q()
# C:\JVA\ASA\CSP SC\RegistrationCountGraph.r
Date2 <- as.Date(paste0(c(rep("2014-", 2), rep("2015-", 2)), 
c("12-04", "12-29", "01-12", "01-26")))
Reg15 <- c(154, 221, 327, 365)
Date <- as.Date(paste0(c(rep("2014-", 5), rep("2015-", 6)), 
c("11-01", "11-26", "12-12", "12-19", "12-27", "01-06", "01-13", "01-22", "01-27", "02-03", "02-06")))
Reg14 <- c(36, 92, 124, 157, 209, 287, 303, 339, 345, 365, 389)
Reg13 <- c(NA, NA, 89, 125, 165, 218, 236, 266, 279, 301, 330)
L <- length(Date)
pd <- pretty(Date, 4)
windows(h=5, w=5)
par(mar=c(4, 4, 2, 2.5), yaxs="i", las=1)
plot(Date, Reg14, type="n", axes=FALSE, las=1, ylim=c(0, 1.1*max(Reg15, Reg14, Reg13, na.rm=TRUE)), 
ylab="Total Count",main="CSP Registration")
axis(1, at=pd, labels=c("Nov 1", "Dec 1", "Jan 1", "Feb 1"))
axis(2)
box()
lines(Date, Reg13, lwd=1, pch=16, type="o", col="gray")
mtext(" CSP\n 2013", side=4, at=Reg13[L], col="lightgray", adj=0)
lines(Date, Reg14, lwd=2, pch=16, type="o", col="gray")
mtext(" CSP\n 2014", side=4, at=Reg14[L], col="darkgray", adj=0)
lines(Date2, Reg15, lwd=3, pch=16, type="o")
mtext(" CSP\n 2015", side=4, at=Reg15[length(Date2)], font=2, adj=0, line=-8)
q()
?bland.altman.ade
??bland.altman.ade
# C:\JVA\GLFC\SLCB\TFViz\PrepareData.r
library(XLConnect)
library(dismo)
library(OpenStreetMap)
library(RJSONIO)
library(XML)
wb <- loadWorkbook("C:/JVA/GLFC/SLCB/TFViz/TF membership.xlsx")
dat <- readWorksheet(wb, sheet="Sheet1", startRow=3)
dat$cityorig <- dat$City
dat$City <- gsub(" ", "", dat$City)
dat$City <- gsub("\\.", "", dat$City)
dat$loc <- paste(dat$cityorig, dat$StateProv, sep=", ")
rm(wb)
### nodes
groups <- c("CORE", "BIG", "LCTF", "BTF", "TTF", "LATF", "PLEN")
add <- dat[seq(groups), 2:6]
add$Last <- groups
add[, -1] <- "x"
nodes <- rbind(dat[, c("Last", "First", "Agency", "City", "StateProv")], add)
nodes$size <- ifelse(nodes$First=="x", 18, 12)
# duplicated last names
dl <- unique(nodes$Last[duplicated(nodes$Last)])
if(length(dl)>0) {
sel <- nodes$Last %in% dl
nodes$Last[sel] <- paste0(nodes$Last[sel], substring(nodes$First[sel], 1, 1))
}
rm(add, dl)
# rev(sort(table(dat$Agency)))
 # USFWS   USGS    DFO    MSU   GLFC  USACE   OMNR    WLU  WIDNR     UG  MNDNR  MIDNR GLIFWC 
    # 18     13     13      5      3      2      2      1      1      1      1      1      1 
### links
rowz <- seq(dat[, 1])
linklist <- vector("list", length(groups))
for(i in seq(groups)) {
group <- groups[i]
memb <- dat[, group]
sel <- !is.na(memb)
linklist[[i]] <- cbind(source=rowz[sel]-1, target=match(group, nodes$Last)-1, bond=match(memb[sel], c("x", "C")))
}
links <- do.call("rbind", linklist)
# add an invisible connection between BIG (of the large cluster) and ... SLCB and PLEN
# links <- rbind(links, c(match(c("SLCB", "BIG"), nodes$Last)-1, 0.5))
# links <- rbind(links, c(match(c("PLEN", "BIG"), nodes$Last)-1, 0.5))
rm(rowz, linklist, i, group, memb, sel)
### locs
# unique location
ul <- unique(dat$loc)
uc <- dat$City[match(ul, dat$loc)]
# find lat/long
geodat <- geocode(ul)
geodat <- geodat[geodat$interpretedPlace!="Lansing Charter Township, MI, USA", ]
# get rid of ON / MI mismatches
mismatches <- c(intersect(grep("ON", geodat$originalPlace), grep("MI", geodat$interpretedPlace)),
intersect(grep("MI", geodat$originalPlace), grep("ON", geodat$interpretedPlace)))
if(length(mismatches)>0) geodat <- geodat[-mismatches, ]
locs <- cbind(long=geodat$longitude, lat=geodat$latitude, City=uc)
rm(ul, uc, geodat, mismatches)
#44.142798,-77.146854
locsdf <- as.data.frame(locs)
locsdf[, 1:2] <- apply(locsdf[, 1:2], 2, as.numeric)
attach(locsdf)
# cushion around bottom, left, top, and right
cushion <- c(0.7, 0.7, 1, 2.5)
doit <- function(SAVE, svgsize, name) {
ul <- c(max(lat)+cushion[3], min(long)-cushion[2])
lr <- c(min(lat)-cushion[1], max(long)+cushion[4])
if(SAVE) {
png(filename=paste0("C:/JVA/GLFC/SLCB/TFViz/", name, ".png"), width=svgsize[1], height=svgsize[2])
} else {
windows(w=svgsize[1]/100, h=svgsize[2]/100, xpinch=100, ypinch=100)
}
mymap <- openmap(ul, lr, type="stamen-watercolor")
mymap2 <- openproj(mymap)
plot(mymap2)
if(SAVE) {
graphics.off()
} else {
points(long, lat, pch=16, col="red")
}
print(cat(paste0("xscale domain: ", ul[2], ", ", lr[2])))
print(cat(paste0("yscale domain: ", lr[1], ", ", ul[1])))
invisible()
}
svgsizeNarrow <- c(585, 200)
svgsizeWide <- c(800, 274)
# doit(TRUE, svgsizeNarrow, "MyMapNarrow")
# doit(FALSE, svgsizeWide, "MyMapWide")
# edit png file in GIMP to add 0.5 transparency
# right click on file, edit with GIMP
# edit, fill with BG color
# edit, fade fill with background color, 50%
# export
detach(locsdf)
### output the data to a file in JSON format
cat(toJSON(list(nodes=as.matrix(nodes), links=links, locs=locs)), collapse=" ", file="C:/JVA/GLFC/SLCB/TFViz/graph.json")
geodat
ls()
locs
# C:\JVA\GLFC\SLCB\TFViz\PrepareData.r
library(XLConnect)
library(dismo)
library(OpenStreetMap)
library(RJSONIO)
library(XML)
wb <- loadWorkbook("C:/JVA/GLFC/SLCB/TFViz/TF membership.xlsx")
dat <- readWorksheet(wb, sheet="Sheet1", startRow=3)
dat$cityorig <- dat$City
dat$City <- gsub(" ", "", dat$City)
dat$City <- gsub("\\.", "", dat$City)
dat$loc <- paste(dat$cityorig, dat$StateProv, sep=", ")
rm(wb)
### nodes
groups <- c("CORE", "BIG", "LCTF", "BTF", "TTF", "LATF", "PLEN")
add <- dat[seq(groups), 2:6]
add$Last <- groups
add[, -1] <- "x"
nodes <- rbind(dat[, c("Last", "First", "Agency", "City", "StateProv")], add)
nodes$size <- ifelse(nodes$First=="x", 18, 12)
# duplicated last names
dl <- unique(nodes$Last[duplicated(nodes$Last)])
if(length(dl)>0) {
sel <- nodes$Last %in% dl
nodes$Last[sel] <- paste0(nodes$Last[sel], substring(nodes$First[sel], 1, 1))
}
rm(add, dl)
# rev(sort(table(dat$Agency)))
 # USFWS   USGS    DFO    MSU   GLFC  USACE   OMNR    WLU  WIDNR     UG  MNDNR  MIDNR GLIFWC 
    # 18     13     13      5      3      2      2      1      1      1      1      1      1 
### links
rowz <- seq(dat[, 1])
linklist <- vector("list", length(groups))
for(i in seq(groups)) {
group <- groups[i]
memb <- dat[, group]
sel <- !is.na(memb)
linklist[[i]] <- cbind(source=rowz[sel]-1, target=match(group, nodes$Last)-1, bond=match(memb[sel], c("x", "C")))
}
links <- do.call("rbind", linklist)
# add an invisible connection between BIG (of the large cluster) and ... SLCB and PLEN
# links <- rbind(links, c(match(c("SLCB", "BIG"), nodes$Last)-1, 0.5))
# links <- rbind(links, c(match(c("PLEN", "BIG"), nodes$Last)-1, 0.5))
rm(rowz, linklist, i, group, memb, sel)
### locs
# unique location
ul <- unique(dat$loc)
uc <- dat$City[match(ul, dat$loc)]
# find lat/long
geodat <- geocode(ul)
geodat <- geodat[geodat$interpretedPlace!="Lansing Charter Township, MI, USA", ]
# get rid of ON / MI mismatches
mismatches <- c(intersect(grep("ON", geodat$originalPlace), grep("MI", geodat$interpretedPlace)),
intersect(grep("MI", geodat$originalPlace), grep("ON", geodat$interpretedPlace)))
if(length(mismatches)>0) geodat <- geodat[-mismatches, ]
locs <- cbind(long=geodat$longitude, lat=geodat$latitude, City=uc)
rm(ul, uc, geodat, mismatches)
#44.142798,-77.146854
locsdf <- as.data.frame(locs)
locsdf[, 1:2] <- apply(locsdf[, 1:2], 2, as.numeric)
attach(locsdf)
# cushion around bottom, left, top, and right
cushion <- c(0.7, 0.7, 1, 2.5)
doit <- function(SAVE, svgsize, name) {
ul <- c(max(lat)+cushion[3], min(long)-cushion[2])
lr <- c(min(lat)-cushion[1], max(long)+cushion[4])
if(SAVE) {
png(filename=paste0("C:/JVA/GLFC/SLCB/TFViz/", name, ".png"), width=svgsize[1], height=svgsize[2])
} else {
windows(w=svgsize[1]/100, h=svgsize[2]/100, xpinch=100, ypinch=100)
}
mymap <- openmap(ul, lr, type="stamen-watercolor")
mymap2 <- openproj(mymap)
plot(mymap2)
if(SAVE) {
graphics.off()
} else {
points(long, lat, pch=16, col="red")
}
print(cat(paste0("xscale domain: ", ul[2], ", ", lr[2])))
print(cat(paste0("yscale domain: ", lr[1], ", ", ul[1])))
invisible()
}
svgsizeNarrow <- c(585, 200)
svgsizeWide <- c(800, 274)
# doit(TRUE, svgsizeNarrow, "MyMapNarrow")
# doit(FALSE, svgsizeWide, "MyMapWide")
# edit png file in GIMP to add 0.5 transparency
# right click on file, edit with GIMP
# edit, fill with BG color
# edit, fade fill with background color, 50%
# export
detach(locsdf)
### output the data to a file in JSON format
cat(toJSON(list(nodes=as.matrix(nodes), links=links, locs=locs)), collapse=" ", file="C:/JVA/GLFC/SLCB/TFViz/graph.json")
cleanup()
q()
tweethead()
q()
tweethead()
q()
tweethead()
# C:\JVA\Consult\Kraus\Gill net walleye\Gill Net Compare.r
library(lme4)
# some needed functions
mytable <- function(...) table(..., useNA="ifany")
allcombs <- function(num, from=0, to=num) {
m <- as.matrix(expand.grid(rep(list(0:1), times=num))) 
n.items <- rowSums(m) 
m[n.items >= from & n.items <= to, ] 
}
# read in and prepare the data
dat <- read.csv("C:/JVA/Consult/Kraus/Gill net walleye/GLIMMIX_alldata_commonmeshes.txt", sep="",
header=FALSE, col.names=c("year", "type", "mesh", "we", "holes", "catch", "secchi", "site", "serial"))
dat$sitef <- as.factor(dat$site)
dat$typef <- as.factor(dat$type)
dat$other <- dat$catch - dat$we
dat$secchig <- cut(dat$secchi, c(0, 1, 2, 4), right=FALSE, labels=FALSE)
dat <- dat[order(dat$year, dat$site, dat$type, dat$mesh), ]
# need to check on an error here ... one case with catch=7 and we=8
dat[dat$other < -0.5, ]
    # year type mesh we holes catch secchi site serial sitef typef other
# 535 2012    3  3.5  8  3933     7      1   23    834    23     3    -1
# site is a random effect representing both spatial (location) and temporal (year) variability
# mesh is a covariate
# type is a factor
obs <- interaction(dat$year, dat$site, dat$type, dat$mesh)
dat[obs %in% obs[duplicated(obs)], ]
# for now, subset site<30, sites greater than 29 were fished by OMNR, and their data were structured differently
# and ignore record with we > catch
sub <- dat[dat$site < 29.5 & dat$other > -0.5, ]
attach(sub)
obs <- interaction(year, site, type, mesh)
sub[obs %in% obs[duplicated(obs)], ]
rm(obs)
sy <- paste(format(site), year, sep="-")
tm <- paste(type, format(10*mesh), sep="-")
# design
mytable(sy, tm)
# covariate
tapply(secchi, list(sy, tm), mean)
# response
tapply(we, list(sy, tm), mean)
rm(sy, tm)
# fit a mixed effects model with all two-way interactions of fixed effects (other, secchi, mesh, type) and site as a random effect
terms <- c("sqrt(other)", "secchi", "poly(mesh,2)", "typef", 
"sqrt(other)*secchi", "sqrt(other)*poly(mesh,2)", "sqrt(other)*typef", "secchi*poly(mesh,2)", "secchi*typef", "poly(mesh,2)*typef")
all <- allcombs(length(terms))
colnames(all) <- terms
del1 <- all[, "sqrt(other)*secchi"]==1 & (all[, "sqrt(other)"]==0 | all[, "secchi"]==0)
del2 <- all[, "sqrt(other)*poly(mesh,2)"]==1 & (all[, "sqrt(other)"]==0 | all[, "poly(mesh,2)"]==0)
del3 <- all[, "sqrt(other)*typef"]==1 & (all[, "sqrt(other)"]==0 | all[, "typef"]==0)
del4 <- all[, "secchi*poly(mesh,2)"]==1 & (all[, "secchi"]==0 | all[, "poly(mesh,2)"]==0)
del5 <- all[, "secchi*typef"]==1 & (all[, "secchi"]==0 | all[, "typef"]==0)
del6 <- all[, "poly(mesh,2)*typef"]==1 & (all[, "poly(mesh,2)"]==0 | all[, "typef"]==0)
all2 <- all[!del1 & !del2 & !del3 & !del4 & !del5 & !del6, ]
formulae <- paste0("sqrt(we)~", apply(all2==1, 1, function(row) paste(terms[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
fits <- lapply(formulae, lmer)
aic <- sapply(fits, AIC)
aicresults <- data.frame(formulae, aic, daic=aic-min(aic))
aicresults <- aicresults[order(aicresults$daic), ]
aicresults[1:10, ]
formulae[aic-min(aic) < 2]
rm(terms, all, del1, del2, del3, del4, del5, del6, all2, formulae, fits, aic)
mytable(sy, tm)
attach(dat)
obs <- interaction(year, site, type, mesh)
dat[obs %in% obs[duplicated(obs)], ]
detach(dat)
cleanup()
# C:\JVA\Consult\Kraus\Gill net walleye\Gill Net Compare.r
library(lme4)
# some needed functions
mytable <- function(...) table(..., useNA="ifany")
allcombs <- function(num, from=0, to=num) {
m <- as.matrix(expand.grid(rep(list(0:1), times=num))) 
n.items <- rowSums(m) 
m[n.items >= from & n.items <= to, ] 
}
# read in and prepare the data
dat <- read.csv("C:/JVA/Consult/Kraus/Gill net walleye/GLIMMIX_alldata_commonmeshes.txt", sep="",
header=FALSE, col.names=c("year", "type", "mesh", "we", "holes", "catch", "secchi", "site", "serial"))
dat$sitef <- as.factor(dat$site)
dat$typef <- as.factor(dat$type)
dat$other <- dat$catch - dat$we
dat$secchig <- cut(dat$secchi, c(0, 1, 2, 4), right=FALSE, labels=FALSE)
dat <- dat[order(dat$year, dat$site, dat$type, dat$mesh), ]
# site is a random effect representing both spatial (location) and temporal (year) variability
# mesh is a covariate
# type is a factor
obs <- interaction(dat$year, dat$site, dat$type, dat$mesh)
dat[obs %in% obs[duplicated(obs)], ]
dim(obs)
length(obs)
rm(obs)
ls()
library(lme4)
# some needed functions
mytable <- function(...) table(..., useNA="ifany")
allcombs <- function(num, from=0, to=num) {
m <- as.matrix(expand.grid(rep(list(0:1), times=num))) 
n.items <- rowSums(m) 
m[n.items >= from & n.items <= to, ] 
}
# read in and prepare the data
dat <- read.csv("C:/JVA/Consult/Kraus/Gill net walleye/GLIMMIX_alldata_commonmeshes.txt", sep="",
header=FALSE, col.names=c("year", "type", "mesh", "we", "holes", "catch", "secchi", "site", "serial"))
dat$sitef <- as.factor(dat$site)
dat$typef <- as.factor(dat$type)
dat$other <- dat$catch - dat$we
dat$secchig <- cut(dat$secchi, c(0, 1, 2, 4), right=FALSE, labels=FALSE)
dat <- dat[order(dat$year, dat$site, dat$type, dat$mesh), ]
# site is a random effect representing both spatial (location) and temporal (year) variability
# mesh is a covariate
# type is a factor
ls()
attach(dat)
sy <- paste(format(site), year, sep="-")
tm <- paste(type, format(10*mesh), sep="-")
# design
mytable(sy, tm)
# covariate
tapply(secchi, list(sy, tm), mean)
# response
tapply(we, list(sy, tm), mean)
rm(sy, tm)
search()
detach(3)
# fit a mixed effects model with all two-way interactions of fixed effects (other, secchi, mesh, type) and site as a random effect
terms <- c("sqrt(other)", "secchi", "poly(mesh,2)", "typef", 
"sqrt(other)*secchi", "sqrt(other)*poly(mesh,2)", "sqrt(other)*typef", "secchi*poly(mesh,2)", "secchi*typef", "poly(mesh,2)*typef")
all <- allcombs(length(terms))
colnames(all) <- terms
del1 <- all[, "sqrt(other)*secchi"]==1 & (all[, "sqrt(other)"]==0 | all[, "secchi"]==0)
del2 <- all[, "sqrt(other)*poly(mesh,2)"]==1 & (all[, "sqrt(other)"]==0 | all[, "poly(mesh,2)"]==0)
del3 <- all[, "sqrt(other)*typef"]==1 & (all[, "sqrt(other)"]==0 | all[, "typef"]==0)
del4 <- all[, "secchi*poly(mesh,2)"]==1 & (all[, "secchi"]==0 | all[, "poly(mesh,2)"]==0)
del5 <- all[, "secchi*typef"]==1 & (all[, "secchi"]==0 | all[, "typef"]==0)
del6 <- all[, "poly(mesh,2)*typef"]==1 & (all[, "poly(mesh,2)"]==0 | all[, "typef"]==0)
all2 <- all[!del1 & !del2 & !del3 & !del4 & !del5 & !del6, ]
formulae <- paste0("sqrt(we)~", apply(all2==1, 1, function(row) paste(terms[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
fits <- lapply(formulae, lmer)
aic <- sapply(fits, AIC)
aicresults <- data.frame(formulae, aic, daic=aic-min(aic))
aicresults <- aicresults[order(aicresults$daic), ]
aicresults[1:10, ]
formulae[aic-min(aic) < 2]
rm(terms, all, del1, del2, del3, del4, del5, del6, all2, formulae, fits, aic)
predict(fit)
resid(fit)
fit <- lmer(sqrt(we) ~ sqrt(other) + secchi + poly(mesh, 2) + typef + secchi*poly(mesh, 2) + poly(mesh, 2)*typef + (1 | sitef))
predict(fit)
resid(fit)
?predict.lmer
class(fit)
?predict.lmerMod
?lmer
?predict
?predict.merMod
?resid
resid
class(fit)
# predictions and residuals with random effects
pred.r <- predict(fit, re.form=NULL)
resid.r <- pred.r - sqrt(we)
# predictions and residuals without random effects
pred.f <- predict(fit, re.form=NA)
resid.f <- pred.f - sqrt(we)
dim(dat)
length(pred.r)
head(dat)
ls()
rm(pred.f, pred.r, resid.f, resid.r)
ls()
# predictions and residuals with random effects
dat$pred.r <- predict(fit, re.form=NULL)
dat$resid.r <- dat$pred.r - sqrt(we)
# predictions and residuals without random effects
dat$pred.f <- predict(fit, re.form=NA)
dat$resid.f <- dat$pred.f - sqrt(we)
head(dat)
all2
# fit a mixed effects model with all two-way interactions of fixed effects (other, secchi, mesh, type) and site as a random effect
terms <- c("sqrt(other)", "secchi", "poly(mesh,2)", "typef", 
"sqrt(other)*secchi", "sqrt(other)*poly(mesh,2)", "sqrt(other)*typef", "secchi*poly(mesh,2)", "secchi*typef", "poly(mesh,2)*typef")
all <- allcombs(length(terms))
colnames(all) <- terms
del1 <- all[, "sqrt(other)*secchi"]==1 & (all[, "sqrt(other)"]==0 | all[, "secchi"]==0)
del2 <- all[, "sqrt(other)*poly(mesh,2)"]==1 & (all[, "sqrt(other)"]==0 | all[, "poly(mesh,2)"]==0)
del3 <- all[, "sqrt(other)*typef"]==1 & (all[, "sqrt(other)"]==0 | all[, "typef"]==0)
del4 <- all[, "secchi*poly(mesh,2)"]==1 & (all[, "secchi"]==0 | all[, "poly(mesh,2)"]==0)
del5 <- all[, "secchi*typef"]==1 & (all[, "secchi"]==0 | all[, "typef"]==0)
del6 <- all[, "poly(mesh,2)*typef"]==1 & (all[, "poly(mesh,2)"]==0 | all[, "typef"]==0)
all2 <- all[!del1 & !del2 & !del3 & !del4 & !del5 & !del6, ]
all2
dim(all2)
head(all2)
stringin
grep("mesh", names(all2))
names(all2)
class(all2)
colnames(all2)
grep("mesh", colnames(all2))
apply(all2[, grep("mesh", colnames(all2))], sum)
apply(all2[, grep("mesh", colnames(all2))], 1, sum)
all2[apply(all2[, grep("mesh", colnames(all2))], 1, sum) > 0, ]
all2[apply(all2[, grep("mesh", colnames(all2))], 1, sum) > 0, ]all2categ <- all2[apply(all2[, grep("mesh", colnames(all2))], 1, sum) > 0, ]
all2categ <- all2[apply(all2[, grep("mesh", colnames(all2))], 1, sum) > 0, ]
summary(all2categ)
all2[all2[, "poly(mesh,2)"] > 0, ]
dim(all2[all2[, "poly(mesh,2)"] > 0, ])
dim(all2categ)
all2categ <- all2[all2[, "poly(mesh,2)"] > 0, ]
colnames(all2)
gsub("poly(mesh,2)", "as.factor(mesh)", colnames(all2))
gsub("poly(mesh,2)", "as.factor(mesh)", colnames(all2), fixed=TRUE)
# pull off all the models that include a mesh term, copy this list, and replace the covariate mesh with the categorical mesh
all2categ <- all2[all2[, "poly(mesh,2)"] > 0, ]
colnames(all2categ) <- gsub("poly(mesh,2)", "as.factor(mesh)", colnames(all2categ), fixed=TRUE)
head(all2)
head(all2categ)
?rbind.fill
both <- rbind.fill(all2, all2categ)
library(plyr)
both <- rbind.fill(all2, all2categ)
both <- rbind.fill(as.data.frame(all2), as.data.frame(all2categ))
head(both)
dim(both)
dim(all2)
dim(all2categ)
both <- rbind.fill(as.data.frame(all2), as.data.frame(all2categ))
both[is.na(both)] <- 0
head(both)
both <- as.matrix(both)
head(both)
# pull off all the models that include a mesh term, copy this list, and replace the covariate mesh with the categorical mesh
all2categ <- all2[all2[, "poly(mesh,2)"] > 0, ]
colnames(all2categ) <- gsub("poly(mesh,2)", "as.factor(mesh)", colnames(all2categ), fixed=TRUE)
# put all models together in one matrix (no mesh models, covariate mesh models, and categorical mesh models)
library(plyr)
all3 <- rbind.fill(as.data.frame(all2), as.data.frame(all2categ))
all3[is.na(all3)] <- 0
all3 <- as.matrix(all3)
formulae <- paste0("sqrt(we)~", apply(all3==1, 1, function(row) paste(terms[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
fits <- lapply(formulae, lmer)
aic <- sapply(fits, AIC)
aicresults <- data.frame(formulae, aic, daic=aic-min(aic))
aicresults <- aicresults[order(aicresults$daic), ]
aicresults[1:10, ]
formulae[aic-min(aic) < 2]
rm(terms, all, del1, del2, del3, del4, del5, del6, all3, formulae, fits, aic)
cleanup()
search()
detach(3)
# C:\JVA\Consult\Kraus\Gill net walleye\Gill Net Compare.r
library(lme4)
# some needed functions
mytable <- function(...) table(..., useNA="ifany")
allcombs <- function(num, from=0, to=num) {
m <- as.matrix(expand.grid(rep(list(0:1), times=num))) 
n.items <- rowSums(m) 
m[n.items >= from & n.items <= to, ] 
}
# read in and prepare the data
dat <- read.csv("C:/JVA/Consult/Kraus/Gill net walleye/GLIMMIX_alldata_commonmeshes.txt", sep="",
header=FALSE, col.names=c("year", "type", "mesh", "we", "holes", "catch", "secchi", "site", "serial"))
dat$sitef <- as.factor(dat$site)
dat$typef <- as.factor(dat$type)
dat$other <- dat$catch - dat$we
dat$secchig <- cut(dat$secchi, c(0, 1, 2, 4), right=FALSE, labels=FALSE)
dat <- dat[order(dat$year, dat$site, dat$type, dat$mesh), ]
attach(dat)
# site is a random effect representing both spatial (location) and temporal (year) variability
# type is a factor
# mesh is a covariate
sy <- paste(format(site), year, sep="-")
tm <- paste(type, format(10*mesh), sep="-")
# design
mytable(sy, tm)
# covariate
tapply(secchi, list(sy, tm), mean)
# response
tapply(we, list(sy, tm), mean)
rm(sy, tm)
# fit a mixed effects model with all two-way interactions of fixed effects (other, secchi, mesh, type) and site as a random effect
terms <- c("sqrt(other)", "secchi", "poly(mesh,2)", "typef", 
"sqrt(other)*secchi", "sqrt(other)*poly(mesh,2)", "sqrt(other)*typef", "secchi*poly(mesh,2)", "secchi*typef", "poly(mesh,2)*typef")
all <- allcombs(length(terms))
colnames(all) <- terms
del1 <- all[, "sqrt(other)*secchi"]==1 & (all[, "sqrt(other)"]==0 | all[, "secchi"]==0)
del2 <- all[, "sqrt(other)*poly(mesh,2)"]==1 & (all[, "sqrt(other)"]==0 | all[, "poly(mesh,2)"]==0)
del3 <- all[, "sqrt(other)*typef"]==1 & (all[, "sqrt(other)"]==0 | all[, "typef"]==0)
del4 <- all[, "secchi*poly(mesh,2)"]==1 & (all[, "secchi"]==0 | all[, "poly(mesh,2)"]==0)
del5 <- all[, "secchi*typef"]==1 & (all[, "secchi"]==0 | all[, "typef"]==0)
del6 <- all[, "poly(mesh,2)*typef"]==1 & (all[, "poly(mesh,2)"]==0 | all[, "typef"]==0)
all2 <- all[!del1 & !del2 & !del3 & !del4 & !del5 & !del6, ]
# pull off all the models that include a mesh term, copy this list, and replace the covariate mesh with the categorical mesh
all2categ <- all2[all2[, "poly(mesh,2)"] > 0, ]
colnames(all2categ) <- gsub("poly(mesh,2)", "as.factor(mesh)", colnames(all2categ), fixed=TRUE)
# put all models together in one matrix (no mesh models, covariate mesh models, and categorical mesh models)
library(plyr)
all3 <- rbind.fill(as.data.frame(all2), as.data.frame(all2categ))
all3[is.na(all3)] <- 0
all3 <- as.matrix(all3)
formulae <- paste0("sqrt(we)~", apply(all3==1, 1, function(row) paste(terms[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
fits <- lapply(formulae, lmer)
aic <- sapply(fits, AIC)
aicresults <- data.frame(formulae, aic, daic=aic-min(aic))
aicresults <- aicresults[order(aicresults$daic), ]
aicresults[1:10, ]
formulae[aic-min(aic) < 2]
rm(terms, all, del1, del2, del3, del4, del5, del6, all3, formulae, fits, aic)
cleanup()
search()
detach()
# C:\JVA\Consult\Kraus\Gill net walleye\Gill Net Compare.r
library(lme4)
# some needed functions
mytable <- function(...) table(..., useNA="ifany")
allcombs <- function(num, from=0, to=num) {
m <- as.matrix(expand.grid(rep(list(0:1), times=num))) 
n.items <- rowSums(m) 
m[n.items >= from & n.items <= to, ] 
}
# read in and prepare the data
dat <- read.csv("C:/JVA/Consult/Kraus/Gill net walleye/GLIMMIX_alldata_commonmeshes.txt", sep="",
header=FALSE, col.names=c("year", "type", "mesh", "we", "holes", "catch", "secchi", "site", "serial"))
dat$sitef <- as.factor(dat$site)
dat$typef <- as.factor(dat$type)
dat$meshf <- as.factor(dat$mesh)
dat$other <- dat$catch - dat$we
dat$secchig <- cut(dat$secchi, c(0, 1, 2, 4), right=FALSE, labels=FALSE)
dat <- dat[order(dat$year, dat$site, dat$type, dat$mesh), ]
attach(dat)
# site is a random effect representing both spatial (location) and temporal (year) variability
# type is a factor
# mesh is a covariate
sy <- paste(format(site), year, sep="-")
tm <- paste(type, format(10*mesh), sep="-")
# design
mytable(sy, tm)
# covariate
tapply(secchi, list(sy, tm), mean)
# response
tapply(we, list(sy, tm), mean)
rm(sy, tm)
# fit a mixed effects model with all two-way interactions of fixed effects (other, secchi, mesh, type) and site as a random effect
terms <- c("sqrt(other)", "secchi", "poly(mesh,2)", "typef", 
"sqrt(other)*secchi", "sqrt(other)*poly(mesh,2)", "sqrt(other)*typef", "secchi*poly(mesh,2)", "secchi*typef", "poly(mesh,2)*typef")
all <- allcombs(length(terms))
colnames(all) <- terms
del1 <- all[, "sqrt(other)*secchi"]==1 & (all[, "sqrt(other)"]==0 | all[, "secchi"]==0)
del2 <- all[, "sqrt(other)*poly(mesh,2)"]==1 & (all[, "sqrt(other)"]==0 | all[, "poly(mesh,2)"]==0)
del3 <- all[, "sqrt(other)*typef"]==1 & (all[, "sqrt(other)"]==0 | all[, "typef"]==0)
del4 <- all[, "secchi*poly(mesh,2)"]==1 & (all[, "secchi"]==0 | all[, "poly(mesh,2)"]==0)
del5 <- all[, "secchi*typef"]==1 & (all[, "secchi"]==0 | all[, "typef"]==0)
del6 <- all[, "poly(mesh,2)*typef"]==1 & (all[, "poly(mesh,2)"]==0 | all[, "typef"]==0)
all2 <- all[!del1 & !del2 & !del3 & !del4 & !del5 & !del6, ]
# pull off all the models that include a mesh term, copy this list, and replace the covariate mesh with the categorical mesh
all2categ <- all2[all2[, "poly(mesh,2)"] > 0, ]
colnames(all2categ) <- gsub("poly(mesh,2)", "meshf", colnames(all2categ), fixed=TRUE)
# put all models together in one matrix (no mesh models, covariate mesh models, and categorical mesh models)
library(plyr)
all3 <- rbind.fill(as.data.frame(all2), as.data.frame(all2categ))
all3[is.na(all3)] <- 0
all3 <- as.matrix(all3)
formulae <- paste0("sqrt(we)~", apply(all3==1, 1, function(row) paste(terms[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
fits <- lapply(formulae, lmer)
aic <- sapply(fits, AIC)
aicresults <- data.frame(formulae, aic, daic=aic-min(aic))
aicresults <- aicresults[order(aicresults$daic), ]
aicresults[1:10, ]
formulae[aic-min(aic) < 2]
rm(terms, all, del1, del2, del3, del4, del5, del6, all3, formulae, fits, aic)
formulae
# site is a random effect representing both spatial (location) and temporal (year) variability
# type is a factor
# mesh is a covariate
sy <- paste(format(site), year, sep="-")
tm <- paste(type, format(10*mesh), sep="-")
# design
mytable(sy, tm)
# covariate
tapply(secchi, list(sy, tm), mean)
# response
tapply(we, list(sy, tm), mean)
rm(sy, tm)
# fit a mixed effects model with all two-way interactions of fixed effects (other, secchi, mesh, type) and site as a random effect
terms <- c("sqrt(other)", "secchi", "poly(mesh,2)", "typef", 
"sqrt(other)*secchi", "sqrt(other)*poly(mesh,2)", "sqrt(other)*typef", "secchi*poly(mesh,2)", "secchi*typef", "poly(mesh,2)*typef")
all <- allcombs(length(terms))
colnames(all) <- terms
del1 <- all[, "sqrt(other)*secchi"]==1 & (all[, "sqrt(other)"]==0 | all[, "secchi"]==0)
del2 <- all[, "sqrt(other)*poly(mesh,2)"]==1 & (all[, "sqrt(other)"]==0 | all[, "poly(mesh,2)"]==0)
del3 <- all[, "sqrt(other)*typef"]==1 & (all[, "sqrt(other)"]==0 | all[, "typef"]==0)
del4 <- all[, "secchi*poly(mesh,2)"]==1 & (all[, "secchi"]==0 | all[, "poly(mesh,2)"]==0)
del5 <- all[, "secchi*typef"]==1 & (all[, "secchi"]==0 | all[, "typef"]==0)
del6 <- all[, "poly(mesh,2)*typef"]==1 & (all[, "poly(mesh,2)"]==0 | all[, "typef"]==0)
all2 <- all[!del1 & !del2 & !del3 & !del4 & !del5 & !del6, ]
# pull off all the models that include a mesh term, copy this list, and replace the covariate mesh with the categorical mesh
all2categ <- all2[all2[, "poly(mesh,2)"] > 0, ]
colnames(all2categ) <- gsub("poly(mesh,2)", "meshf", colnames(all2categ), fixed=TRUE)
# put all models together in one matrix (no mesh models, covariate mesh models, and categorical mesh models)
library(plyr)
all3 <- rbind.fill(as.data.frame(all2), as.data.frame(all2categ))
all3[is.na(all3)] <- 0
all3 <- as.matrix(all3)
formulae <- paste0("sqrt(we)~", apply(all3==1, 1, function(row) paste(terms[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
formulae
summary(all3)
all3==1
summary(all3==1)
formulae <- paste0("sqrt(we)~", apply(all3==1, 1, function(row) paste(terms[row], collapse="+")), "+(1|sitef)")
formulae
all3[110:120, ]
length(terms)
dim(all2)
dim(all3)
colnames(all2)
colnames(all3)
formulae <- paste0("sqrt(we)~", apply(all3==1, 1, function(row) paste(colnames(all3)[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
formulae
fits <- lapply(formulae, lmer)
aic <- sapply(fits, AIC)
aicresults <- data.frame(formulae, aic, daic=aic-min(aic))
aicresults <- aicresults[order(aicresults$daic), ]
aicresults[1:10, ]
formulae[aic-min(aic) < 2]
rm(terms, all, del1, del2, del3, del4, del5, del6, all3, formulae, fits, aic)
# you can read the help on the predict function for lmer models here
?predict.merMod
# predictions and residuals with random effects
dat$pred.r <- predict(fit, re.form=NULL)
dat$resid.r <- dat$pred.r - sqrt(we)
# predictions and residuals without random effects
dat$pred.f <- predict(fit, re.form=NA)
dat$resid.f <- dat$pred.f - sqrt(we)
# the simplest version of the best models (daic < 2) was
fit <- lmer(sqrt(we) ~ sqrt(other) + secchi + poly(mesh, 2) + typef + secchi*poly(mesh, 2) + poly(mesh, 2)*typef + (1 | sitef))
# you can read the help on the predict function for lmer models here
#   ?predict.merMod
# predictions and residuals with random effects
dat$pred.r <- predict(fit, re.form=NULL)
dat$resid.r <- dat$pred.r - sqrt(we)
# predictions and residuals without random effects
dat$pred.f <- predict(fit, re.form=NA)
dat$resid.f <- dat$pred.f - sqrt(we)
detach(dat)
# fit a mixed effects model with all two-way interactions of fixed effects (other, secchi, mesh, type) and site as a random effect
terms <- c("sqrt(other)", "secchi", "poly(mesh,2)", "typef", 
"sqrt(other)*secchi", "sqrt(other)*poly(mesh,2)", "sqrt(other)*typef", "secchi*poly(mesh,2)", "secchi*typef", "poly(mesh,2)*typef")
all <- allcombs(length(terms))
colnames(all) <- terms
del1 <- all[, "sqrt(other)*secchi"]==1 & (all[, "sqrt(other)"]==0 | all[, "secchi"]==0)
del2 <- all[, "sqrt(other)*poly(mesh,2)"]==1 & (all[, "sqrt(other)"]==0 | all[, "poly(mesh,2)"]==0)
del3 <- all[, "sqrt(other)*typef"]==1 & (all[, "sqrt(other)"]==0 | all[, "typef"]==0)
del4 <- all[, "secchi*poly(mesh,2)"]==1 & (all[, "secchi"]==0 | all[, "poly(mesh,2)"]==0)
del5 <- all[, "secchi*typef"]==1 & (all[, "secchi"]==0 | all[, "typef"]==0)
del6 <- all[, "poly(mesh,2)*typef"]==1 & (all[, "poly(mesh,2)"]==0 | all[, "typef"]==0)
all2 <- all[!del1 & !del2 & !del3 & !del4 & !del5 & !del6, ]
search()
# pull off all the models that include a mesh term, copy this list, and replace the covariate mesh with the categorical mesh
all2categ <- all2[all2[, "poly(mesh,2)"] > 0, ]
colnames(all2categ) <- gsub("poly(mesh,2)", "meshf", colnames(all2categ), fixed=TRUE)
# put all models together in one matrix (no mesh models, covariate mesh models, and categorical mesh models)
library(plyr)
all3 <- rbind.fill(as.data.frame(all2), as.data.frame(all2categ))
all3[is.na(all3)] <- 0
all3 <- as.matrix(all3)
formulae <- paste0("sqrt(we)~", apply(all3==1, 1, function(row) paste(colnames(all3)[row], collapse="+")), "+(1|sitef)")
formulae[1] <- "sqrt(we)~1+(1|sitef)"
fits <- lapply(formulae, lmer, dat)
aic <- sapply(fits, AIC)
aicresults <- data.frame(formulae, aic, daic=aic-min(aic))
aicresults <- aicresults[order(aicresults$daic), ]
aicresults[1:10, ]
formulae[aic-min(aic) < 2]
rm(terms, all, del1, del2, del3, del4, del5, del6, all3, formulae, fits, aic)
# the simplest version of the best models (daic < 2) was
fit <- lmer(sqrt(we) ~ sqrt(other) + secchi + poly(mesh, 2) + typef + secchi*poly(mesh, 2) + poly(mesh, 2)*typef + (1 | sitef))
fit <- lmer(sqrt(we) ~ sqrt(other) + secchi + poly(mesh, 2) + typef + secchi*poly(mesh, 2) + poly(mesh, 2)*typef + (1 | sitef), dat)
# predictions and residuals with random effects
dat$pred.r <- predict(fit, re.form=NULL)
dat$resid.r <- dat$pred.r - sqrt(we)
# predictions and residuals without random effects
dat$pred.f <- predict(fit, re.form=NA)
dat$resid.f <- dat$pred.f - sqrt(we)
head(dat)
# predictions and residuals with random effects
dat$pred.r <- predict(fit, re.form=NULL)
dat$resid.r <- dat$pred.r - sqrt(dat$we)
# predictions and residuals without random effects
dat$pred.f <- predict(fit, re.form=NA)
dat$resid.f <- dat$pred.f - sqrt(dat$we)
head(dat)
aicresults[1:10, ]
aicresults[, 1]
grep("poly", aicresults[, 1])
grep("meshf", aicresults[, 1])
aicresults[1:38, ]
aicc
AICc
aicresult$aic.woe <- exp(-aicresults$daic/2)/sum(exp(-aicresults$daic/2))
aicresults$aic.woe <- exp(-aicresults$daic/2)/sum(exp(-aicresults$daic/2))
aicresults[1:38, ]
formulae[aic-min(aic) < 2]
formulae[aicresults$aic-min(aicresults$aic) < 2]
aicresults$formulae[aicresults$aic-min(aicresults$aic) < 2]
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 10 July 2014", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat2[is.na(dat2$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
sethdat <- dat2[!is.na(dat2$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- sethdat[sethdat$ID %in% sethdat$ID[duplicated(sethdat$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
sethdat <- sethdat[!(sethdat$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(sethdat[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(sethdat$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(sethdat[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
if(FALSE) {
# compare new resids to Seth's resids
windows()
plot(rdat[, 15], sethdat[, rtrussvars][, 15])
lm(sethdat[, rtrussvars][, 15] ~ rdat[, 15])
# compare new resids to Seth's resids
windows()
plot(rdat[, 25], sethdat[, rtrussvars][, 25])
lm(sethdat[, rtrussvars][, 25] ~ rdat[, 25])
}
rm(wb, dat1, datraw, dat2, sethdat, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), 1:10]
tab[, 8:10] <- round(tab[, 8:10], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", figcount, " and ", figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
ks <- 1:14
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to 14 (since there were 14 sites).",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title="14 Sites, Both Sexes")
figu("Recommended number of clusters based on the optimum average silhouette width, 14 sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, 14 sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
fig <- function() fig.grpdif(title="14 Sites, Both Sexes")
figu("Difference in truss measurements between the cluster groups, 14 sites and both sexes combined, 14 sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title="14 Sites, Both Sexes")
figu("Centroid cluster analysis starting with a single cluster at each site, 14 sites and both sexes combined.", h=4, w=4)
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males at 13 sites was ", pamcl$nc, " (Figure ", figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Males")
figu("Recommended number of clusters based on the optimum average silhouette width, males at 13 sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Males")
figu("Difference in truss measurements between the cluster groups, males at 13 sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title="13 Sites, Males")
figu("Centroid cluster analysis starting with a single cluster at each site, males at 13 sites.", h=4, w=4)
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females at 13 sites was ", pamcl$nc, " (Figure ", figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Females")
figu("Recommended number of clusters based on the optimum average silhouette width, females at 13 sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Females")
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at 13 sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title="13 Sites, Females")
figu("Centroid cluster analysis starting with a single cluster at each site, females at 13 sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in c(2, 4, 7, 8)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
?cheat
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 10 July 2014", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat2[is.na(dat2$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
sethdat <- dat2[!is.na(dat2$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- sethdat[sethdat$ID %in% sethdat$ID[duplicated(sethdat$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
sethdat <- sethdat[!(sethdat$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(sethdat[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(sethdat$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(sethdat[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
if(FALSE) {
# compare new resids to Seth's resids
windows()
plot(rdat[, 15], sethdat[, rtrussvars][, 15])
lm(sethdat[, rtrussvars][, 15] ~ rdat[, 15])
# compare new resids to Seth's resids
windows()
plot(rdat[, 25], sethdat[, rtrussvars][, 25])
lm(sethdat[, rtrussvars][, 25] ~ rdat[, 25])
}
rm(wb, dat1, datraw, dat2, sethdat, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), 1:10]
tab[, 8:10] <- round(tab[, 8:10], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
ks <- 1:14
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to 14 (since there were 14 sites).",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title="14 Sites, Both Sexes")
figu("Recommended number of clusters based on the optimum average silhouette width, 14 sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, 14 sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
fig <- function() fig.grpdif(title="14 Sites, Both Sexes")
figu("Difference in truss measurements between the cluster groups, 14 sites and both sexes combined, 14 sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title="14 Sites, Both Sexes")
figu("Centroid cluster analysis starting with a single cluster at each site, 14 sites and both sexes combined.", h=4, w=4)
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Males")
figu("Recommended number of clusters based on the optimum average silhouette width, males at 13 sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Males")
figu("Difference in truss measurements between the cluster groups, males at 13 sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title="13 Sites, Males")
figu("Centroid cluster analysis starting with a single cluster at each site, males at 13 sites.", h=4, w=4)
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Females")
figu("Recommended number of clusters based on the optimum average silhouette width, females at 13 sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Females")
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at 13 sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title="13 Sites, Females")
figu("Centroid cluster analysis starting with a single cluster at each site, females at 13 sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in c(2, 4, 7, 8)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 10 July 2014", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat2[is.na(dat2$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
sethdat <- dat2[!is.na(dat2$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- sethdat[sethdat$ID %in% sethdat$ID[duplicated(sethdat$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
sethdat <- sethdat[!(sethdat$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(sethdat[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(sethdat$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(sethdat[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
if(FALSE) {
# compare new resids to Seth's resids
windows()
plot(rdat[, 15], sethdat[, rtrussvars][, 15])
lm(sethdat[, rtrussvars][, 15] ~ rdat[, 15])
# compare new resids to Seth's resids
windows()
plot(rdat[, 25], sethdat[, rtrussvars][, 25])
lm(sethdat[, rtrussvars][, 25] ~ rdat[, 25])
}
rm(wb, dat1, datraw, dat2, sethdat, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), 1:10]
tab[, 8:10] <- round(tab[, 8:10], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
ks <- 1:14
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to 14 (since there were 14 sites).",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title="14 Sites, Both Sexes")
figu("Recommended number of clusters based on the optimum average silhouette width, 14 sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, 14 sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="14 Sites, Both Sexes")
figu("Difference in truss measurements between the cluster groups, 14 sites and both sexes combined, 14 sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title="14 Sites, Both Sexes")
figu("Centroid cluster analysis starting with a single cluster at each site, 14 sites and both sexes combined.", h=4, w=4)
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Males")
figu("Recommended number of clusters based on the optimum average silhouette width, males at 13 sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Males")
figu("Difference in truss measurements between the cluster groups, males at 13 sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title="13 Sites, Males")
figu("Centroid cluster analysis starting with a single cluster at each site, males at 13 sites.", h=4, w=4)
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Females")
figu("Recommended number of clusters based on the optimum average silhouette width, females at 13 sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Females")
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at 13 sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title="13 Sites, Females")
figu("Centroid cluster analysis starting with a single cluster at each site, females at 13 sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in c(2, 4, 7, 8)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
head(more)
head(dat2)
head(sethdat)
lls()
head(dat)
head(dat, 2)
head(more, 2)
set.diff(names(dat), names(more))
setdiff(names(dat), names(more))
setdiff(casefold(names(dat)), casefold(names(more)))
setdiff(names(dat), casefold(names(more), upper=TRUE))
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
fromnames <- c("Picture.No.", "Fish.ID", "TL..mm.")
tonames <-  c("OBS",   "ID",  "TL")
recode(names(more), fromnames, tonames)
names(more) <- recode(names(more), fromnames, tonames)
setdiff(names(dat), names(more))
names(more) <- recode(names(more), fromnames, tonames)
setdiff(names(dat), names(more))
names(dat)
names(more)
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
# missing AGE
fromnames <- c("Picture.No.", "Fish.ID", "TL..mm.")
tonames <-  c("OBS",   "ID",  "TL")
names(more)
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
# missing AGE
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
fromnames <- c("Picture.No.", "Fish.ID", "TL..mm.")
tonames <-  c("OBS",   "ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
# missing AGE
fromnames <- c("Picture.No.", "Fish.ID", "TL..mm.")
tonames <-  c("OBS",   "ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
names(more)
setdiff(names(dat), names(more))
library(plyr)
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
# missing AGE
fromnames <- c("Picture.No.", "Fish.ID", "TL..mm.")
tonames <-  c("OBS",   "ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
ls()
head(dat2, 2)
head(more, 2)
setdiff(names(dat2), names(more))
dat3 <- rbind.fill(dat2, more)
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
# missing AGE
fromnames <- c("Picture.No.", "Fish.ID", "TL..mm.")
tonames <-  c("OBS",   "ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
dat3 <- rbind.fill(dat2, more)
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 27 January 2015", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat3[is.na(dat3$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
dat4 <- dat3[!is.na(dat3$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- dat4[dat4$ID %in% dat4$ID[duplicated(dat4$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
dat4 <- dat4[!(dat4$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(dat4[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(dat4$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(dat4[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), 1:10]
tab[, 8:10] <- round(tab[, 8:10], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
graphics.off()
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
ks <- 1:14
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to 14 (since there were 14 sites).",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title="14 Sites, Both Sexes")
figu("Recommended number of clusters based on the optimum average silhouette width, 14 sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, 14 sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, 14 sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="14 Sites, Both Sexes")
figu("Difference in truss measurements between the cluster groups, 14 sites and both sexes combined, 14 sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title="14 Sites, Both Sexes")
figu("Centroid cluster analysis starting with a single cluster at each site, 14 sites and both sexes combined.", h=4, w=4)
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Males")
figu("Recommended number of clusters based on the optimum average silhouette width, males at 13 sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Males")
figu("Difference in truss measurements between the cluster groups, males at 13 sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title="13 Sites, Males")
figu("Centroid cluster analysis starting with a single cluster at each site, males at 13 sites.", h=4, w=4)
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Females")
figu("Recommended number of clusters based on the optimum average silhouette width, females at 13 sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Females")
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at 13 sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title="13 Sites, Females")
figu("Centroid cluster analysis starting with a single cluster at each site, females at 13 sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in c(2, 4, 7, 8)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
summary(dat4)
head(more)
table(more$LAKE)
table(more$SITE)
table(dat4$SITE)
length(table(dat4$SITE))
head(subscmf)
head(subm)
nsites
length(unique(dat$LS))
length(unique(more$LS))
14+8
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
more$OBS <- 1:dim(more)[1]
# missing AGE
fromnames <- c("Fish.ID", "TL..mm.")
tonames <-  c("ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
dat3 <- rbind.fill(dat2, more)
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 27 January 2015", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat3[is.na(dat3$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
dat4 <- dat3[!is.na(dat3$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- dat4[dat4$ID %in% dat4$ID[duplicated(dat4$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
dat4 <- dat4[!(dat4$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(dat4[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(dat4$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(dat4[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
rm(wb, dat1, datraw, dat3, dat4, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), c(1:6, 8:10)]
tab[, 8:10] <- round(tab[, 8:10], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
#####################################################################################
# remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
nsites <- length(unique(dat$LS))
ks <- 1:nsites
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to ", nsites, " (since there were ", nsites, " sites).",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title=paste(nsites, "Sites, Both Sexes")
figu("Recommended number of clusters based on the optimum average silhouette width, ", nsites, " sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, ", nsites, " sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, ", nsites, " sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, ", nsites, " sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsites, "Sites, Both Sexes")
figu("Difference in truss measurements between the cluster groups, ", nsites, "  sites and both sexes combined, ", nsites, " sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title=paste(nsites, "Sites, Both Sexes")
figu("Centroid cluster analysis starting with a single cluster at each site, ", nsites, " sites and both sexes combined.", h=4, w=4)
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Males")
figu("Recommended number of clusters based on the optimum average silhouette width, males at 13 sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Males")
figu("Difference in truss measurements between the cluster groups, males at 13 sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title="13 Sites, Males")
figu("Centroid cluster analysis starting with a single cluster at each site, males at 13 sites.", h=4, w=4)
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Females")
figu("Recommended number of clusters based on the optimum average silhouette width, females at 13 sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Females")
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at 13 sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title="13 Sites, Females")
figu("Centroid cluster analysis starting with a single cluster at each site, females at 13 sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in c(2, 4, 7, 8)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
more$OBS <- 1:dim(more)[1]
# missing AGE
fromnames <- c("Fish.ID", "TL..mm.")
tonames <-  c("ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
dat3 <- rbind.fill(dat2, more)
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 27 January 2015", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat3[is.na(dat3$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
dat4 <- dat3[!is.na(dat3$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- dat4[dat4$ID %in% dat4$ID[duplicated(dat4$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
dat4 <- dat4[!(dat4$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(dat4[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(dat4$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(dat4[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
rm(wb, dat1, datraw, dat3, dat4, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), c(1:6, 8:10)]
tab[, 7:9] <- round(tab[, 7:9], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
#####################################################################################
# remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
nsites <- length(unique(dat$LS))
ks <- 1:nsites
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to ", nsites, " (since there were ", nsites, " sites).",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title=paste(nsites, "Sites, Both Sexes"))
figu("Recommended number of clusters based on the optimum average silhouette width, ", nsites, " sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, ", nsites, " sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, ", nsites, " sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, ", nsites, " sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsites, "Sites, Both Sexes"))
figu("Difference in truss measurements between the cluster groups, ", nsites, "  sites and both sexes combined, ", nsites, " sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title=paste(nsites, "Sites, Both Sexes"))
figu("Centroid cluster analysis starting with a single cluster at each site, ", nsites, " sites and both sexes combined.", h=4, w=4)
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Males")
figu("Recommended number of clusters based on the optimum average silhouette width, males at 13 sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Males")
figu("Difference in truss measurements between the cluster groups, males at 13 sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title="13 Sites, Males")
figu("Centroid cluster analysis starting with a single cluster at each site, males at 13 sites.", h=4, w=4)
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Females")
figu("Recommended number of clusters based on the optimum average silhouette width, females at 13 sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Females")
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at 13 sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title="13 Sites, Females")
figu("Centroid cluster analysis starting with a single cluster at each site, females at 13 sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in c(2, 4, 7, 8)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
lapply(split(more, more$LS), summary)
head(more)
as.numeric(more$SEX)
mytable(more$SEX)
as.numeric(more$ID)
more$ID[is.na(as.numeric(more$ID))]
cleanup()
search()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
more$OBS <- 1:dim(more)[1]
# missing AGE
fromnames <- c("Fish.ID", "TL..mm.")
tonames <-  c("ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
more$SEX <- as.numeric(more$SEX)
dat3 <- rbind.fill(dat2, more)
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 27 January 2015", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat3[is.na(dat3$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
dat4 <- dat3[!is.na(dat3$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- dat4[dat4$ID %in% dat4$ID[duplicated(dat4$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
dat4 <- dat4[!(dat4$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(dat4[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(dat4$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(dat4[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
rm(wb, dat1, datraw, dat3, dat4, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), c(1:6, 8:10)]
tab[, 7:9] <- round(tab[, 7:9], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
#####################################################################################
# remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
nsites <- length(unique(dat$LS))
ks <- 1:nsites
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to ", nsites, " (the number of sites represented).",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title=paste(nsites, "Sites, Both Sexes"))
figu("Recommended number of clusters based on the optimum average silhouette width, ", nsites, " sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, ", nsites, " sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, ", nsites, " sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, ", nsites, " sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsites, "Sites, Both Sexes"))
figu("Difference in truss measurements between the cluster groups, ", nsites, "  sites and both sexes combined, ", nsites, " sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title=paste(nsites, "Sites, Both Sexes"))
figu("Centroid cluster analysis starting with a single cluster at each site, ", nsites, " sites and both sexes combined.", h=4, w=4)
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Males")
figu("Recommended number of clusters based on the optimum average silhouette width, males at 13 sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at 13 sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Males")
figu("Difference in truss measurements between the cluster groups, males at 13 sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title="13 Sites, Males")
figu("Centroid cluster analysis starting with a single cluster at each site, males at 13 sites.", h=4, w=4)
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females at 13 sites was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title="13 Sites, Females")
figu("Recommended number of clusters based on the optimum average silhouette width, females at 13 sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at 13 sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at 13 sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title="13 Sites, Females")
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at 13 sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title="13 Sites, Females")
figu("Centroid cluster analysis starting with a single cluster at each site, females at 13 sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in c(2, 4, 7, 8)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
head(subm)
length(unique(subm$LS))
length(unique(subf$LS))
length(unique(submf$LS))
max(length(unique(subm$LS)), length(unique(subf$LS)), length(unique(submf$LS)))
suls
suls[c(5, 7, 10, 11)]
dput(suls[c(5, 7, 10, 11)])
picksites <- c("H - ST. MARY", "O - CHAUMONT", "S - DEVILS I", "S - GRAND PO")
match(picksites, suls)
cleanup()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
more$OBS <- 1:dim(more)[1]
# missing AGE
fromnames <- c("Fish.ID", "TL..mm.")
tonames <-  c("ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
more$SEX <- as.numeric(more$SEX)
dat3 <- rbind.fill(dat2, more)
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 27 January 2015", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat3[is.na(dat3$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
dat4 <- dat3[!is.na(dat3$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- dat4[dat4$ID %in% dat4$ID[duplicated(dat4$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
dat4 <- dat4[!(dat4$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(dat4[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(dat4$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(dat4[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
rm(wb, dat1, datraw, dat3, dat4, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), c(1:6, 8:10)]
tab[, 7:9] <- round(tab[, 7:9], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
#####################################################################################
# remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
nsitesmf <- length(unique(submf$LS))
nsitesm <- length(unique(subm$LS))
nsitesf <- length(unique(subf$LS))
ks <- 1:nsitesmf
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to the number of sites represented: ", 
nsitesfm, " sites for both sexes combined, ", nsitesm, " for males, and ", nsitesf, " for females.",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title=paste(nsitesmf, "Sites, Both Sexes"))
figu("Recommended number of clusters based on the optimum average silhouette width, ", nsitesmf, " sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, ", nsitesmf, " sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, ", nsitesmf, " sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, ", nsitesmf, " sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsitesmf, "Sites, Both Sexes"))
figu("Difference in truss measurements between the cluster groups, ", nsitesmf, " sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title=paste(nsitesmf, "Sites, Both Sexes"))
figu("Centroid cluster analysis starting with a single cluster at each site, ", nsitesmf, " sites and both sexes combined.", h=4, w=4)
ks <- 1:nsitesm
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title=paste(nsitesm, "Sites, Males"))
figu("Recommended number of clusters based on the optimum average silhouette width, males at ", nsitesm, " sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at ", nsitesm, " sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at ", nsitesm, " sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at ", nsitesm, " sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsitesm, "Sites, Males"))
figu("Difference in truss measurements between the cluster groups, males at ", nsitesm, " sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title=paste(nsitesm, "Sites, Males"))
figu("Centroid cluster analysis starting with a single cluster at each site, males at ", nsitesm, " sites.", h=4, w=4)
ks <- 1:nsitesf
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title=paste(nsitesf, "Sites, Females"))
figu("Recommended number of clusters based on the optimum average silhouette width, females at ", nsitesf, " sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at ", nsitesf, " sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at ", nsitesf, " sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at ", nsitesf, " sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsitesf, "Sites, Females"))
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at ", nsitesf, " sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title=paste(nsitesf, "Sites, Females"))
figu("Centroid cluster analysis starting with a single cluster at each site, females at ", nsitesf, " sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
picksites <- c("H - ST. MARY", "O - CHAUMONT", "S - DEVILS I", "S - GRAND PO")
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in match(picksites, suls)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
cleanup()
# C:\JVA\Consult\Yule\CiscoMorpho\CiscoMorph.r
# bring in functions
source("C:/JVA/Consult/Yule/CiscoMorpho/CiscoMorphFunctions.r")
library(fpc)
library(rpart)
library(rpart.plot)
library(seriation)
library(plyr)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Cisco size-corrected residuals FINAL.xls")
dat1 <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat1$LS <- paste(substring(dat1$LAKE, 1, 1), dat1$SITE, sep=" - ")
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/FISH MORPHOMETRICS with total lengths RAW Truss measurements.xlsx")
datraw <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/Re-measured photos for Jean.xlsx")
remeas <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
dat2 <- cbind(dat1, datraw[match(dat1$ID, datraw$ID.), paste0("L", 1:31)])
ltrussvars <- paste0("L", 1:31)
rtrussvars <- paste0("r", 1:31)
# replace truss lengths for 21 fish photos that were remeasured
dat2[match(remeas$ID., dat2$ID), ltrussvars] <- remeas[, ltrussvars]
# more Lake Huron (Manitoulin Island) and Lake Erie data (22 Jan 2015 e-mail, https://mail.google.com/mail/u/0/#inbox/14b12f1714db6fe0)
wb <- loadWorkbook("C:/JVA/Consult/Yule/CiscoMorpho/2014 Huron and Erie Cisco morphometric measurements.xlsx")
more <- readWorksheet(wb, sheet=getSheets(wb)[1], startRow=1)
more$OBS <- 1:dim(more)[1]
# missing AGE
fromnames <- c("Fish.ID", "TL..mm.")
tonames <-  c("ID",  "TL")
names(more) <- recode(names(more), fromnames, tonames)
names(more) <- casefold(names(more), upper=TRUE)
more$LS <- paste(substring(more$LAKE, 1, 1), more$SITE, sep=" - ")
more$SEX <- as.numeric(more$SEX)
dat3 <- rbind.fill(dat2, more)
doc <- startrtf(file="CiscoMorph", dir="C:/JVA/Consult/Yule/CiscoMorpho")
heading("Exploring Dan's Cisco Morphometrics Data")
heading("Jean V. Adams - 27 January 2015", 2)
para("There was one row in the cisco morphometrics data with missing truss measurements (Table ", jvamiscenv$tabcount, ").",
"  This row was eliminated from further analysis.")
tab <- dat3[is.na(dat3$L1), c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars[1:5])]
tabl("Records with missing truss measurements.", row.names=FALSE)
dat4 <- dat3[!is.na(dat3$L1), ]
para("There were a few rows in the cisco morphometrics data with duplicate IDs.",
"  Most of the measurements were pretty close, but a few were a bit further off (Table ", jvamiscenv$tabcount, ").",
"  Dan checked into these, and could only find photos for OBSs 563 and 675,",
" so all of the other observations were removed prior to analysis.")
a <- dat4[dat4$ID %in% dat4$ID[duplicated(dat4$ID)], ]
b <- a[c(1, 3, 5, 7, 9), rtrussvars] - a[c(2, 4, 6, 8, 10), rtrussvars]
#plot(sort(abs(unlist(b))))
tab <- a[, c(1:7, 7+c(9, 16, 28, 29))]
tab[, 8:11] <- format(round(tab[, 8:11], 2))
tabl("Records with duplicate IDs.  Truss measurements are given for those measures that were off by more than 0.3.", row.names=FALSE)
dat4 <- dat4[!(dat4$OBS %in% tab$OBS[!(tab$OBS %in% c(563, 675))]), ]
para("Prior to analysis, the size component was removed from the morphometric measures.",
"  Truss measurements (in mm) were natural log transformed, and the first principal component",
" (based on the covariance matrix) of these measures was used as general measure of size (Figure ", jvamiscenv$figcount, ").")
# log transform the lengths
ldat <- log(dat4[, ltrussvars])
# use the first principal component as size
size <- princomp(ldat, cor=FALSE, scores=TRUE)$scores[, 1]
fig <- function() {
par(mar=c(4, 4, 1, 1))
plot(dat4$TL, size, xlab="Total length of fish  (mm)", ylab="General measure of size  (PC1)", las=1)
}
figu("Relation between fish total length and derived general measure of fish size based on the first",
" principal component score of the log transformed truss measurements.", h=4, w=4)
# regress size (x) on each of the log transformed lengths (y) to get a residual
rdat <- sapply(ldat, function(y) lm(y ~ size)$resid)
dimnames(rdat)[[2]] <- rtrussvars
dat <- cbind(dat4[, c("OBS", "ID", "SEX", "AGE", "LAKE", "SITE", "LS", "TL", ltrussvars)], size, rdat)
# write.csv(dat, "C:/JVA/Consult/Yule/CiscoMorpho/Cisco morpho with Jean residuals.csv", row.names=FALSE)
rm(wb, dat1, datraw, dat3, dat4, a, b, ldat, size, rdat)
para("There were also ", sum(is.na(dat$SEX)), " records where sex was missing (Table ", jvamiscenv$tabcount, ").")
a <- table(dat$LS[is.na(dat$SEX)])
tab <- data.frame(LakeSite=names(a), Nrecords=as.numeric(a))
tabl("Number of records, by lake and site, with nothing entered for sex.", row.names=FALSE)
para("And ", sum(!is.na(dat$SEX) & !(dat$SEX %in% 1:2)), " records where sex was not equal to 1 or 2 (Table ", jvamiscenv$tabcount, ").")
tab <- dat[!is.na(dat$SEX) & !(dat$SEX %in% 1:2), c(1:6, 8:10)]
tab[, 7:9] <- round(tab[, 7:9], 2)
tabl("Records with sex not equal to 1 or 2.", row.names=FALSE)
para("For analyses conducted for individual sexes, I used only those records with sex equal to 1 (males) or 2 (females) .",
"  In all cases (both sexes, just males, and just females),",
" I scaled the data (subtracting the mean and dividing by the standard deviation),",
" so that all of the truss measurements would have the same mean and variance.",
"  This ensures that each truss measure will be given the same amount of weight in a cluster analysis.")
# subset the data
submf.all <- dat[!duplicated(dat$ID), ]
para("I looked at the length distribution and sex composition at all sites (Figures ", jvamiscenv$figcount, " and ", jvamiscenv$figcount+1, ").",
"  Only those fish greater than or equal to 300 mm were used in the tables, figures, and analyses that follow Figure ", jvamiscenv$figcount, ".")
susl <- sort(unique(dat$LS))
tlmed <- tapply(submf.all$TL, submf.all$LS, median)
ord <- order(tlmed)
fig <- function() {
par(mfrow=c(length(susl), 1), mar=c(0, 0, 0, 0), oma=c(4, 1, 1, 0))
for(i in ord) {
sel <- submf.all$LS == susl[i]
hist(submf.all$TL[sel], breaks=seq(200, 550, 10), axes=FALSE, col="gray", xlab="", ylab="", main="")
mtext(susl[i], side=3, line=-1, adj=0.05, cex=0.7)
abline(v=seq(200, 550, 50))
abline(v=tlmed[i], col="cyan", lwd=2)
}
axis(1, outer=TRUE, lwd=0, lwd.ticks=1)
mtext("Total length  (mm)", side=1, outer=TRUE, line=2.5)
mtext("Frequency", side=2, outer=TRUE, line=-0.5)
}
figu("Length frequency distribution of all fish by lake and site, sites ordered by median length (vertical cyan lines).", newpage="port")
# further subset the data, only fish >= 300 mm
submf <- submf.all[submf.all$TL >= 300, ]
subm <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==1, ]
subf <- dat[!duplicated(dat$ID) & !is.na(dat$SEX) & dat$SEX==2, ]
# rescale the data
subscmf <- scale(submf[, rtrussvars])
subscm <- scale(subm[, rtrussvars])
subscf <- scale(subf[, rtrussvars])
if(FALSE) {
# error check truss measurements
submftruss <- data.frame(t(submf[, ltrussvars]))
names(submftruss) <- submf$ID
fc1 <- lapply(submftruss, buildfish)
fc2 <- lapply(submftruss, buildfish, upfirst=FALSE)
fc1[sapply(fc1, function(x) any(is.na(x)))]
fc2[sapply(fc2, function(x) any(is.na(x)))]
buildfish(submf[submf$ID==106439, ltrussvars])
buildfish(submf[submf$ID==106439, ltrussvars], upfirst=FALSE)
sort(signif(submf[submf$ID==106439, ltrussvars], 3))
# error in ID 106439, L5=54.9, L10=182, L11=122 ... L10 too big??
# this error seems to be fixed with the remeasuring
t1 <- t(sapply(fc1, fc2truss))
t2 <- t(sapply(fc2, fc2truss))
r1 <- submf[, ltrussvars] - t1[, ltrussvars]
r2 <- submf[, ltrussvars] - t2[, ltrussvars]
mse1 <- sqrt(apply(r1^2, 1, sum))
mse2 <- sqrt(apply(r2^2, 1, sum))
mse <- pmax(mse1, mse2, na.rm=TRUE)
sel <- mse > 20
round(r1[sel, ])
round(r2[sel, ])
t1[sel, ]
t2[sel, ]
submf[sel, ]
#####################################################################################
# remeas[remeas$ID. %in% submf$ID[sel], ]
count <- 0
for(i in 1:30) {
for(j in (i+1):31) {
if(count %% 9 == 0) {
windows()
par(mfrow=c(3, 3), mar=c(4, 4, 1, 1), cex=0.5)
}
count <- count + 1
plotblank(submf[, ltrussvars[i]], submf[, ltrussvars[j]], xlab=ltrussvars[i], ylab=ltrussvars[j])
text(submf[, ltrussvars[i]], submf[, ltrussvars[j]], seq(submf$ID))
}}
# these are rows selected as outliers from the plots
a <- c(586, 146, 117, 66, 255, 164, 215, 14, 374, 74, 97, 226, 239, 39, 571, 193, 738, 305, 219, 75)
mse[a]
b <- sort(unique(c(seq(sel)[sel], a)))
btruss <- data.frame(t(submf[b, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[b]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
fishpord2 <- c(1:14, 1)
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(9, 3))
for(i in 1:length(fc)) {
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=submf$ID[b][i])
lines(fc[[i]][fishpord, ], type="o")
lines(fc1[[i]][fishpord2, ], type="o", col="red")
lines(fc2[[i]][fishpord2, ], type="o", col="blue")
}
allison <- c(106414, 106439, 106509, 106517, 106518, 106540, 106220, 106250, 106262, 106268, 106297, 87077,
87081, 87089, 87126, 87143, 8105018, 28, 102661, 102003, 102945)
btruss <- data.frame(t(submf[submf$ID %in% allison, ltrussvars]/submf$TL))
names(btruss) <- submf$ID[submf$ID %in% allison]
fc <- lapply(btruss, function(x) fishpts(x)[[2]])
fc1 <- lapply(btruss, function(x) buildfish(x))
fc2 <- lapply(btruss, function(x) buildfish(x, upfirst=FALSE))
xyr <- apply(do.call(rbind, fc), 2, range)
windows(h=9, w=6.5)
par(mar=c(0, 0, 1, 0), mfrow=c(7, 3))
for(i in 1:length(fc)) {
if(i==6) frame()
j <- match(allison[-6][i], names(btruss))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="", main=allison[-6][i])
lines(fc[[j]][fishpord, ], type="o")
lines(fc1[[j]][fishpord2, ], type="o", col="red")
lines(fc2[[j]][fishpord2, ], type="o", col="blue")
}
}
subb <- rbind(subm, subf)
pmale <- tapply(subb$SEX==1, subb$LS, mean)
ord <- order(pmale)
fig <- function() {
par(mar=c(4, 11, 1, 1), xaxs="i")
barplot(rbind(pmale[ord], 1-pmale[ord]), horiz=TRUE, las=1, xlim=c(0, 1), col=c("blue", "red"), xlab="Proportion males", ylab="")
box()
abline(v=seq(0.2, 0.8, 0.2))
}
figu("Sex composition by lake and site, sites ordered by the proportion of males (blue).", h=4, w=4)
nsitesmf <- length(unique(submf$LS))
nsitesm <- length(unique(subm$LS))
nsitesf <- length(unique(subf$LS))
ks <- 1:nsitesmf
pamcl <- pamk(data=subscmf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("I used the PAM (partitioning around medioids) method for clustering.",
"  I let the number of clusters range from 1 to the number of sites represented: ", 
nsitesmf, " sites for both sexes combined, ", nsitesm, " for males, and ", nsitesf, " for females.",
"  The recommended number of clusters for data from both sexes combined",
" based on the optimum average silhouette width was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig.nclusters <- function(title) {
plot(ks[-1], pamcl$crit[-1], las=1, type="b", 
xlab="No. of clusters", ylab="Average silhouette width", main=title)
abline(v=pamcl$nc, lty=2)
sel <- ks==pamcl$nc
points(ks[sel], pamcl$crit[sel], pch=16, cex=1.5)
}
fig <- function() fig.nclusters(title=paste(nsitesmf, "Sites, Both Sexes"))
figu("Recommended number of clusters based on the optimum average silhouette width, ", nsitesmf, " sites and both sexes combined.", h=4, w=4)
# look at the proportion of fish in each lake/site assigned to each cluster group
tab.prop <- function(grp, df) {
tot <- tapply(!is.na(grp), list(df$LS, grp), sum)
tot[is.na(tot)] <- 0
prop <- tot/apply(tot, 1, sum)
ord <- seriate(prop, method="PCA")
g <- prop[get_order(ord, 1), get_order(ord, 2)]
dimnames(g)[[2]] <- paste0("G", dimnames(g)[[2]])
list(g=g, gord=get_order(ord, 2))
}
both <- tab.prop(group, submf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, ", nsitesmf, " sites and both sexes combined.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig.bargrp <- function() {
par(mar=c(4, 8, 1, 1),  xaxs="i")
barplot(t(g)[, dim(g)[1]:1], horiz=TRUE, las=1, xlim=c(0, 1), col=colz[both$gord], xlab="Proportion of fish", ylab="")
box()
}
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, ", nsitesmf, " sites and both sexes combined.", h=4, w=4)
rm(tlmed, ord, subb, pmale, g)
para("I divided the truss measurements by the fish total length and calculated the median of these values for each cluster group.",
"  Then I built a representative fish diagram of each cluster group based on these values (Figure ", jvamiscenv$figcount, ").")
fishpord <- c(1:4, 6, 8, 10, 12, 14, 13, 11, 9, 7, 5, 1)
#fishpord <- c(1:14, 1)
medun <- t(apply(submf[, ltrussvars]/submf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig.gfish <- function() {
par(mar=c(0, 0, 1, 0))
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
for(i in 1:length(fc)) {
co <- fc[[i]]
lines(co[fishpord, ], col=colz[i], type="o")
}
mtext(c("Head", "Tail"), side=3, adj=c(0.05, 0.95), line=-1)
legend("top", paste("Group", 1:pamcl$nc), col=colz, lty=1, bty="n", lwd=3, horiz=TRUE)
}
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, ", nsitesmf, " sites and both sexes combined.",
"  Fish are lined up by their noses.", h=3, w=6.5)
fig.grpdif <- function(title) {
plot(1:dim(med2)[1], med2$mdif, ylim=c(-1, 1)*max(amdif), las=1,
xlab="Truss  (r#)", ylab="Difference between scaled measures of group 1 and 2", main=title)
abline(h=0)
segments(1:dim(med2)[1], med2$mdif, 1:dim(med2)[1], 0)
}
if(pamcl$nc == 2) {
para("I calculated the median of each of the scaled measurements for each group.",
"  Then I looked at the difference between those medians to see which truss measurements",
" were the most different between the groups (Figure ", jvamiscenv$figcount, ").")
# median measures per group
med <- t(apply(subscmf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsitesmf, "Sites, Both Sexes"))
figu("Difference in truss measurements between the cluster groups, ", nsitesmf, " sites and both sexes combined.", 
h=4, w=4)
}
para("I also looked at how the clustering would have proceeded if we had started with a single cluster of fish at each site.",
"  For this, I used agglomerative hierarchical clustering with the centroid method and the squared Euclidean distances (Figure ",
jvamiscenv$figcount, ".)")
cent <- apply(subscmf, 2, tapply, submf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(submf$LS))
fig.dendro <- function(title) {
par(mar=c(1, 4, 1, 1))
plot(hc1, xlab="", sub="", , main=title)
}
fig <- function() fig.dendro(title=paste(nsitesmf, "Sites, Both Sexes"))
figu("Centroid cluster analysis starting with a single cluster at each site, ", nsitesmf, " sites and both sexes combined.", h=4, w=4)
ks <- 1:nsitesm
pamcl <- pamk(data=subscm, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for males was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title=paste(nsitesm, "Sites, Males"))
figu("Recommended number of clusters based on the optimum average silhouette width, males at ", nsitesm, " sites.", h=4, w=4)
both <- tab.prop(group, subm)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, males at ", nsitesm, " sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, males at ", nsitesm, " sites.", h=4, w=4)
medun <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, males at ", nsitesm, " sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscm, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsitesm, "Sites, Males"))
figu("Difference in truss measurements between the cluster groups, males at ", nsitesm, " sites.", h=4, w=4)
}
cent <- apply(subscm, 2, tapply, subm$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subm$LS))
fig <- function() fig.dendro(title=paste(nsitesm, "Sites, Males"))
figu("Centroid cluster analysis starting with a single cluster at each site, males at ", nsitesm, " sites.", h=4, w=4)
ks <- 1:nsitesf
pamcl <- pamk(data=subscf, krange=ks, criterion="asw", usepam=TRUE, scaling=FALSE, diss=FALSE)
group <- pamcl$pamobject$clustering
para("The recommended number of clusters for females was ", pamcl$nc, " (Figure ", jvamiscenv$figcount, ").")
fig <- function() fig.nclusters(title=paste(nsitesf, "Sites, Females"))
figu("Recommended number of clusters based on the optimum average silhouette width, females at ", nsitesf, " sites.", h=4, w=4)
both <- tab.prop(group, subf)
g <- both$g
tab <- data.frame(LakeSite=dimnames(g)[[1]], format(round(g, 4)))
tabl("Proportion of fish assigned to cluster groups in each lake and site, females at ", nsitesf, " sites.", row.names=FALSE)
colz <- colr(1:pamcl$nc, "brown", "orange")
fig <- function() fig.bargrp()
figu("Proportion of fish assigned to cluster groups in each lake and site, females at ", nsitesf, " sites.", h=4, w=4)
medun <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, group, median, na.rm=TRUE))
fc <- lapply(data.frame(medun), function(x) fishpts(x)[[2]])
#fc <- lapply(data.frame(medun), function(x) buildfish(x))
xyr <- apply(do.call(rbind, fc), 2, range)
fig <- function() fig.gfish()
figu("Diagram of fish based on median truss measurements (scaled by total length) for each cluster group, females at ", nsitesf, " sites.",
"  Fish are lined up by their noses.", h=3, w=6.5)
if(pamcl$nc == 2) {
# median measures per group
med <- t(apply(subscf, 2, tapply, group, median))
med2 <- data.frame(med, mdif = med[, 1] - med[, 2])
med3 <- med2[order(med2$mdif), ]
amdif <- abs(med2$mdif)
fig <- function() fig.grpdif(title=paste(nsitesf, "Sites, Females"))
figu("Difference in residual (log transformed, size corrected, and scaled) truss measurements between the cluster groups,",
" females at ", nsitesf, " sites.", h=4, w=4)
}
cent <- apply(subscf, 2, tapply, subf$LS, mean)
hc1 <- hclust(dist(cent)^2, method="centroid", members=table(subf$LS))
fig <- function() fig.dendro(title=paste(nsitesf, "Sites, Females"))
figu("Centroid cluster analysis starting with a single cluster at each site, females at ", nsitesf, " sites.", h=4, w=4)
medun.ms <- t(apply(subm[, ltrussvars]/subm$TL, 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fs <- t(apply(subf[, ltrussvars]/subf$TL, 2, tapply, subf$LS, median, na.rm=TRUE))
medun.mu <- t(apply(subm[, ltrussvars], 2, tapply, subm$LS, median, na.rm=TRUE))
medun.fu <- t(apply(subf[, ltrussvars], 2, tapply, subf$LS, median, na.rm=TRUE))
fc.ms <- lapply(data.frame(medun.ms), function(x) fishpts(x)[[2]])
fc.fs <- lapply(data.frame(medun.fs), function(x) fishpts(x)[[2]])
fc.mu <- lapply(data.frame(medun.mu), function(x) fishpts(x)[[2]])
fc.fu <- lapply(data.frame(medun.fu), function(x) fishpts(x)[[2]])
# fc.ms <- lapply(data.frame(medun.ms), function(x) buildfish(x))
# fc.fs <- lapply(data.frame(medun.fs), function(x) buildfish(x))
# fc.mu <- lapply(data.frame(medun.mu), function(x) buildfish(x))
# fc.fu <- lapply(data.frame(medun.fu), function(x) buildfish(x))
xyr.s <- apply(rbind(do.call(rbind, fc.ms), do.call(rbind, fc.fs)), 2, range)
xyr.u <- apply(rbind(do.call(rbind, fc.mu), do.call(rbind, fc.fu)), 2, range)
suls <- dimnames(medun.ms)[[2]]
picksites <- c("H - ST. MARY", "O - CHAUMONT", "S - DEVILS I", "S - GRAND PO")
fig <- function() {
par(mfrow=c(4, 2), mar=c(0, 0, 1, 0), oma=c(0, 0, 2, 0))
for(i in match(picksites, suls)) {
eqscplot(1, 1, type="n", xlim=xyr.s[, 1], ylim=xyr.s[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.ms[[i]][fishpord, ], col="blue", type="o")
lines(fc.fs[[i]][fishpord, ], col="red", type="o")
mtext(paste("     ", suls[i]), side=3, adj=0, line=-2)
eqscplot(1, 1, type="n", xlim=xyr.u[, 1], ylim=xyr.u[, 2], axes=FALSE, xlab="", ylab="")
lines(fc.mu[[i]][fishpord, ], col="blue", type="o")
lines(fc.fu[[i]][fishpord, ], col="red", type="o")
}
mtext(c("Scaled", "Unscaled"), side=3, adj=c(0.2, 0.8), outer=TRUE)
legend("top", c("Male", "Female"), col=c("blue", "red"), lty=1, bty="n")
}
figu("Comparison of median fish shapes of males and females at four selected sites,",
" using both scaled (by total length, left diagrams) and unscaled (right diagrams) truss measurements.",
"  Fish are lined up by their noses.", newpage="port")
endrtf()
rm(med, med2, med3, amdif, cent, hc1, pamcl, g, medun, fc, xyr)
if(FALSE) {
# print the points for one fish to overlay on photo
co <- fishpts(dat[dat$ID==500, ltrussvars])[[2]]
# co <- buildfish(dat[dat$ID==500, ltrussvars])
xyr <- apply(co, 2, range)
windows()
eqscplot(1, 1, type="n", xlim=xyr[, 1], ylim=xyr[, 2], axes=FALSE, xlab="", ylab="")
points(co[fishpord, ], col="red", pch=16)
}
dim(more)
cleanup()
tweethead()
q()
?rpart
??rpart
library(rpart)
?rpart
rpart
fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
jvatree <- function(...) {
tree1 <- rpart(...)
tab <- tree1$cptable
xerror <- tab[, "xerror"]
xstd <- tab[, "xstd"]
CP <- tab[, "CP"]
indx.min.xerror <- which.min(xerror)
chosen.cp <- CP[xerror < (xerror + xstd)[indx.min.xerror]][1]
tree2 <- prune(tree1, cp=chosen.cp)
tree2
}
plot(fit)
text(fit, use.n = TRUE)
jvatree <- function(...) {
tree1 <- rpart(...)
tab <- tree1$cptable
xerror <- tab[, "xerror"]
xstd <- tab[, "xstd"]
CP <- tab[, "CP"]
indx.min.xerror <- which.min(xerror)
chosen.cp <- CP[xerror < (xerror + xstd)[indx.min.xerror]][1]
tree2 <- prune(tree1, cp=chosen.cp)
tree2
}
fit2 <- jvatree(Kyphosis ~ Age + Number + Start, data=kyphosis)
windows()
plot(fit2)
text(fit, use.n=TRUE)
fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
fit2 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis,
              parms = list(prior = c(.65,.35), split = "information"))
fit3 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis,
              control = rpart.control(cp = 0.05))
summary(fit)
fit
fit2
fit3
mtcars
fit <- rpart(mpg ~ ., data=mtcars)
plot(fit)
fit <- jvatree(mpg ~ ., data=mtcars)
plot(fit)
fit <- rpart(Mileage ~ Price + Country + Reliability + Type, data=cu.summary)
plot(fit)
fit <- jvatree(Mileage ~ Price + Country + Reliability + Type, data=cu.summary)
plot(fit)
fit <- jvatree(Mileage ~ Price + Country + Reliability + Type, data=cu.summary)
plot(fit)
text(fit, use.n = TRUE)
par(xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
fit <- jvatree(Mileage ~ Price + Country + Reliability + Type, data=cu.summary)
windows()
par(xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
fit <- jvatree(Mileage ~ Price + Country + Reliability + Type, data=cu.summary)
windows()
par(xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
fit <- jvatree(Mileage ~ Country + Reliability + Type, data=cu.summary)
windows()
par(xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
fit <- jvatree(Mileage ~ Country + Reliability + Type, data=cu.summary)
windows()
par(xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
cleanup()
search()
graphics.off()
q()
pkgin("jvamisc")
?cu.summary
?data
mtcars
head(mtcars)
fit <- rpart(hp ~ x, data=mtcars)
fit2 <- jvatree(hp ~ x, data=mtcars)
windows()
par(mfrow=c(2, 1), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
library(rpart)
jvatree <- function(...) {
tree1 <- rpart(...)
tab <- tree1$cptable
xerror <- tab[, "xerror"]
xstd <- tab[, "xstd"]
CP <- tab[, "CP"]
indx.min.xerror <- which.min(xerror)
chosen.cp <- CP[xerror < (xerror + xstd)[indx.min.xerror]][1]
tree2 <- prune(tree1, cp=chosen.cp)
tree2
}
fit <- rpart(hp ~ x, data=mtcars)
fit2 <- jvatree(hp ~ x, data=mtcars)
windows()
par(mfrow=c(2, 1), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(hp ~ ., data=mtcars)
fit2 <- jvatree(hp ~ ., data=mtcars)
windows()
par(mfrow=c(2, 1), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
head(mtcars_
head(mtcars)
fit <- rpart(mpg ~ ., data=mtcars)
fit2 <- jvatree(mpg ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(disp ~ ., data=mtcars)
fit2 <- jvatree(disp ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(cyl ~ ., data=mtcars)
fit2 <- jvatree(cyl ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(drat ~ ., data=mtcars)
fit2 <- jvatree(drat ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(wt ~ ., data=mtcars)
fit2 <- jvatree(wt ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(qsec ~ ., data=mtcars)
fit2 <- jvatree(qsec ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(vs ~ ., data=mtcars)
fit2 <- jvatree(vs ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(am ~ ., data=mtcars)
fit2 <- jvatree(am ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(gear ~ ., data=mtcars)
fit2 <- jvatree(gear ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
fit <- rpart(carb ~ ., data=mtcars)
fit2 <- jvatree(carb ~ ., data=mtcars)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
find("mtcars)
find("mtcars")
search()
ls(4)
head(airquality)
fit <- rpart(Ozone ~ ., data=airquality)
fit2 <- jvatree(Ozone ~ ., data=airquality)
windows()
par(mfrow=c(1, 2), xpd=NA) # otherwise on some devices the text is clipped
plot(fit)
text(fit, use.n = TRUE)
plot(fit2)
text(fit2, use.n = TRUE)
pkgup("jvamisc")
q()
